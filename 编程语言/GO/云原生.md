
# 云原生概念

什么是云？
就是把计算资源抽象出来

- 在包括公有云、私有云、混合云等动态环境中构建和运行规模化应用的能力。
- 云原生是一种思想，是技术、企业管理方法的**集合**。
- 技术层面
	- 应用程序从设计之初就为在云上运行而做好准备。
	- 云平台基于自动化体系。
	- 
- 
- 流程层面
	- 基于DevOps,CI/CD。


# Docker 核心技术

## 传统分层架构 vs 微服务

![image.png](http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241015221133.png?OSSAccessKeyId=LTAI5tBK1gnqyQLHK2d7sx6F&Expires=9000000001&Signature=GqTbzWJg3lQe4IcZsgB24Dg89Oo=)

传统分层架构 (左侧):

- 单一大型应用程序
- 包含三个主要层:
    - UI (用户界面)
    - Business Logic (业务逻辑)
    - Data Access Layer (数据访问层)
- 这些层紧密耦合在一起
- 底部有一个单一的数据库


微服务架构 (右侧):
- 将应用拆分成多个小型、独立的服务
- 每个微服务负责特定的功能
- UI层与多个微服务通过网络调用交互(会有大量的网络调用)
- 每个微服务可以有自己的数据库
- 服务之间通过网络通信


> [!multi-column]
>
>> [!note]+ 分层架构
>>
>>- 简单系统
>>	- 易部署
>>	- 易测试
>>	- 易横向扩展
>>- 复杂系统
>>	- 难以理解整体 
>>	- 不易快速维护 
>>	- 启动慢
>>	- 部署慢
>>	- 变更引起的回归问题多 
>>	- 难以做持续集成和持续部署
>
>> [!warning]+ 微服务
>>
>>- 简单系统
>>	- 微服务架构增加了系统的复杂性,把集中部署变成了分布式式部署。需要实现基于消息或RPC的进程间通讯,需要处理部分失败等分布式系统的复杂问题 
>>	- 微服务采用分区数据库架构,一个事物需要更新不同微服务的数据库,分布式事务更复杂(失败回滚）
>>	- 测试更复杂 
>>	- 部署监控更复杂
>>- 复杂系统
>>	- 将一个庞大系统分解成高内聚，松耦合的组件使得系统部署更快，更易理解和维护 
>>	- 不同服务由不同team维护，分工更细，更关注和关注业务微服务往往是异构系统，开发团队可自主选择技术栈 
>>	- 微服务架构使每个服务独立部署，易于持续集成和持续部署 
>>	- 每个微服务独立扩展


微服务改造
分离微服务的方法建议：
- 审视并发现可以**分离的业务逻辑**业务逻辑
- 寻找天生**隔离的代码模块**，可以借助于**静态代码分析工具**
- 不同**并发规模**，不同**内存需求**的模块都可以分离出不同的微服务，此方法可提高资源利用率，节省成本

一些常用的可微服务化的组件：
- 用户和账户管理
- 授权和会话管理
- 系统配置
- 通知和通讯服务
- 照片，多媒体，元数据等

分解原则：基于 size, scope and capabilities


## 微服务间通讯

**点对点**：
- 多用于系统内部多组件之间通讯；
- 有大量的重复模块如认证授权；
- 缺少统一规范，如监控，审计等功能；
- 后期维护成本高，服务和服务的依赖关系错综复杂难以管理


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016000127.png" alt="image.png" style="zoom:60%;" />


**API 网关**
- 基于一个轻量级的 message gateway
- 新 API 通过注册至 Gateway 实现
- 整合实现 Common function

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016000338.png" alt="image.png" style="zoom:60%;" />

## 理解 Docker


- 基于 Linux 内核的 Cgroup(是Linux内核的一个功能，用于**限制**、记录和**隔离**进程组使用的**物理资源**（CPU、内存、磁盘I/O等）)，Namespace(提供了**进程隔离**的功能，使得容器中的进程看到的系统环境是独立的，与其他容器和宿主机隔离)，以及 Union FS(一种**分层**、轻量级并且高性能的文件系统)等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术，由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。
- 最初实现是基于 LXC，从 0.7 以后开始去除 LXC，转而使用自行开发的 Libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 Containerd。
- Docker 在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护，使得 Docker 技术比虚拟机技术更为轻便、快捷。



**为什么要用 Docker**

- 更**高效地利用系统资源**
- 更快速的启动时间
- **一致**的运行环境
- 持续交付和部署
- 更轻松地**迁移**
- 更轻松地维护和扩展


虚拟机和容器运行态的对比


> [!multi-column]
>
>> [!note]+ 图片
>>
>>- <img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016001548.png" alt="image.png" style="zoom:60%;" />
>
>> [!warning]+ 说明
>>
>>- 结构从下到上： 
>>	- Server (物理服务器)
>>	- Host OS (宿主操作系统)
>>	- Hypervisor (虚拟机监控器)
>>	- Guest OS (客户操作系统)
>>	- Bins/Libs (二进制文件和库)
>>	- App (应用程序)
>>- 特点：
>>	- 每个虚拟化应用不仅包含应用本身（可能只有几十MB），还包括必要的二进制文件和库。 
>>	- 还需要一个完整的客户操作系统，可能占用几十GB的空间。 
>>	- 资源占用较大，启动较慢。



> [!multi-column]
>
>> [!note]+ 图片
>>
>> <img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016001837.png" alt="image.png" style="zoom:60%;" />
>
>> [!warning]+ 说明
>>
>> - 结构从下到上：
>>   - Server (物理服务器)
>>   - Host OS (宿主操作系统)
>>   - Docker Engine (Docker引擎)
>>   - Bins/Libs (二进制文件和库)
>>   - App (应用程序)
>> - 特点：
>>   - Docker容器只包含应用程序及其依赖。
>>   - 作为宿主操作系统上的隔离进程运行。
>>   - 与其他容器共享宿主操作系统的内核。
>>   - 享有虚拟机的资源隔离和分配优势。
>>   - 更加轻量级、可移植性强、效率高。
>


**性能对比**

| 特性 | 容器 | 虚拟机 |
|------|------|--------|
| 启动 | 秒级 | 分钟级 |
| 硬盘使用 | 一般为MB | 一般为GB |
| 性能 | 接近原生 | 弱于原生 |
| 系统支持量 | 单机支持上千个容器 | 一般几十个 |

**容器标准**

- Open Container Initiative（OCI）
	- 轻量级开放式管理组织（项目）
- OCI 主要定义两个规范
	- Runtime Specification
		- 文件系统包如何解压至硬盘，共运行时运行。
	- Image Specification
		- 如何通过构建系统打包，生成镜像清单（Manifest）、文件系统序列化文件、镜像配置


**容器主要特性**

- 隔离性
- 安全性
- 可配额
- 便携性

### Namespace

- Linux Namespace 是一种 Linux Kernel 提供的资源隔离方案：
	- 系统可以为进程分配不同的 Namespace；
	- 并保证不同的Namespace资源独立分配、进程彼此隔离，即不同的Namespace下的进程互不干扰。

Linux 内核代码中 Namespace 的实现

- 进程数据结构
```c
struct task_struct {
...
/* namespaces */
struct nsproxy *nsproxy;
...
}
```

- Namespace 数据结构

```c
struct nsproxy {

	atomic_t count;
	struct uts_namespace *uts_ns; // 主机名和域名隔离
	struct ipc_namespace *ipc_ns; // 进程间通信资源隔离
	struct mnt_namespace *mnt_ns; // 文件系统挂载点隔离
	struct pid_namespace // 进程ID隔离
	
*pid_ns_for_children;
	struct net         *net_ns;
}
```


Linux 对 Namespace 操作方法

- clone
	在创建新进程的**系统调用**时，可以通 flags参数指定需要新建的 Namespace 类型：
```c
// CLONE_NEWCGROUP / CLONE_NEWIPC / CLONE_NEWNET / CLONE_NEWNS /
CLONE_NEWPID /
CLONE_NEWUSER / CLONE_NEWUTS
int clone(int (*fn)(void *), void *child_stack, int flags, void *arg)
```

- setns
	该系统调用可以让调用进程加入某个已经存在的 Namespace 中：
```c
Int setns(int fd, int nstype)
```
- unshare
	该系统调用可以将调用进程移动到新的Namespace 下：
```c
int unshare(int flags)
```

隔离性 – Linux Namespace 

| Namespace 类型 | 隔离资源 | Kernel版本 |
|----------------|----------|------------|
| IPC            | System V IPC和 POSIX 消息队列 | 2.6.19 |
| Network        | 网络设备、网络协议栈、网络端口等 | 2.6.29 |
| PID            | 进程 | 2.6.14 |
| Mount          | 挂载点 | 2.4.19 |
| UTS            | 主机名和域名 | 2.6.19 |
| USR            | 用户和用户组 | 3.8 |

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016005411.png" alt="image.png" style="zoom:60%;" />

Pid namespace 
- 不同用户的进程就是通过 Pid namespace 隔离开的，且不同 namespace 中可以有相同 Pid。
- 有了 Pid namespace, 每个 namespace 中的 Pid 能够相互隔离。


net namespace 
- 网络隔离是通过 net namespace 实现的， 每个 net namespace 有独立的 network devices, IP addresses, IP routing tables, `/proc/net` 目录。
- Docker 默认采用 veth 的方式将 container 中的虚拟网卡同 host 上的一个 docker bridge: docker0 连接在一起。


ipc namespace 
- Container 中进程交互还是采用 linux 常见的进程间交互方法 （interprocess communication – IPC）, 包括常见的信号量、消息队列和共享内存。
- container 的进程间交互实际上还是host上具有相同Pid namespace中的进程间交互，因此需要在 IPC资源申请时加入 namespace 信息 
- 每个 IPC 资源有一个唯一的 32 位 ID

mnt namespace 
- mnt namespace 允许不同 namespace 的进程看到的文件结构不同，这样每个 namespace 中的进程所看到的文件目录就被隔离开了。

uts namespace 
- UTS(“UNIX Time-sharing System”) namespace允许每个 container 拥有独立的 hostname 和 domain name, 使其在网络上可以被视作一个独立的节点而非 Host 上的一个进程。

user namespace 
- 每个 container 可以有不同的 user 和 group id, 也就是说可以在 container 内部用 container 内部的用户执行程序而非 Host 上的用户。


关于 namespace 的常用操作

- 查看当前系统的 namespace：
	`lsns –t <type>`
- 查看某进程的 namespace：
	`ls -la /proc/<pid>/ns/`
- 进入某 namespace 运行命令：
	`nsenter -t <pid> -n ip addr`


### Cgroups

- Cgroups （Control Groups）是 Linux 下用于对一个或一组进程进行资源控制和监控的机制；
- 可以对诸如 CPU 使用时间、内存、磁盘 I/O 等进程所需的资源进行限制；
- 不同资源的具体管理工作由相应的 Cgroup 子系统（Subsystem）来实现；
- 针对不同类型的资源限制，只要将限制策略在不同的的子系统上进行关联即可；
- Cgroups 在不同的系统资源管理子系统中以**层级树**（Hierarchy）的方式来组织管理：每个 Cgroup 都可以包含其他的子 Cgroup，因此子 Cgroup 能使用的资源除了受本 Cgroup 配置的资源参数限制，还受到父Cgroup 设置的资源限制 


Linux 内核代码中 Cgroups 的实现


- 进程数据结构

```c
struct task_struct
{
	#ifdef CONFIG_CGROUPS
	struct css_set __rcu *cgroups;
	struct list_head cg_list;
	#endif
}
```

- `css_set` 是 cgroup_subsys_state 对象的集合数据结构
```c
struct css_set {
	/*
	* Set of subsystem states, one for each subsystem. This array is
	* immutable after creation apart from the init_css_set during
	* subsystem registration (at boot time).
	*/
	struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];
};
```


可配额/可度量 - Control Groups (cgroups)


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016020104.png" alt="image.png" style="zoom:60%;" />


cgroups实现了对资源的配额和度量
- cgroups 实现了对资源的配额和度量
- blkio： 这个子系统设置限制每个块设备的输入输出控制。例如:磁盘，光盘以及 USB 等等。
- CPU： 这个子系统使用调度程序为 cgroup 任务提供 CPU 的访问。
- cpuacct： 产生 cgroup 任务的 CPU 资源报告。
- cpuset： 如果是多核心的 CPU，这个子系统会为 cgroup 任务分配单独的 CPU 和内存。
- devices： 允许或拒绝 cgroup 任务对设备的访问。
- freezer:  暂停和恢复cgroup任务
- memory:  设置每个cgroup的内存限制以及产生内存资源报告
- net_cls: 标记每个网络包以供cgroup方便使用
- ns:  名称空间子系统
- pid:  进程标识子系统


CPU 子系统

- cpu.shares： 可出让的能获得 CPU 使用时间的**相对**值。
- cpu.cfs_period_us：cfs_period_us 用来配置时间周期长度，单位为 us（微秒）。
- cpu.cfs_quota_us：cfs_quota_us 用来配置当前 Cgroup 在 cfs_period_us 时间内最多能使用的 CPU 时间数，单位为 us（微秒）。
- cpu.stat ： Cgroup 内的进程使用的 CPU 时间统计。
- nr_periods ：经过 cpu.cfs_period_us 的时间周期数量。
- nr_throttled ： 在经过的周期内，有多少次因为进程在指定的时间周期内用光了配额时间而受到限制。
- throttled_time ： Cgroup 中的进程被限制使用 CPU 的总用时，单位是 ns（纳秒）。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016022736.png" alt="image.png" style="zoom:60%;" />

Linux 调度器
内核默认提供了5个调度器，Linux 内核使用 struct sched_class 来对调度器进行抽象：
- Stop 调度器，stop_sched_class：优先级最高的调度类，可以抢占其他所有进程，不能被其他进程抢占； 最高优先级调度算法
- Deadline 调度器，dl_sched_class：使用红黑树，把进程按照绝对截止期限进行排序，选择最小进程进行调度运行；最短作业优先的变体
- RT 调度器， rt_sched_class：实时调度器，为每个优先级维护一个队列； FCFS和Round Robin
- **CFS 调度器， cfs_sched_class：完全公平调度器，采用完全公平调度算法，引入虚拟运行时间概念；**
- IDLE-Task 调度器， idle_sched_class：空闲调度器，每个 CPU 都会有一个 idle 线程，当没有其他进程可以调度时，调度运行 idle 线程。


CFS 调度器
- CFS 是 Completely Fair Scheduler 简称，即完全公平调度器。
- CFS 实现的主要思想是维护为任务提供处理器时间方面的平衡，这意味着应给进程分配相当数量的处理器。
- 分给某个任务的时间失去平衡时，应给失去平衡的任务分配时间，让其执行。
- CFS 通过虚拟运行时间（vruntime）来实现平衡，维护提供给某个任务的时间量。
	- vruntime = 实际运行时间`*`1024 / 进程权重
- 进程按照各自不同的速率在物理时钟节拍内前进，优先级高则权重大，其虚拟时钟比真实时钟跑得慢，但获得比较多的运行时间。

vruntime 红黑树
CFS 调度器没有将进程维护在运行队列中，而是维护了一个以虚拟运行时间为顺序的红黑树。 红黑树的主要特点有：
1. 自平衡，树上没有一条路径会比其他路径长出俩倍。
2. O(log n) 时间复杂度，能够在树上进行快速高效地插入或删除进程。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016024056.png" alt="image.png" style="zoom:60%;" />

**CFS进程调度**
- 在时钟周期开始时，调度器调用 `__schedule()` 函数来开始调度的运行。
- `__schedule()` 函数调用 `pick_next_task()` 让进程调度器从就绪队列中选择一个最合适的进程 next，即红黑树最左边的节点。
- 通过 `context_switch()` 切换到新的地址空间，从而保证 next 进程运行。
- 在时钟周期结束时，调度器调用 `entity_tick()` 函数来更新进程负载、进程状态以及 `vruntime`（当前vruntime + 该时钟周期内运行的时间）。
- 最后，将该进程的虚拟时间与就绪队列红黑树中最左边的调度实体的虚拟时间做比较，如果小于坐左边的时间，则不用触发调度，继续调度当前调度实体。


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016024547.png" alt="image.png" style="zoom:70%;" />


**cpuacct 子系统**
用于统计 Cgroup 及其子 Cgroup 下进程的 CPU 的使用情况。
- cpuacct.usage
包含该 Cgroup 及其子 Cgroup 下进程使用 CPU 的时间，单位是 ns（纳秒）。
- cpuacct.stat
包含该 Cgroup 及其子 Cgroup 下进程使用的 CPU 时间，以及用户态和内核态的时间。


Memory 子系统
- memory.usage_in_bytes
	cgroup 下进程使用的内存，包含 cgroup 及其子 cgroup 下的进程使用的内存
- memory.max_usage_in_bytes
	cgroup 下进程使用内存的最大值，包含子 cgroup 的内存使用量。
- memory.limit_in_bytes
	设置 Cgroup 下进程最多能使用的内存。如果设置为 -1，表示对该 cgroup 的内存使用不做限制。
- memory.soft_limit_in_bytes
	这个限制并不会阻止进程使用超过限额的内存，只是在系统内存足够时，会优先回收超过限额的内存，使之向限定值靠拢。
- memory.oom_control
	**设置是否在 Cgroup 中使用 OOM（Out of Memory）Killer，默认为使用。当属于该 cgroup 的进程使用的内存超过最大的限定值时，会立刻被 OOM Killer 处理。**

**Cgroup driver**
systemd:
- 当操作系统使用 systemd 作为 init system 时，初始化进程生成一个根 cgroup 目录结构并作为 cgroup管理器。
- systemd 与 cgroup 紧密结合，并且为每个 systemd unit 分配 cgroup。
cgroupfs:
- docker 默认用 cgroupfs 作为 cgroup 驱动。
存在问题：
- 在 systemd 作为 init system 的系统中，默认并存着两套 groupdriver。
- 这会使得系统中 Docker 和 kubelet 管理的进程被 cgroupfs 驱动管，而 systemd 拉起的服务由systemd驱动管，让cgroup管理混乱且容易在资源紧张时引发问题。
因此 kubelet 会默认`--cgroup-driver=systemd`，若运行时 cgroup 不一致时，kubelet 会报错。


### 文件系统
Union FS
- 将不同目录挂载到同一个虚拟文件系统下（unite several directories into a single virtual filesystem）的文件系统
- 支持为每一个成员目录（类似Git Branch）设定 readonly、readwrite 和 whiteout-able 权限
- 文件系统分层, 对 readonly 权限的 branch 可以逻辑上进行修改(增量地, 不影响 readonly 部分的)。
- 通常 Union FS 有两个用途, 一方面可以将多个 disk 挂到同一个目录下, 另一个更常用的就是将一个readonly 的 branch 和一个 writeable 的 branch 联合在一起。


容器镜像

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016091513.png" alt="image.png" style="zoom:40%;" />

增量构建：
- 每个Dockerfile在共享层之上添加了特定的层（如复制app.jar或下载elasticsearch）。这种增量构建方式使得更新应用变得非常高效，只需要修改或重建变化的层。


典型的 Linux 文件系统组成：
- Bootfs（boot file system）
	- Bootloader - 引导加载 kernel，
	- Kernel - 当 kernel 被加载到内存中后 umount bootfs。
- rootfs （root file system）
	- `/dev，/proc，/bin，/etc` 等标准目录和文件。
	- 对于不同的 linux 发行版, bootfs 基本是一致的，但 rootfs 会有差别。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016092026.png" alt="image.png" style="zoom:60%;" />


Docker启动 
Linux 
- 在启动后,首先将rootfs设置为readonly,进行一系列检查,然后将其切换为"readwrite"供用户使用。 
Docker启动 
- 初始化时也是将rootfs以readonly方式加载并检查,然而接下来利用union mount的方式将一个readwrite文件系统挂载在readonly的rootfs之上; 
- 并且允许再次将下层的FS(file system)设定为readonly并且向上叠加。
- 这样一组readonly和一个writeable的结构构成一个container的运行时态,每一个FS被称作一个FS 层。

统一视图：
- Union Mount 技术提供了一个统一的文件系统视图。
- 当您查看文件系统时，您看到的是所有层叠加后的结果。
- 您无法直接看到哪些文件来自只读层，哪些是在可写层新创建或修改的。


写操作
由于镜像具有共享特性，所以对容器可写层的操作需要依赖存储驱动提供的写时复制和用时分配机制，以此来支持对容器可写层的修改，进而提高对存储和内存资源的利用率。
- 写时复制
写时复制，即 Copy-on-Write。一个镜像可以被多个容器使用，但是不需要在内存和磁盘上做多个拷贝。在需要对镜像提供的文件进行修改时，该文件会从镜像的文件系统被复制到容器的可写层的文件系统进行修改，而镜像里面的文件不会改变。不同容器对文件的修改都相互独立、互不影响。
- 用时分配
按需分配空间，而非提前分配，即当一个文件被创建出来后，才会分配空间。


容器存储驱动

|     存储驱动      |       Docker       |             Containerd              |
| :-----------: | :----------------: | :---------------------------------: |
|     AUFS      | 在Ubuntu或者Debian上支持 |                 不支持                 |
|   OverlayFS   |         支持         |                 支持                  |
| Device Mapper |         支持         |                 支持                  |
|     Btrfs     |         支持         | 社区版本在Ubuntu或者Debian上支持，企业版本在SLES上支持 |
|      ZFS      |         支持         |                 不支持                 |


|     存储驱动      |                       优点                       |                                      缺点                                       |          应用场景          |
| :-----------: | :--------------------------------------------: | :---------------------------------------------------------------------------: | :--------------------: |
|     AUFS      |             Docker 最早支持的驱动类型，稳定性高              |        并未进入主线的内核，因此只能在有限的场合下使用。另外在实现上具有多层结构，在层比较多的场景下，做写时复制有时会需要比较长的时间        |       少 I/O 的场景        |
|   OverlayFS   |  并入主线内核，可以在目前几乎所有发行版本上使用。实现上只有两层，因此性能比 AUFS 高  | 写时复制机制需要复制整个文件，而不能只针对修改部分进行复制，因此对大文件操作会需要比较长的时间。其中 Overlay 在 Docker 的后续版本中被移除 |       少 I/O 的场景        |
| Device Mapper |     并入主线内核，针对块操作，性能比较高。修改文件时只需复制需要修改的块，效率高     |                       不同容器之间不能共享缓存。在 Docker 的后续版本中会被移除                        |        I/O 密集场景        |
|     BtrFS     |         并入主线内核，虽然是文件级操作系统，但是可以对块进行操作。          |                              需要消耗比较多的内存，稳定性相对比较差                              | 需要支持 Snapshot 等比较特殊的场景 |
|      ZFS      | 不同的容器之间可以共享缓存，多个容器访问相同的文件能够共享一个一一的 Page Cache。 |                 在频繁写操作的场景下，会产生比较严重的磁盘碎片。需要消耗比较多的内存，另外稳定性相对比较差                 |       容器高密度部署的场景       |

以 OverlayFS 为例

OverlayFS 也是一种与 AUFS 类似的联合文件系统，同样属于文件级的存储驱动，包含了最初的 Overlay 和更新更稳定的 overlay2。
**Overlay 只有两层：upper 层和 lower 层，Lower 层代表镜像层，upper 层代表容器可写层。**

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016094424.png" alt="image.png" style="zoom:60%;" />


OCI 容器标准
Open Container Initiative
- OCI 组织于 2015 年创建，是一个致力于定义容器镜像标准和运行时标准的开放式组织。
- OCI 定义了镜像标准（Runtime Specification）、运行时标准（Image Specification）和分发标准（Distribution Specification）
	- 镜像标准定义应用如何打包
	- 运行时标准定义如何解压应用包并运行
	- 分发标准定义如何分发容器镜像


### Docker 引擎架构

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016095802.png" alt="image.png" style="zoom:60%;" />


网络
Null(--net=None)
- 把容器放入独立的网络空间但不做任何网络配置；
- 用户需要通过运行docker network命令来完成网络配置。
Host
- 使用主机网络名空间，复用主机网络。
Container
- 重用其他容器的网络。
Bridge(--net=bridge)
- 使用Linux网桥和iptables提供容器互联，Docker在每台主机上创建一个名叫docker0的网桥，通过veth pair来连接该主机的每一个EndPoint。

Overlay(libnetwork,libkv)
- 通过网络封包实现。
Remote(work with remote drivers)
- Underlay :
	- 使用现有底层网络,为每一个容器配置可路由的网络IP。
- Overlay:
	- 通过网络封包实现。


Null 模式


- Null 模式是一个空实现；
- 可以通过 Null 模式启动容器并在宿主机上通过命令为容器配置网络。

```c
mkdir -p /var/run/netns
find -L /var/run/netns -type l -delete
ln -s /proc/$pid/ns/net /var/run/netns/$pid
ip link add A type veth peer name B
brctl addif br0 A
ip link set A up
ip link set B netns $pid
ip netns exec $pid ip link set dev B name eth0
ip netns exec $pid ip link set eth0 up
ip netns exec $pid ip addr add
$SETIP/$SETMASK dev eth0
ip netns exec $pid ip route add default via
$GATEWAY
```

默认模式– 网桥和 NAT

为主机 eth0 分配 IP 192.168.0.101；
启动 docker daemon，查看主机 iptables；
- `POSTROUTING -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE`
在主机启动容器：
- `docker run -d --name ssh -p 2333:22 centos-ssh`
- Docker 会以标准模式配置网络：
	- 创建 veth pair；
	- 将 veth pair的一端连接到 docker0 网桥；
	- veth pair 的另外一端设置为容器名空间的 eth0；
	- 为容器名空间的 eth0 分配 ip；
	- 主机上的 Iptables 规则：`PREROUTING -A DOCKER ! -i docker0 -p tcp -m tcp --dport 2333 -j DNAT --to- destination 172.17.0.2:22`。


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016101949.png" alt="image.png" style="zoom:60%;" />

Underlay
- 采用 Linux 网桥设备（sbrctl），通过物理网络连通容器；
- 创建新的网桥设备 mydr0；
- 将主机网卡加入网桥；
- 把主机网卡的地址配置到网桥，并把默认路由规则转移到网桥 mydr0；
- 启动容器；
- 创建 veth 对，并且把一个 peer 添加到网桥 mydr0；
- 配置容器把 veth 的另一个 peer 分配给容器网卡

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016104916.png" alt="image.png" style="zoom:70%;" />

Docker Libnetwork Overlay
- Docker overlay 网络驱动原生支持多主机网络；
- Libnetwork 是一个内置的基于 VXLAN 的网络驱动。

VXLAN

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016190852.png" alt="image.png" style="zoom:60%;" />

Overlay network sample – Flannel


- 同一主机内的 Pod 可以使用网桥进行通信。
- 不同主机上的 Pod 将通过flanneld将其流量封装在UDP数据包中。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016190957.png" alt="image.png" style="zoom:60%;" />

Pod（豆荚）:
- Pod是Kubernetes中最小的可部署单元。
- 它可以包含一个或多个紧密关联的容器。
- 这些容器共享网络命名空间、IP地址和端口空间。
- 可以理解为一个逻辑主机，里面运行着密切相关的应用组件。
- 例如：一个Pod可能包含一个Web服务器容器和一个数据库容器。

Flanneld:
- Flanneld是Flannel网络方案的核心组件。
- Flannel是一个为Kubernetes设计的网络解决方案。
- Flanneld在每个集群节点上运行，创建和管理一个大的虚拟网络。
- 主要功能：  
    a. 为每个Pod分配唯一的IP地址。  
    b. 在不同节点的Pod之间创建一个Overlay网络。  
    c. 封装/解封装跨节点的网络流量。
- 使得不同节点上的Pod可以直接通信，就像在同一个局域网内。


Flannel packet sample 

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241016191808.png" alt="image.png" style="zoom:80%;" />

创建 docker 镜像
定义 dockerfile

```dockerfile
FROM ubuntu
# so apt-get doesn't complain
ENV DEBIAN_FRONTEND=noninteractive
RUN sed -i 's/^exit 101/exit 0/' /usr/sbin/policy-rc.d
RUN \
apt-get update && \
apt-get install -y ca-certificates && \
apt-get install -y curl && \
rm -rf /var/lib/apt/lists/*
ADD ./bin/eic eic
ENTRYPOINT ["/eic"]
```
docker build



### Dockerfile 的最佳实践

回顾12Factor之进程
- 运行环境中，应用程序通常是以一个和多个**进程**运行的。
	- 12-Factor 应用的进程必须无状态（Stateless）且无共享（Share nothing）。
- 任何需要持久化的数据都要存储在后端服务内，比如数据库。
	- 应在构建阶段将源代码编译成待执行应用。
- Session Sticky 是 12-Factor 极力反对的。
	- Session 中的数据应该保存在诸如 Memcached 或 Redis 这样的带有过期时间的缓存中。

Docker 遵循以上原则管理和构建应用。


**理解构建上下文（Build Context）**
- 当运行 docker build 命令时，当前工作目录被称为构建上下文。
- docker build 默认查找当前目录的 Dockerfile 作为构建输入，也可以通过 –f 指定 Dockerfile。
	```
	docker build –f ./Dockerfile
	```
- 当 docker build 运行时，首先会把构建上下文传输给 docker daemon，把没用的文件包含在构建上下文时，会导致传输时间长，构建需要的资源多，构建出的镜像大等问题。
- 试着到一个包含文件很多的目录运行下面的命令，会感受到差异；
	- `docker build -f $GOPATH/src/github.com/cncamp/golang/httpserver/Dockerfile；`
	- `docker build $GOPATH/src/github.com/cncamp/golang/httpserver/；`
	- `可以通过.dockerignore文件从编译上下文排除某些文件`。
- 因此需要确保构建上下文清晰，比如创建一个专门的目录放置 Dockerfile，并在目录中运行 docker build。


镜像构建日志
```sh
docker build $GOPATH/src/github.com/cncamp/golang/httpserver/
Sending build context to Docker daemon  14.57MB
Step 1/4 : FROM ubuntu
---> cf0f3ca922e0
Step 2/4 : ENV MY_SERVICE_PORT=80
---> Using cache
---> a7d824f74410
Step 3/4 : ADD bin/amd64/httpserver /httpserver
---> Using cache
---> 00bb47fce704
Step 4/4 : ENTRYPOINT /httpserver
---> Using cache
---> f77ee3366d08
Successfully built f77ee3366d08
```


Build Cache
构建容器镜像时，Docker 依次读取 `Dockerfile `中的指令，并按顺序依次执行构建指令。
Docker 读取指令后，会先**判断缓存**中是否有可用的已存镜像，只有已存镜像不存在时才会重新构建。
- 通常 Docker 简单判断Dockerfile中的指令与镜像。
- 针对 ADD 和 COPY 指令，Docker 判断该镜像层每一个文件的内容并生成一个 **checksum**，与现存镜像比较时，Docker 比较的是二者的 checksum。
- 其他指令，比如` RUN apt-get -y update，Docker` 简单比较与现存镜像中的指令字串是否一致。
- 当某一层 cache 失效以后，所有所有层级的 cache 均一并失效，后续指令都重新构建镜像。


多段构建（Multi-stage build）
- 有效减少镜像层级的方式
```dockerfile
FROM golang:1.16-alpine AS build
RUN apk add --no-cache git
RUN go get github.com/golang/dep/cmd/dep

COPY Gopkg.lock Gopkg.toml /go/src/project/
WORKDIR /go/src/project/
RUN dep ensure -vendor-only

COPY . /go/src/project/
RUN go build -o /bin/project（只有这个二进制文件是产线需要的，其他都是waste）

FROM scratch
COPY --from=build /bin/project /bin/project
ENTRYPOINT ["/bin/project"]
CMD ["--help"]
```


Dockerfile 常用指令
- FROM：选择基础镜像，推荐 alpine
	`FROM [--platform=<platform>] <image>[@<digest>] [AS <name>]`
- LABELS：按标签组织项目
	`LABEL multi.label1="value1" multi.label2="value2" other="value3”`
	配合 label filter 可过滤镜像查询结果
	docker images -f label=multi.label1="value1"
- RUN：执行命令
	最常见的用法是 `RUN apt-get update && apt-get install`，这两条命令应该永远用&&连接，如果分开执行，`RUN apt-get update` 构建层被缓存，可能会导致新 package 无法安装
- CMD：容器镜像中包含应用的运行命令，需要带参数
	`CMD ["executable", "param1", "param2"…]`

- EXPOSE：发布端口
	`EXPOSE <port> [<port>/<protocol>...]`
	- 是镜像创建者和使用者的约定
	- 在 `docker run –P` 时，docker 会自动映射 expose 的端口到主机大端口，如`0.0.0.0:32768->80/tcp`
- ENV 设置环境变量
	`ENV <key>=<value> ...`


- ADD：从源地址（文件，目录或者 URL）复制文件到目标路径
	- ``ADD [--chown=<user>:<group>] <src>... <dest>``
	- `ADD [--chown=<user>:<group>] [“<src>”,... “<dest>”]` （路径中有空格时使用）
- ADD 支持 Go 风格的通配符，如 `ADD check* /testdir/`
- src 如果是文件，则必须包含在编译上下文中，ADD 指令无法添加编译上下文之外的文件
- src 如果是 URL
	- 如果 `dest` 结尾没有`/`，那么 `dest` 是目标文件名，如果 `dest` 结尾有`/`，那么 `dest` 是目标目录名
- 如果 src 是一个目录，则所有文件都会被复制至 dest
- 如果 src 是一个本地压缩文件，则在 ADD 的同时完整解压操作
- 如果 dest 不存在，则 ADD 指令会创建目标目录
- 应尽量减少通过 `ADD URL` 添加 remote 文件，建议使用 curl 或者 `wget && untar`

- COPY：从源地址（文件，目录或者URL）复制文件到目标路径
	- `COPY [--chown=<user>:<group>] <src>... <dest>`
	- `COPY [--chown=<user>:<group>] ["<src>",... "<dest>"]` // 路径中有空格时使用
		- COPY 的使用与 ADD 类似，但有如下区别
		- COPY 只支持本地文件的复制，不支持 URL
		- COPY 不解压文件
		- COPY 可以用于多阶段编译场景，可以用前一个临时镜像中拷贝文件
			- `COPY --from=build /bin/project /bin/project`
- COPY 语义上更直白，复制本地文件时，优先使用 COPY

- ENTRYPOINT：定义可以执行的容器镜像入口命令
	- `ENTRYPOINT ["executable", "param1", "param2"]` // docker run参数追加模式
	- `ENTRYPOINT command param1 param2` // docker run 参数替换模式
		- `docker run –entrypoint` 可替换 Dockerfile 中定义的 ENTRYPOINT
		- ENTRYPOINT 的最佳实践是用 ENTRYPOINT 定义镜像主命令，并通过 CMD 定义主要参数，如下所示
		- `ENTRYPOINT ["s3cmd"]`
		- `CMD ["--help"]`


- VOLUME： 将指定目录定义为外挂存储卷，Dockerfile 中在该指令之后所有对同一目录的修改都无效
	- `VOLUME ["/data"]`
	- 等价于 `docker run –v /data`，可通过 `docker inspect` 查看主机的 `mount point`，
	- `/var/lib/docker/volumes/<containerid>/_data`
- USER：切换运行镜像的用户和用户组，因安全性要求，越来越多的场景要求容器应用要以 non-root 身份运行
	- `USER <user>[:<group>]`
- WORKDIR：等价于 cd，切换工作目录
	- `WORKDIR /path/to/workdir`
- 其他非常用指令
	- ARG
	- ONBUILD
	- STOPSIGNAL
	- HEALTHCHECK
	- SHELL

- 不要安装无效软件包。
- 应简化镜像中同时运行的进程数，理想状况下，每个镜像应该只有一个进程。
- 当无法避免同一镜像运行多进程时，应选择合理的初始化进程（init process）。
- 最小化层级数
	- 最新的 docker 只有 RUN， COPY，ADD 创建新层，其他指令创建临时层，不会增加镜像大小。
		- 比如 EXPOSE 指令就不会生成新层。
	- 多条 RUN 命令可通过连接符连接成一条指令集以减少层数。
	- 通过多段构建减少镜像层数。
- 把多行参数按字母排序，可以减少可能出现的重复参数，并且提高可读性。
- 编写 dockerfile 的时候，应该把变更频率低的编译指令优先构建以便放在镜像底层以有效利用 build cache。
- 复制文件时，每个文件应独立复制，这确保某个文件变更时，只影响改文件对应的缓存。

多进程的容器镜像
- 选择适当的 init 进程
	- 需要捕获 SIGTERM 信号并完成子进程的优雅终止
	- 负责清理退出的子进程以避免僵尸进程
开源项目
https://github.com/krallin/tini


基于 Docker 镜像的版本管理

- Docker tag
- docker tag 命令可以为容器镜像添加标签
- `docker tag 0e5574283393 hub.docker.com/cncamp/httpserver:v1.0`
- hub.docker.com： 镜像仓库地址，如果不填，则默认为 hub.docker.com
- cncamp: repositry
- httpserver：镜像名
- v1.0：tag，常用来记录版本信息


Docker tag 与 github 的版本管理合力


- 以 Kubernetes 为例
	- 开发分支
		- git checkout master
	- Release 分支
		- git checkout –b release-1.21
	- 在并行期，所有的变更同时放进 master 和 release branch
	- 版本发布
		- 以 release branch 为基础构建镜像，并为镜像标记版本信息：docker tag 0e5574283393 
	- k8s.io/kubernetes/apiserver:v1.21
	- 在 github 中保存 release 代码快照
		- git tag v1.21

Docker 优势

**封装性**：
- 不需要再启动内核，所以应用扩缩容时可以秒速启动。
- 资源利用率高，直接使用宿主机内核调度资源，性能损失小。
- 方便的 CPU、内存资源调整。
- 能实现秒级快速回滚。

**镜像增量分发**：
- 由于采用了 Union FS， 简单来说就是支持将不同的目录挂载到同一个虚拟文件系统下，并实现一种 layer 的概念，每次发布只传输变化的部分，节约带宽


**封装性**：
- 一键启动所有依赖服务，测试不用为搭建环境犯愁，PE 也不用为建站复杂担心。
- 镜像一次编译，随处使用。
- 测试、生产环境高度一致（数据除外）。

**隔离性**：
- 应用的运行环境和宿主机环境无关，完全由镜像控制，一台物理机上部署多种环境的镜像测试。
- 多个应用版本可以并存在机器上。

**社区活跃**：
- Docker 命令简单、易用，社区十分活跃，且周边组件丰富


# Kubernetes 架构原则和对象设计

## 云计算

### 什么是云计算？

云计算是对网络资源、计算资源、存储资源的一种抽象，云计算本质上是一种按需提供的计算模型，它通过网络访问一个可配置的**共享计算资源池**（如网络、服务器、存储、应用程序和服务），这些资源能够被快速供应和释放，将最小化管理工作量或服务提供商的**交互**。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241018212409.png" alt="image.png" style="zoom:60%;" />
1. 云业务层：  
	包含多个工作负载(Workload)，这些是运行在云平台上的各种应用和服务。

### 云计算平台的分类
- 以 Openstack 为典型的虚拟化平台
	- 虚拟机构建和业务代码部署分离。
	- 可变的基础架构使后续维护风险变大。
- 以谷歌borg为典型的**基于进程的作业调度**平台
	- 技术的迭代引发borg的换代需求。
		- 早期的**隔离**依靠 chroot jail 实现
		- 一些不合理的设计需要在新产品中改进。
			- 对象之间的强依赖job 和 task 是强包含关系，不利于重组。
			- 所有容器共享IP，会导致端口冲突，隔离困难等问题。
			- 为超级用户添加复杂逻辑导致系统过于复杂。




## Kubernetes 架构基础

### Borg
<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241018214518.png" alt="image.png" style="zoom:60%;" />

- 顶层应用服务：
    - Gmail：Google的电子邮件服务
    - Google Docs：在线文档编辑服务
    - Web Search：Google的网络搜索引擎
    
    这些是运行在Borg之上的面向用户的主要服务。
    
- 中层基础设施组件：
    
    - Flumjava：可能是Google的大规模日志处理系统
    - MillWheel：Google的流处理框架
    - Pregel：大规模图处理系统
    - GFS/CFS：Google文件系统/集群文件系统
    - Bigtable：Google的分布式存储系统
    - Megastore：结构化数据的存储系统
    - MapReduce：Google的分布式计算框架
    这些组件是Google的核心基础设施，为上层应用提供各种功能支持。
- 底层：Borg  
    Borg作为底层的集群管理和资源**调度系统**，支撑着所有上层的服务和基础设施组件。

### Google Borg 简介

特性
- 物理资源利用率高。
- 服务器共享，在进程级别做隔离。
- 应用高可用，故障恢复时间短。
- 调度策略灵活。
- 应用接入和使用方便，提供了完备的Job描述语言，服务发现，实时状态监控和诊断工具。

优势
- 对外隐藏底层资源管理和调度、故障处理等。
- 实现应用的高可靠和高可用。
- 足够弹性，支持应用跑在成千上万的机器上。


基本概念

Workload 
- prod:在线任务,长期运行、对延时敏感、面向终端用户等,比如 Gmail, Google Docs, Web Search服务等。 
- non-prod:离线任务,也称为批处理任务 (Batch),比如一些 分布式计算服务等。

Cell 
- 一个Cell上跑一个集群管理系统Borg。 
- 通过定义Cell可以让Borg对服务器资源进行统一抽象,作为用户就无需知道自己的应用跑在哪台机器上,也不用关心资源分配、程序安装、依赖管理、健康检查及故障恢复等。

Job和Task 
- 用户以Job的形式提交应用部署请求。一个Job包含一个或多个相同的Task,每个Task运行相同的应用程序, Task数量就是应用的副本数。 
- 每个Job可以定义属性、元信息和优先级, 优先级涉及到抢占式调度过程。

Naming 
- Borg的服务发现通过BNS(Borg Name Service)来实现。 
- `50.jfoo.ubar.cc.borg .google.com`可表示 在一个名为cc的Cell中由用户uBar部署的 一个名为jFoo的Job下的第50个Task。

### Borg 架构

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241018220339.png" alt="image.png" style="zoom:70%;" />


**Borgmaster 主进程**：
- 处理客户端RPC请求，比如创建Job，查询Job等。
- 维护系统组件和服务的状态，比如服务器、Task等。
- 负责与Borglet通信。
**Scheduler 进程**：
- 调度策略
	- Worst Fit
	- Best Fit
	- Hybrid
- 调度优化
	- `Score caching`: 当服务器或者任务的状态未发生变更或者变更很少时，直接采用缓存数据，避免重复计算。
	- `Equivalence classes`: 调度同一 Job 下多个相同的 Task 只需计算一次。
	- `Relaxed randomization`: 引入一些随机性，即每次随机选择一些机器，只要符合需求的服务器数量达到一定值时，就可以停止计算，无需每次对Cell中所有服务器进行`feasibility checking`。
**Borglet**：
- Borglet是部署在所有服务器上的**Agent**，负责接收`Borgmaster`进程的指令。

应用高可用
- 被抢占的 non-prod 任务放回 pending queue，等待重新调度。
- **多副本应用跨故障域部署**。所谓故障域有大有小，比如相同机器、相同机架或相同电源插座等，一挂全挂。
- 对于类似服务器或操作系统升级的维护操作，避免大量服务器同时进行。
- 支持**幂等性**，支持客户端重复操作。
- **当服务器状态变为不可用时，要控制重新调度任务的速率**。因为 Borg 无法区分是节点故障还是出现了短暂的
- 网络分区，如果是后者，静静地等待网络恢复更利于保障服务可用性。
- 当某种“任务 @ 服务器”的组合出现故障时，下次重新调度时需避免这种组合再次出现，因为极大可能会再次出现相同故障。
- **记录详细的内部信息，便于故障排查和分析**。
- **保障应用高可用的关键性设计原则**：无论何种原因，即使 Borgmaster 或者 Borglet 挂掉、失联，都不能杀掉正在运行的服务（Task）。


Borg系统自身高可用
- Borgmaster组件**多副本**设计(冗余组件)。
- 采用一些简单的和底层（low-level）的工具来部署Borg系统实例，避免引入过多的外部依赖。
- 每个Cell的Borg均独立部署，避免不同 Borg 系统相互影响。

**资源利用率**
- 通过将在线任务（prod）和离线任务（non-prod，Batch）混合部署，空闲时，离线任务可以充分利用计算资源；繁忙时，在线任务通过抢占的方式保证优先得到执行，合理地利用资源。
- 98% 的服务器实现了混部。
- 90% 的服务器中跑了超过25  Task 和 4500 个线程。
- 在一个中等规模的Cell里，在线任务和离线任务独立部署比混合部署所需的服务器数量多出约20%-30%。可以简单算一笔账，Google的服务器数量在千万级别，按20%算也是百万级别，大概能省下的服务器采购费用就是百亿级别了，这还不包括省下的机房等基础设施和电费等费用。


### Brog 调度原理


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241020190111.png" alt="image.png" style="zoom:60%;" />
- 实际使用资源（绿色区域）：  
    这表示任务实际消耗的资源量。它是任务运行时真正使用的计算资源。
- 保留资源（蓝色区域）：  
    这是为任务预留的资源，可能包括实际使用的资源和一些**额外缓冲**。这确保了任务有足够的资源来应对需求波动。
- 回收资源（黄色区域）：  
    这部分资源虽然分配给了任务，但实际上并未被使用。Borg可以将这些**资源回收**并分配给其他任务，提高整体资源利用率。 
- 限制资源（整个矩形区域）：  
    这代表了分配给任务的最大资源限制。任务不能超过这个限制使用资源。
- 资源回收机制：  
    图中的注释指出，在任务启动300秒后，Borg会进行资源回收工作。它会重新计算保留资源，将其设置为"实际使用资源 + 安全资源"。这个过程每隔几秒就会重新计算一次。
- 动态调整（红色箭头）：  
    这个箭头表示资源分配的动态性质。随着时间推移和任务需求的变化，各种资源类别（实际使用、保留、回收）的比例会不断调整。


#### 隔离性
**安全性隔离**：
- 早期采用 Chroot jail，后期版本基于 Namespace。
**性能隔离**：
- 采用基于Cgroup的容器技术实现。
- 在线任务（prod）是延时敏感（latency-sensitive）型的，优先级高，而离线任务（non-prod，Batch）优先级低。
- Borg 通过**不同优先级之间的抢占式调度**来优先保障在线任务的性能，牺牲离线任务。
- Borg 将资源类型分成两类：
	- 可压榨的（compressible），CPU 是可压榨资源，资源耗尽不会终止进程；
	- 不可压榨的（non-compressible），内存是不可压榨资源，资源耗尽进程会被终止。



#### 什么是 Kubernetes（K8s）

Kubernetes 是谷歌开源的**容器集群管理系统**，是 Google多年大规模容器管理技术 Borg 的开源版本，主要功能包括：
- 基于容器的应用部署、维护和滚动升级；
- 负载均衡和服务发现；
- 跨机器和跨地区的集群调度；
- 自动伸缩；
- 无状态服务和有状态服务；
- 插件机制保证扩展性。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241020190726.png" alt="image.png" style="zoom:60%;" />

- Kubelet：运行在每个节点上的代理，负责管理容器
    - CRI (Container Runtime Interface)：容器运行时接口，允许使用不同的容器运行时
    - CRI shim：CRI的适配层，连接Kubelet和具体的容器运行时
    - Container runtime：实际运行容器的组件
    - Container：运行的容器实例
- 通信流程：
    - Kubelet通过gRPC client与CRI shim通信
    - CRI shim通过gRPC server接收指令，并与container runtime交互
    - Container runtime负责实际创建和管理容器

Kubernetes如何工作：
- 资源管理：K8s将集群的计算资源（CPU、内存、存储等）抽象化，形成一个大型的资源池。
- 容器调度：它决定在集群中的哪些节点上运行哪些容器，以优化资源利用。
- 自动化操作：K8s自动处理容器的部署、扩展、故障恢复等任务。

为什么需要Kubernetes：
- 大规模管理：当你有成百上千的容器和大量服务器时，手动管理变得不可能。K8s提供了自动化解决方案。
- 资源优化：K8s能更有效地利用集群资源，提高整体效率。
- 一致性和可靠性：无论在开发、测试还是生产环境，K8s都能提供一致的运行环境。


#### 命令式（ Imperative）vs 声明式（ Declarative）

命令式系统关注 “如何做”
在软件工程领域，命令式系统是写出解决某个问题、完成某个任务或者达到某个目标的明确步骤。此方法明确写出系统应该执行某指令，并且期待系统返回期望结果。

声明式系统关注“做什么”
在软件工程领域，声明式系统指程序代码描述系统应该做什么而不是怎么做。仅限于描述要达到什么目的，如何达到目的交给系统。


**声明式（Declaritive）系统规范**
命令式:
- 我要你做什么,怎么做,请严格按照我说的做。
声明式:
- 我需要你帮我做点事,但是我只告诉你我需要你做什么,不是你应该怎么做。
- 直接声明:我直接告诉你我需要什么。
- 间接声明:我不直接告诉你我的需求,我会把我的需求放在特定定的地方,请在方便的时候拿出来处理。
幂等性:
- 状态固定,每次我要你做事,请给我返回相同结果。
面向对象的:
- 把一切抽象成对象。


Kubernetes:声明式系统
Kubernetes的所有管理能力构建在对象抽象的基础上,核心对象包括:
- **Node**:计算节点的抽象,用来描述计算节点的资源抽象、健康状态等。
- **Namespace**:资源隔离的基本单位,可以简单理解为文件系统中中的目录结构。
- **Pod**:用来描述应用实例,包括镜像地址、资源需求等。Kubernetes中最核心的对象,也是打通应用和基础架构的秘密武器。
- **Service**:服务如何将应用发布成服务,本质上是负载均衡和域名服务的声明。


#### Kubernetes 采用与 Borg 类似的架构

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241020204829.png" alt="image.png" style="zoom:60%;" />

1. K8s Master（主节点）:
   - API Server: 所有组件和外部请求的交互中心
   - scheduler: 负责Pod的调度
   - Controllers: 管理各种控制器（如Deployment, ReplicaSet等）(一直回去监控当前的集群状态，如果监控的对象发生了变化，控制器就会去做一些配置的操作，最终是为了确保高可用)
   - etcd: 分布式键值存储，保存集群的所有配置和状态信息
2. Kubelet: 
   运行在每个工作节点上，负责管理该节点上的容器,当请求来了，会检测pod的镜像的所需要的资源，去调容器接口(比如docker run什么的)

3. 外部接口:
   - kubectl: 命令行工具，用于与API Server交互
   - Dashboard: Web界面，用于可视化管理集群
   - Config files & Images: 配置文件和容器镜像

4. Container Registry:
   存储容器镜像的仓库

5. Web browsers:
   用于访问Dashboard的浏览器

这种架构设计允许Kubernetes实现高度的可扩展性和灵活性：
- 中央控制平面（Master）管理整个集群
- 工作节点（Kubelet）执行实际的容器运行
- API Server作为统一的接口，便于扩展和集成
- etcd提供可靠的数据存储
- 各种控制器实现不同的自动化功能


#### 主要组件

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241020215951.png" alt="image.png" style="zoom:80%;" />

##### Kubernetes 的主节点（Master Node）

- **API服务器API Server**：这是 Kubernetes 控制面板中唯一带有用户可访问 API 以及用户可交互的组件。API服务器会暴露一个 RESTful 的Kubernetes API并使用JSON格式的清单文件（manifest files）。
- **群的数据存储Cluster Data Store**：Kubernetes 使用 “etcd” 。 这是一个强大的、稳定的、高可用的键值存储，被Kubernetes用于长久储存所有的API对象。
- **控制管理器 Controller Manager**：被称为“kube-controller manager”，它运行着所有处理集群日常任务的控制器。包括了节点控制器、副本控制器、端点（endpoint）控制器以及服务账户等。
- **调度器Scheduler**：调度器会监控新建的pods（一组或一个容器）并将其分配给节点。

##### Kubernetes 的工作节点（Worker Node）

**Kubelet**：负责调度到对应节点的Pod的**生命周期管理**，执行任务并将Pod状态报告给主节点的渠道，通过**容器运行时**（拉取镜像、启动和停止容器等）来运行这些容器。它还会定期执行被请求的容器的健康探测程序。
**Kube-proxy**：它负责节点的网络，在主机上维护网络规则并执行连接转发。它还负责对正在服务的pods进行负载平衡。

##### etcd
etcd是CoreOS基于 Raft 开发的分布式 key-value 存储，可用于**服务发现、共享配置**以及一致性保障（如数据库选主、分布式锁等）。
- 基本的 key-value 存储；
- 监听机制：允许客户端监听特定key的变化，实现实时更新。
- key的过期及**续约机制**，用于监控和服务发现；
- 原子CAS和CAD，用于分布式锁和leader选举。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241020221808.png" alt="image.png" style="zoom:60%;" />

**直接访问 etcd 的数据**

- 通过 etcd 进程查看启动参数
- 进入容器
	- `ps -ef|grep etcd`
	- `sh: ps: command not found`
- 怎么办？到主机 Namespace 查看 cert 信息
- 进入容器查询数据

```sh
export ETCDCTL_API=3
etcdctl --endpoints https://localhost:2379 --cert /etc/kubernetes/pki/etcd/server.crt --key
/etc/kubernetes/pki/etcd/server.key --cacert /etc/kubernetes/pki/etcd/ca.crt get --keys-only --prefix /
```

监听对象变化
```sh
etcdctl --endpoints https://localhost:2379 --cert /etc/kubernetes/pki/etcd/server.crt --key
/etc/kubernetes/pki/etcd/server.key --cacert /etc/kubernetes/pki/etcd/ca.crt watch --prefix
/registry/services/specs/default/mynginx
```


##### APIServer

Kube-APIServer是Kubernetes最重要的核心组件之一，主要提供以下功能：
- 提供集群管理的 REST API 接口，包括:
	- 认证 Authentication；
	- 授权 Authorization；
	- 准入 Admission（Mutating & Valiating）。
- 提供其他模块之间的数据交互和通信的枢纽（其他模块通过 APIServer 查询或修改数据，只有 APIServer 才直接操作 etcd）。
- APIServer 提供 etcd 数据缓存以减少集群对 etcd 的访问。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241020223607.png" alt="image.png" style="zoom:60%;" />

##### APIServer 展开

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241020224510.png" alt="image.png" style="zoom:60%;" />


1. Kube-APIServer:
   - APIHandler: 接收API请求的入口点。
   - AuthN (Authentication): 验证请求的身份。
   - Rate Limit: 限制请求速率，防止过载。
   - Auditing: 记录审计日志。
   - AuthZ (Authorization): 检查请求是否有权限执行相应操作。
   - K8s RBAC: Kubernetes的基于角色的访问控制系统。
   - Aggregator: 聚合多个API服务。
   - Mutating Webhook: 修改请求的内容。
   - Schema Validation: 验证请求的结构是否符合API规范。
   - Validating Webhook: 对请求进行最终的有效性验证。

2. 外部服务:
   - AuthService: 外部认证服务。
   - Mutating Webhook Service: 外部的请求修改服务。
   - Validating Webhook Service: 外部的请求验证服务。

3. Aggregated APIServer:
   - 这是一个扩展的API服务器，可以处理自定义资源。
   - 包含自己的Mutating Webhook、Schema Validation和Validating Webhook。

4. etcd:
   - Kubernetes的核心数据存储，所有的集群状态和配置都存储在这里。

请求处理流程:
1. 请求首先由APIHandler接收。
2. 然后进行身份验证（AuthN）。
3. 应用速率限制（Rate Limit）。
4. 记录审计日志（Auditing）。
5. 进行权限检查（AuthZ），使用K8s RBAC系统。
6. 通过Aggregator处理，可能会转发到Aggregated APIServer。
7. 应用Mutating Webhook，可能修改请求内容。
8. 进行Schema Validation，确保请求结构正确。
9. 最后应用Validating Webhook进行最终验证。
10. 如果所有检查都通过，请求最终会被处理，可能涉及对etcd的读写操作。


##### Controller Manager

- Controller Manager 是集群的**大脑**，是确保整个集群动起来的关键；
- 作用是确保Kubernetes遵循声明式系统规范，确保系统的真实状态（Actual State）与用户定义的期望状态（Desired State）一致；
- Controller Manager 是多个控制器的组合，每个 Controller 事实上都是一个control loop，负责侦听其管控的对象，当对象发生变更时完成配置；
- Controller 配置失败通常会触发自动重试，整个集群会在控制器不断重试的机制下确保最终一致性（Eventual Consistency）。

控制器的工作流程

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241020225127.png" alt="image.png" style="zoom:60%;" />

工作流程:

1. Informer监听Kubernetes API服务器，接收资源变化的事件。
2. 根据事件类型（添加、删除、更新），调用相应的事件处理函数。
3. 事件处理函数生成一个唯一的键，代表需要处理的对象。
4. 这个键被加入到工作队列中。
5. 工作队列对入队的项目进行**速率限制**，防止过载。
6. 多个worker并行从队列中取出工作项进行处理。
7. 每个worker同步处理分配给它的对象，可能涉及与API服务器的交互。

###### Informer 的内部机制

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241020231439.png" alt="image.png" style="zoom:60%;" />

流程步骤：

1. Reflector从API Server获取资源对象并添加到Delta Fifo Queue。
2. Informer从队列中弹出对象。
3. Informer将对象传递给Indexer进行存储。
4. Indexer将对象和键存储在Thread Safe Store中。
5. Informer调用相应的Resource Event Handler。
6. Event Handler将对象的键添加到WorkQueue。
7. Worker从WorkQueue取出键。
8. Worker使用键从Indexer检索最新的对象状态并处理。


控制器的协同工作原理

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241020231710.png" alt="image.png" style="zoom:60%;" />

1. CRI (Container Runtime Interface):
	- 容器运行时接口，用于与具体的容器运行时（如Docker）交互
2. CNI (Container Network Interface):
	- 容器网络接口，负责配置Pod的网络
3. CSI (Container Storage Interface):
	- 容器存储接口，负责管理和挂载存储卷


使用 Deployment 而不是直接使用 Pod 有很多优势。以下是主要原因：

1. 声明式更新:
    - Deployment 允许您声明性地更新 Pod 和 ReplicaSet。
    - 您可以轻松更改 Pod 模板（如镜像版本），Deployment 会自动管理更新过程。
2. 滚动更新:
    - Deployment 提供内置的滚动更新策略，可以逐步更新 Pod，确保服务的连续性。
    - 可以控制更新速率，并在出现问题时自动回滚。
3. 版本控制:
    - Deployment 保留了更新的历史记录，允许您轻松回滚到之前的版本。
4. 扩展性:
    - 可以轻松地扩展或缩减应用程序实例数量，只需更改 replicas 字段。
5. 自愈能力:
    - 如果 Pod 因任何原因失败或被删除，Deployment 会自动创建新的 Pod 来替代。
6. 状态管理:
    - Deployment 通过 ReplicaSet 持续监控所需的 Pod 状态，确保实际状态始终匹配期望状态。
7. 暂停和恢复:
    - 可以暂停 Deployment，对其底层 Pod 模板进行多项修复，然后恢复它以开始新的滚动更新。
8. 网络身份稳定:
    - 即使 Pod 被重新创建，通过 Service 资源，也可以保持稳定的网络身份。


###### Scheduler

特殊的 Controller，工作原理与其他控制器无差别。
Scheduler 的特殊职责在于监控当前集群所有未调度的 Pod，并且获取当前集群所有节点的健康状况和资源
使用情况，为待调度 Pod 选择最佳计算节点，完成调度。
调度阶段分为：
- Predict：过滤不能满足业务需求的节点，如资源不足、端口冲突等。
- Priority：按既定要素将满足调度需求的节点评分，选择最佳节点。
- Bind：将计算节点与 Pod 绑定，完成调度。


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241021002802.png" alt="image.png" style="zoom:60%;" />

###### Kubelet

Kubernetes 的初始化系统（init system）
- 从不同源获取 Pod 清单，并按需求启停 Pod 的核心组件：
	- Pod 清单可从本地文件目录，给定的 HTTPServer 或 Kube-APIServer 等源头获取；
	- Kubelet 将运行时，网络和存储抽象成了 CRI，CNI，CSI。
- 负责汇报当前节点的资源信息和健康状态；
- 负责Pod的健康检查和状态汇报。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241021002949.png" alt="image.png" style="zoom:80%;" />



###### Kube-Proxy
- 监控集群中用户**发布**的服务，并完成**负载均衡**配置。
- 每个节点的 Kube-Proxy 都会配置相同的负载均衡策略，使得整个集群的服务发现建立在分布式负载均衡器之上，服务调用无需经过额外的网络跳转（Network Hop）。
- 负载均衡配置基于不同插件实现：
	- userspace。
	- 操作系统网络协议栈不同的 Hooks 点和插件：
		- iptables；
		- ipvs。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241021003715.png" alt="image.png" style="zoom:80%;" />

###### 推荐的 Add-ons
- kube-dns：负责为整个集群提供 DNS 服务；
- Ingress Controller：为服务提供外网入口；
- MetricsServer：提供资源监控；
- Dashboard：提供 GUI；
- Fluentd-Elasticsearch：提供集群日志采集、存储与查询。

#### 了解 kubectl

##### Kubectl 命令和 kubeconfig

- kubectl 是一个 Kubernetes 的命令行工具，它允许Kubernetes用户以命令行的方式与 Kubernetes 交互，其默认读取配置文件 `~/.kube/config`。
- kubectl 会将接收到的用户请求转化为 rest 调用以rest client 的形式与 apiserver 通讯。
- apiserver的地址，用户信息等配置在 kubeconfig。


```yml
apiVersion: v1
clusters:
- cluster:
	certificate-authority-data: REDACTED
	server: https://127.0.0.1:54729
 name: kind-kind
contexts:
- context:
	cluster: kind-kind
	user: kind-kind
 name: kind-kind
current-context: kind-kind
kind: Config
users:
- name: kind-kind
user:
	client-certificate-data: REDACTED
	client-key-data: REDACTED
```


##### kubectl 常用命令
`kubectl get po –oyaml -w`
kubectl 可查看对象。
`-oyaml` 输出详细信息为 yaml 格式。
-w watch 该对象的后续变化。
`-owide` 以详细列表的格式查看对象。


##### Kubectl describe
kubectl describe 展示资源的详细信息和相关 Event。
```sh
kubectl describe po ubuntu-6fcf6c67db-xvmjh
....
Events:
Type    Reason     Age    From                          Message
---- ------
---- ----
-------
Normal  Scheduled  8m13s  default-scheduler             Successfully assigned ubuntu-6fcf6c67db-xvmjh to k8smaster
Normal  Pulling    7m56s kubelet, k8smaster  pulling image "ubuntu:16.04"
Normal  Pulled     7m50s kubelet, k8smaster  Successfully pulled image "ubuntu:16.04"
Normal  Created    7m50s kubelet, k8smaster  Created container
Normal  Started    7m50s kubelet, k8smaster  Started container
```


##### kubectl exec
kubectl exec 提供进入运行容器的通道，可以进入容器进行 debug 操作。

```sh
# kubectl exec -it ubuntu-6fcf6c67db-xvmjh bash
root@ubuntu-6fcf6c67db-xvmjh:/# hostname -f
ubuntu-6fcf6c67db-xvmjh
root@ubuntu-6fcf6c67db-xvmjh:/#
...
```

##### kubectl logs
Kubectl logs 可查看 pod 的标准输入（stdout, stderr），与 tail 用法类似。
```sh
jianqli:~# kubectl logs ubuntu-6fcf6c67db-xvmjh
Mon Mar 25 14:56:02 UTC 2019
Mon Mar 25 14:56:05 UTC 2019
Mon Mar 25 14:56:08 UTC 2019
Mon Mar 25 14:56:11 UTC 2019
Mon Mar 25 14:56:14 UTC 2019
...
```


#### 深入理解 Kubernetes

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241021030426.png" alt="image.png" style="zoom:60%;" />

#### Kubernetes 生态系统

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241021030524.png" alt="image.png" style="zoom:60%;" />


#### Kubernetes 设计理念

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241021030837.png" alt="image.png" style="zoom:60%;" />


#### Kubernetes Master

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241021031339.png" alt="image.png" style="zoom:60%;" />

### 分层架构

- **核心层**：Kubernetes最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境。
- **应用层**：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS解析等）。
- **管理层**：系统度量（如基础设施、容器和网络的度量）、自动化（如自动扩展、动态Provision等）、策略管理（RBAC、Quota、PSP、NetworkPolicy 等）。
- **接口层**：Kubectl命令行工具、客户端SDK以及集群联邦。
- **生态系统**：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴：
	- Kubernetes外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS应用、ChatOps等；
	- Kubernetes内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241021031700.png" alt="image.png" style="zoom:60%;" />


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241021032326.png" alt="image.png" style="zoom:70%;" />

#### API 设计原则
- **所有 API 都应是声明式的**
	- 相对于命令式操作，声明式操作对于重复操作的效果是稳定的，这对于容易出现数据丢失或重复的分布式环境来说是很重要的。
	- 声明式操作更易被用户使用，可以使系统向用户隐藏实现的细节，同时也保留了系统未来持续优化的可能性。
	- 此外，声明式的 API 还隐含了所有的 API 对象都是名词性质的，例如Service、Volume这些API都是名词，这些名词描述了用户所期望得到的一个目标对象。
- **API 对象是彼此互补而且可组合的**
	- 这实际上鼓励API对象尽量实现面向对象设计时的要求，即“高内聚，松耦合”，对业务相关的概念有一个合适的分解，提高分解出来的对象的可重用性。
- **高层 API 以操作意图为基础设计**
	- 如何能够设计好 API，跟如何能用面向对象的方法设计好应用系统有相通的地方，高层设计一定是从业务出发，而不是过早的从技术实现出发。
	- 因此，针对 Kubernetes 的高层 API 设计，一定是以Kubernetes的业务为基础出发，也就是以系统调度管理容器的操作意图为基础设计。

- **低层 API 根据高层 API 的控制需要设计**
	- 设计实现低层 API 的目的，是为了被高层 API 使用，考虑减少冗余、提高重用性的目的，低层 API的设计也要以需求为基础，要尽量抵抗受技术实现影响的诱惑。
- **尽量避免简单封装，不要有在外部 API 无法显式知道的内部隐藏的机制**
	- 简单的封装，实际没有提供新的功能，反而增加了对所封装 API 的依赖性。
	- 例如 StatefulSet 和 ReplicaSet，本来就是两种 Pod 集合，那么 Kubernetes 就用不同 API 对象来定义它们，而不会说只用同一个 ReplicaSet，内部通过特殊的算法再来区分这个 ReplicaSet 是有状态的还是无状态。

- **API 操作复杂度与对象数量成正比**
	- API 的操作复杂度不能超过 O(N)，否则系统就不具备水平伸缩性了。
- **API 对象状态不能依赖于网络连接状态**
	- 由于众所周知，在分布式环境下，网络连接断开是经常发生的事情，因此要保证 API 对象状态能应对网络的不稳定，API 对象的状态就不能依赖于网络连接状态。
- **尽量避免让操作机制依赖于全局状态**
	- 因为在分布式系统中要保证全局状态的同步是非常困难的。


#### Kubernetes 如何通过对象的组合完成业务描述

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241021033558.png" alt="image.png" style="zoom:60%;" />




##### 架构设计原则

- 只有 APIServer 可以直接访问 etcd 存储，其他服务必须通过 Kubernetes API 来访问集群状态；
- 单节点故障不应该影响集群的状态；
- 在没有新请求的情况下，所有组件应该在故障恢复后继续执行上次最后收到的请求（比如网络分区或服务重启等）；
- 所有组件都应该在内存中保持所需要的状态，APIServer 将状态写入 etcd 存储，而其他组件则通过 APIServer 更新并监听所有的变化；
- 优先使用事件监听而不是轮询。


##### 引导（Bootstrapping）原则
- Self-hosting 是目标。
- 减少依赖，特别是稳态运行的依赖。
- 通过分层的原则管理依赖。
- 循环依赖问题的原则：
	- 同时还接受其他方式的数据输入（比如本地文件等），这样在其他服务不可用时还可以手动配置引导服务；
	- 状态应该是可恢复或可重新发现的；
	- 支持简单的启动临时实例来创建稳态运行所需要的状态，使用分布式锁或文件锁等来协调不同状态的切换（通常称为 pivoting 技术）；
	- 自动重启异常退出的服务，比如副本或者进程管理器等。
