# 套接字-Socket
## 概念

- 局域网和广域网
	- 局域网：局域网将**一定区域内**的各种计算机、外部设备和数据库连接起来形成计算机通信的私有网络。
	- 广域网：又称**广域网、外网、公网**。是连接不同地区局域网或城域网计算机通信的远程公共网络。
- IP(Internet Protocol)：本质是一个整形数,用于表示计算机在网络中的地址。IP协议版本有两个：IPv4和IPv6
	- IPv4(Internet Protocol version4)：
		- 使用一个32位的整形数描述一个IP地址,4个字节,int型
		- 也可以使用一个点分十进制字符串描述这个IP地址： 192.168.247.135
		- 分成了4份,每份1字节,8bit(char),最大值为 255
			- `0.0.0.0` 是最小的IP地址
			- `255.255.255.255`是最大的IP地址
		- 按照IPv4协议计算,可以使用的IP地址共有  $2^{32}$ 个
- IPv6(Internet Protocol version6)：
	- 使用一个128位的整形数描述一个IP地址,16个字节
	- 也可以使用一个字符串描述这个IP地址：`2001:0db8:3c4d:0015:0000:0000:1a2f:1a2b`
	- 分成了8份,每份2字节,每一部分以16进制的方式表示
	- 按照IPv6协议计算,可以使用的IP地址共有 $2^{128}$ 个
- 查看IP地址

```shell
# linux
$ ifconfig

# windows
$ ipconfig

# 测试网络是否畅通
# 主机a: 192.168.1.11
# 当前主机: 192.168.1.12
$ ping 192.168.1.11     # 测试是否可用连接局域网
$ ping www.baidu.com    # 测试是否可用连接外网

# 特殊的IP地址: 127.0.0.1  ==> 和本地的IP地址是等价的
# 假设当前电脑没有联网, 就没有IP地址, 又要做网络测试, 可用使用 127.0.0.1 进行本地测试
```

- 端口

端口的作用是<font color=#ff0000>定位到主机上的某一个进程</font>,通过这个端口进程就可以接受到对应的网络数据了。

> 比如: 在电脑上运行了微信和QQ, 小明通过客户端给我的的微信发消息, 电脑上的微信就收到了消息, 为什么?
> 
> 运行在电脑上的微信和QQ都绑定了不同的端口
> 通过IP地址可以定位到某一台主机,通过端口就可以定位到主机上的某一个进程
> 通过指定的IP和端口,发送数据的时候对端就能接受到数据了

端口也是一个整形数`unsigned short`,一个16位整形数,有效端口的取值范围是：`0 ~ 65535`(0 ~ $2^{16}$-1)

提问：计算机中所有的进程都需要关联一个端口吗,一个端口可以被重复使用吗?
- 不需要,如果这个进程不需要网络通信,那么这个进程就不需要绑定端口的
- 一个端口只能给某一个进程使用,多个进程不能同时使用同一个端口

- OSI/ISO 网络分层模型

OSI(Open System Interconnect),即开放式系统互联。 一般都叫OSI参考模型,是ISO(国际标准化组织组织)在1985年研究的网络互联模型。

<img src="https://subingwen.cn/linux/socket/ip%E5%9B%9B%E5%B1%82%E5%8D%8F%E8%AE%AE%E6%A8%A1%E5%9E%8B.png" alt="image.png" style="zoom:80%;" />

> [!seealso]
> - 物理层：负责最后将信息编码成电流脉冲或其它信号用于网上传输
> - 数据链路层:
> 	- 数据链路层通过物理网络链路供数据传输。
> 	- 规定了0和1的分包形式,确定了网络数据包的形式；
> - 网络层
> 	- 网络层负责在源和终点之间建立连接;
> 	- 此处需要确定计算机的位置,通过IPv4,IPv6格式的IP地址来找到对应的主机
> - 传输层
> 	- 传输层向高层提供可靠的端到端的网络数据流服务。
> 	- 每一个应用程序都会在网卡注册一个端口号,该层就是端口与端口的通信
> - 会话层
> 	- 会话层建立、管理和终止表示层与实体之间的通信会话；
> 	- 建立一个连接(自动的手机信息、自动的网络寻址);
> - 表示层:
> 	- 对应用层数据编码和转化, 确保以一个系统应用层发送的信息可以被另一个系统应用层识别;


## 网络协议

网络协议指的是计算机网络中互相通信的**对等实体**之间交换信息时所必须遵守的规则的集合。一般系统网络协议包括五个部分：通信环境,传输服务,词汇表,信息的编码格式,时序、规则和过程。先来通过下面几幅图了解一下常用的网络协议的格式：

- TCP协议 -> 传输层协议

<img src="https://subingwen.cn/linux/socket/tcp.png" alt="image.png" style="zoom:80%;" />

- UDP协议 -> 传输层协议

<img src="https://subingwen.cn/linux/socket/udp.png" alt="image.png" style="zoom:80%;" />

- IP协议 -> 网络层协议

<img src="https://subingwen.cn/linux/socket/ip.png" alt="image.png" style="zoom:50%;" />

- 以太网帧协议 -> 网络接口层协议

<img src="https://subingwen.cn/linux/socket/mac.png" alt="image.png" style="zoom:70%;" />
- 数据的封装

<img src="https://subingwen.cn/linux/socket/1558001080021.png" alt="image.png" style="zoom:90%;" />

在网络通信的时候, 程序猿需要负责的应用层数据的处理(最上层)
- 应用层的数据可以使用某些协议进行封装, 也可以不封装
- 程序猿需要调用发送数据的接口函数,将数据发送出去
- 程序猿调用的API做底层数据处理]
	- 传输层使用传输层协议打包数据
	- 网络层使用网络层协议打包数据
	- 网络接口层使用网络接口层协议打包数据
	- 数据被发送到internet
- 接收端接收到发送端的数据
	- 程序猿调用接收数据的函数接收数据
	- 调用的API做相关的底层处理:
		- 网络接口层拆包 ==> 网络层的包
		- 网络层拆包 ==> 网络层的包
		- 传输层拆包 ==> 传输层数据
- 如果应用层也使用了协议对数据进行了封装,数据的包的解析需要程序猿做

## socket编程

Socket套接字由远景研究规划局(Advanced Research Projects Agency, ARPA)资助加里福尼亚大学伯克利分校的一个研究组研发。其目的是将TCP/IP协议相关软件移植到UNIX类系统中。(socket可以看成是用户进程与内核网络协议栈的编程接)设计者开发了一个接口,以便应用程序能简单地调用该接口通信。这个接口不断完善,最终形成了Socket套接字。Linux系统采用了Socket套接字,因此,Socket接口就被广泛使用,到现在已经成为事实上的标准。与套接字相关的函数被包含在头文件`sys/socket.h`中。具体作用看这个[链接](obsidian://bookmaster?type=open-book&bid=iFEzcTWTepAzCxKJ&aid=e5ab866e-b94b-ec2e-d191-9cc2055b9ef7&page=29)


<img src="https://www.subingwen.cn/linux/socket/%E6%8F%92%E5%BA%A7.png" alt="image.png" style="zoom:70%;" />

通过上面的描述可以得知,套接字对应程序猿来说就是一套网络通信的接口,使用这套接口就可以完成网络通信。网络通信的主体主要分为两部分：`客户端`和`服务器端`。在客户端和服务器通信的时候需要频繁提到三个概念：`IP`、`端口`、`通信数据`,下面介绍一下需要注意的一些细节问题。

### 字节序
在各种计算机体系结构中,对于字节、字等的存储机制有所不同,因而引发了计算机通信领域中一个很重要的问题,即通信双方交流的信息单元(比特、字节、字、双字等等)应该以什么样的顺序进行传送。如果不达成一致的规则,通信双方将无法进行正确的编/译码从而导致通信失败。

<font color=#ff0000>字节序,顾名思义字节的顺序,就是大于一个字节类型的数据在内存中的存放顺序,也就是说对于单字符来说是没有字节序问题的,字符串是单字符的集合,因此字符串也没有字节序问题。</font>

目前在各种体系的计算机中通常采用的字节存储机制主要有两种：Big-Endian 和 Little-Endian,下面先从字节序说起。

[[零散知识点#大小端存储|大小端概念]]

<font color=#ff0000>套接字通信过程中操作的数据都是大端存储的,包括：接收/发送的数据、IP地址、端口。</font>

```c
// 有一个16进制的数, 有32位 (int): 0xab5c01ff
// 字节序, 最小的单位: char 字节, int 有4个字节, 需要将其拆分为4份
// 一个字节 unsigned char, 最大值是 255(十进制) ==> ff(16进制) 
                 内存低地址位                内存的高地址位
--------------------------------------------------------------------------->
小端:         0xff        0x01        0x5c        0xab
大端:         0xab        0x5c        0x01        0xff
```

<img src="https://www.subingwen.cn/linux/socket/little.png" alt="image.png" style="zoom:50%;" />
<img src="https://www.subingwen.cn/linux/socket/big.png" alt="image.png" style="zoom:50%;" />

函数
> BSD Socket提供了封装好的转换接口,方便程序员使用。包括从主机字节序到网络字节序的转换函数：htons、htonl；从网络字节序到主机字节序的转换函数：ntohs、ntohl。

```c
#include <arpa/inet.h>
// u:unsigned
// 16: 16位, 32:32位
// h: host, 主机字节序
// n: net, 网络字节序
// s: short
// l: int

// 这套api主要用于 网络通信过程中 IP 和 端口 的 转换
// h代表host,n代表network s代表short l代表long
// 将一个短整形从主机字节序 -> 网络字节序
uint16_t htons(uint16_t hostshort);	
// 将一个整形从主机字节序 -> 网络字节序
uint32_t htonl(uint32_t hostlong);	

// 将一个短整形从网络字节序 -> 主机字节序
uint16_t ntohs(uint16_t netshort)
// 将一个整形从网络字节序 -> 主机字节序
uint32_t ntohl(uint32_t netlong);
```

**主机字节序**
	不同的主肌有不同的字节序,如x86为小端字节序,Motorola 6800为大端字节序,ARM字节序是可配置的。

**网络字节序**
	网络字节序规定为大 端字节序
### IP地址转换
虽然IP地址本质是一个整形数,但是在使用的过程中都是通过**一个字符串**来描述,下面的函数描述了如何将一个字符串类型的IP地址进行大小端转换：

```c
// 主机字节序的IP地址转换为网络字节序
// 主机字节序的IP地址是字符串, 网络字节序IP地址是整形
int inet_pton(int af, const char *src, void *dst); 
```

- 参数:
	- `af`: 地址族(IP地址的家族包括ipv4和ipv6)协议
		- `AF_INET`: ipv4格式的ip地址
		- `AF_INET6`: ipv6格式的ip地址
	- `src`: 传入参数, 对应要转换的点分十进制的ip地址: 192.168.1.100
	- `dst`: 传出参数, 函数调用完成, 转换得到的大端整形IP被写入到这块内存中
- 返回值：成功返回1,失败返回0或者-1

```c
#include <arpa/inet.h>
// 将大端的整形数, 转换为小端的点分十进制的IP地址        
const char *inet_ntop(int af, const void *src, char *dst, socklen_t size);
```

- 参数:
	- `af`: 地址族协议
		- `AF_INET`: ipv4格式的ip地址
		- `AF_INET6`: ipv6格式的ip地址
	- `src`: 传入参数, 这个指针指向的内存中存储了大端的整形IP地址
	- `dst`: 传出参数, 存储转换得到的小端的点分十进制的IP地址
	- `size`: 修饰dst参数的, 标记dst指向的内存中最多可以存储多少个字节
- 返回值:
- 成功: 指针指向第三个参数对应的内存地址, 通过返回值也可以直接取出转换得到的IP字符串
- 失败: NULL

还有一组函数也能进程IP地址大小端的转换,但是只能处理ipv4的ip地址：

```c
// 点分十进制IP -> 大端整形
in_addr_t inet_addr (const char *cp);

// 大端整形 -> 点分十进制IP
char* inet_ntoa(struct in_addr in);
```


**套接字类型**
- 流式套接字(SOCK_STREAM)  --> 对应[[计算机网络#网络层|TCP]]
	- 提供面向连接的、可靠的数据传输服务,数据无差错,无重复的发送,且按发送顺序接收。
- 数据报式套接字(SOCK_DGRAM) --> 对应UDP 
	- 提供无连接服务。不提供无错保证,数据可能丢失或重复,并且接收顺序混乱。
- 原始套接字(SOCK_RAW)


### sockaddr 数据结构

<img src="https://www.subingwen.cn/linux/socket/sockaddr.png" alt="image.png" style="zoom:80%;" />

```c
// 在写数据的时候不好用

struct sockaddr { // 属于通用的地址结构 可以用于任何协议的接口
	sa_family_t sa_family;       // 地址族协议, ipv4
	char        sa_data[14];     // 端口(2字节) + IP地址(4字节) + 填充(8字节)
}

typedef unsigned short  uint16_t;
typedef unsigned int    uint32_t;
typedef uint16_t in_port_t;
typedef uint32_t in_addr_t;
typedef unsigned short int sa_family_t;
#define __SOCKADDR_COMMON_SIZE (sizeof (unsigned short int))

struct in_addr
{
    uint32_t s_addr; // 网络地址
};  

// sizeof(struct sockaddr) == sizeof(struct sockaddr_in)
struct sockaddr_in // 网际套接字地址结构 仅仅应用于IPV4地址结构
{
    sa_family_t sin_family;		/* 地址族协议: AF_INET 使用IPV4协议 */
    in_port_t sin_port;         /* 端口, 2字节-> 大端  */
    struct in_addr sin_addr;    /* IP地址, 4字节 -> 大端  */
    /* 填充 8字节 */
    unsigned char sin_zero[sizeof (struct sockaddr) - sizeof(sin_family) -
               sizeof (in_port_t) - sizeof (struct in_addr)];
};  
```

### 套接字函数
使用套接字通信函数需要包含头文件`<arpa/inet.h>`,包含了这个头文件`<sys/socket.h>`就不用在包含了。

```c
// 创建一个套接字
int socket(int domain, int type, int protocol);
```

- 参数:
- `domain`: 使用的地址族协议
	- `AF_INET`: 使用IPv4格式的ip地址
	- `AF_INET6`: 使用IPv4格式的ip地址
- `type`:
	- `SOCK_STREAM`: 使用流式的传输协议
	- `SOCK_DGRAM`: 使用报式(报文)的传输协议
- `protocol`: 一般写0即可, 使用默认的协议
	- `SOCK_STREAM`: 流式传输默认使用的是tcp
	- `SOCK_DGRAM`: 报式传输默认使用的udp
- 返回值:
- 成功: 可用于套接字通信的文件描述符
- 失败: -1

<font color=#ff0000>函数的返回值是一个文件描述符,通过这个文件描述符可以操作内核中的某一块内存,网络通信是基于这个文件描述符来完成的</font>。

```c
// 将文件描述符和本地的IP与端口进行绑定   
int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
```

- 参数:
	- sockfd: 监听的文件描述符, 通过socket()调用得到的返回值
	- addr: 传入参数, 要绑定的IP和端口信息需要初始化到这个结构体中,IP和端口要转换为网络字节序
	- addrlen: 参数addr指向的内存大小, sizeof(struct sockaddr)
- 返回值：成功返回0,失败返回-1

```c
// 给监听的套接字设置监听
int listen(int sockfd, int backlog);
```

- 参数:
	- `sockfd`: 文件描述符, 可以通过调用`socket()`得到,在监听之前必须要绑定`bind()`
	- `backlog`: 同时能处理的最大连接要求,最大值为128
- 返回值：函数调用成功返回0,调用失败返回 -1

```c
// 等待并接受客户端的连接请求, 建立新的连接, 会得到一个新的文件描述符(通信的)		
int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
```

- 参数:
	- `sockfd`: 监听的文件描述符
	- `addr`: 传出参数, 里边存储了建立连接的客户端的地址信息
	- `addrlen`: 传入传出参数,用于存储addr指向的内存大小
- 返回值：函数调用成功,得到一个文件描述符, 用于和建立连接的这个客户端通信,调用失败返回 -1

<font color=#ff0000>这个函数是一个阻塞函数,当没有新的客户端连接请求的时候,该函数阻塞；当检测到有新的客户端连接请求时,阻塞解除,新连接就建立了,得到的返回值也是一个文件描述符,基于这个文件描述符就可以和客户端通信了。</font>

```c
// 接收数据
ssize_t read(int sockfd, void *buf, size_t size);
ssize_t recv(int sockfd, void *buf, size_t size, int flags);
```

- 参数:
	- `sockfd`: 用于通信的文件描述符, `accept()`函数的返回值
	- `buf`: 指向一块有效内存, 用于存储接收是数据
	- `size`: 参数buf指向的内存的容量
	- `flags`: 特殊的属性, 一般不使用, 指定为 0
- 返回值:
	- 大于0：实际接收的字节数
	- 等于0：对方断开了连接
	- -1：接收数据失败了

|      选项名      |                           含义                           | 可用于发送 | 可用于接收 |
| :-----------: | :----------------------------------------------------: | :---: | :---: |
|  MSG_CONFIRM  | 指示链路层协议持续监听, 直到得到答复.(仅能用于SOCK_DGRAM和SOCK_RAW类型的socket) |   Y   |   N   |
| MSG_DONTROUTE |     不查看路由表, 直接将数据发送给本地的局域网络的主机(代表发送者知道目标主机就在本地网络中)     |   Y   |   N   |
| MSG_DONTWAIT  |                          非阻塞                           |   Y   |   Y   |
|   MSG_MORE    |     告知内核有更多的数据要发送, 等到数据写入缓冲区完毕后,一并发送.减少短小的报文提高传输效率     |   Y   |   N   |
|  MSG_WAITALL  |                  读操作一直等待到读取到指定字节后才会返回                  |   N   |   Y   |
|   MSG_PEEK    |                   看一下内缓存数据, 并不会影响数据                    |   N   |   Y   |
|    MSG_OOB    |                       发送或接收紧急数据                        |   Y   |   Y   |
| MSG_NOSIGNAL  |          向读关闭的管道或者socket连接中写入数据不会触发SIGPIPE信号           |   Y   |   N   |

在网络编程中,**带外数据**(Out-of-Band Data)指的是一种**绕过常规数据传输流**的特殊数据传输方式。这种数据被发送在一个独立的通道上,不遵循主数据流的规则。带外数据通常用于传输紧急信息,比如网络控制消息、中断信号或是其他优先级高的信息。其目的是让这些信息能够绕过常规数据的排队机制,快速到达接收方,从而确保重要的控制信息或指令能够及时处理。

带外数据的特点包括：

1. **独立的传输通道**：尽管带外数据和常规数据最终都是通过同一个物理网络连接传输,但在逻辑层面,带外数据被视为在一个独立的通道上发送。这意味着即使常规数据流遭遇阻塞或延迟,带外数据仍然可以被发送和接收。
2. **高优先级处理**：由于带外数据通常用于传输紧急或优先级高的信息,因此网络协议和操作系统通常会给予带外数据高优先级的处理。这确保了即使在网络拥塞或资源紧张的情况下,带外数据也能尽快被处理。
3. **受限的数据量**：带外数据并不适用于传输大量数据。由于它是为了传输紧急或控制信息而设计的,因此只能传输较小的数据量。
    

在TCP/IP协议栈中,TCP协议支持带外数据的概念。TCP允许发送一小部分紧急数据,这些数据可以插入到正常的数据流中,并且接收端可以被特别通知有紧急数据到达。一个常见的例子是,当远程终端需要立即处理中断信号(如Ctrl+C操作)时,可以通过带外数据的方式发送这个信号。

```c
ret = recv(connfd, buf, sizeof(buf) - 1, MSG_OOB);
```
1. **使用`MSG_OOB`标志**：当`recv`函数调用时带有`MSG_OOB`标志,它尝试读取所接收数据流中的带外数据。带外数据是指发送方标记为紧急的数据,它通过一个独立的逻辑通道传输。在TCP协议中,带外数据允许发送方指定某些数据为紧急,从而使接收方能够优先处理这部分数据。因此,指定`MSG_OOB`标志的`recv`调用是尝试接收这种紧急数据。
2. **不使用`MSG_OOB`标志(即标志位为0)**：当`recv`函数调用时不带`MSG_OOB`标志(即标志位设置为0或未指定任何特殊标志),它正常读取数据流中的普通数据。这意味着函数会按照数据到达的顺序,从套接字接收缓冲区中读取数据,而不会尝试检测或优先处理任何标记为紧急的数据。


<font color=#ff0000>如果连接没有断开,接收端接收不到数据,接收数据的函数会阻塞等待数据到达,数据到达后函数解除阻塞,开始接收数据,当发送端断开连接,接收端无法接收到任何数据,但是这时候就不会阻塞了,函数直接返回0。</font>

```c
// 发送数据的函数
ssize_t write(int fd, const void *buf, size_t len);
ssize_t send(int fd, const void *buf, size_t len, int flags);
ssize_t writev(int fd, const struct iovec *iov, int iovcnt); // writev 调用允许从多个缓冲区(多个不连续的内存区域)进行写入。
struct iovec {
    void  *iov_base; // Buffer address
    size_t iov_len;  // Buffer length
};
```

- 参数:
	- fd: 通信的文件描述符, accept() 函数的返回值
	- buf: 传入参数, 要发送的字符串
	- iov:
		- 类型: `const struct iovec *`
		- 描述: 指向`iovec`结构数组的指针,每个结构包含指向数据缓冲区的指针和该缓冲区的大小(字节)。
	- iovcnt:
		- 类型: int
		- 描述: 这是 iov 数组中元素的数量。这个数值告诉 writev 函数有多少个独立的内存块需要写入。
	- len: 要发送的字符串的长度
	- flags: 特殊的属性, 一般不使用, 指定为 0
- 返回值：
	- 大于0：实际发送的字节数,和参数len是相等的
	- -1：发送数据失败了

```c
// 成功连接服务器之后, 客户端会自动随机绑定一个端口
// 服务器端调用accept()的函数, 第二个参数存储的就是客户端的IP和端口信息
int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
```

- 参数:
	- sockfd: 通信的文件描述符, 通过调用socket()函数就得到了
	- addr: 存储了要连接的服务器端的地址信息: iP 和 端口,这个IP和端口也需要转换为大端然后再赋值
	- addrlen: addr指针指向的内存的大小 sizeof(struct sockaddr)
- 返回值：连接成功返回0,连接失败返回-1

```c
#include <sys/socket.h>
int getsockname(int sockfd, struct sockaddr *addr, socklen_t *addrlen); // 用于获取与某个套接字关联的本地协议地址。这在套接字被动态绑定到一个端口(即没有显式指定端口,让操作系统选择一个可用端口)时特别有用,因为它允许程序查询被分配的本地端口号。
```
- 参数：
	- sockfd：要查询的套接字文件描述符。
	- addr：一个指向 struct sockaddr 结构体的指针,该结构体将被填充为套接字的本地地址。
	- addrlen：一个指向 socklen_t 变量的指针,最初应该包含 addr 指向的缓冲区的大小。函数返回时,这个变量将被设置为地址结构的实际长度。
- 返回值：成功时返回0；失败时返回-1,并设置 errno。


```c
#include <sys/socket.h>
int getpeername(int sockfd, struct sockaddr *addr, socklen_t *addrlen); // 用于获取与某个套接字关联的远程协议地址。这在需要知道与之通信的对端的网络地址信息时非常有用,例如,在一个服务器程序中,你可能想要记录客户端的IP地址和端口号。
```
- 参数
	- sockfd：要查询的套接字文件描述符。
	- addr：一个指向 struct sockaddr结构体的指针,该结构体将被填充为套接字的远程地址。
	- addrlen：一个指向 socklen_t变量的指针,最初应该包含addr指向的缓冲区的大小。函数返回时,这个变量将被设置为地址结构的实际长度。
- 返回值：同 getsockname。


## TCP通信流程

TCP是一个面向连接的,安全的,流式传输协议,这个协议是一个传输层协议。

- 面向连接：是一个双向连接,通过三次握手完成,断开连接需要通过四次挥手完成。
- 安全：tcp通信过程中,会对发送的每一数据包都会进行校验, 如果发现数据丢失, 会自动重传
- 流式传输：发送端和接收端处理数据的速度,数据的量都可以不一致

<img src="https://www.subingwen.cn/linux/socket/tcp.jpg" alt="image.png" style="zoom:80%;" />
### 服务器端通信流程

1. 创建用于监听的套接字, 这个套接字是一个文件描述符 ---> 类似于安装了一部话机

```c
int lfd=socket();
```

2. 将得到的监听的文件描述符和本地的IP端口进行绑定 ---> 相当于话机绑定号码
```c
bind()
```

3. 设置监听(成功之后开始监听, 监听的是客户端的连接) 让一个未连接的socket变为一个被动的监听socket,它告诉操作系统这个socket将被用来接受来自客户端的连接请求。简单来说,它让socket准备好接受连接请求。

```c
listen();
```

4. 等待并接受客户端的连接请求, 建立新的连接, 会得到一个新的文件描述符(通信的),没有新连接请求就阻塞

```c
int cfd=accept();
```

5. 通信,读写操作默认都是阻塞的

```c
// 接收数据
read(); / recv();
// 发送数据
write(); / send()
```

6. 断开连接,关闭套接字

```c
close();
```

在tcp的服务器端, 有两类文件描述符
- 监听的文件描述符
	- 只需要有一个
	- 不负责和客户端通信, 负责检测客户端的连接请求, 检测到之后调用accept就可以建立新的连接
- 通信的文件描述符
	- 负责和建立连接的客户端通信
	- 如果有N个客户端和服务器建立了新的连接, 通信的文件描述符就有N个,每个客户端和服务器都对应一个通信的文件描述符

<img src="https://www.subingwen.cn/linux/socket/1558084711685.png" alt="image.png" style="zoom:100%;" />

- 文件描述符对应的内存结构：
	- `一个文件文件描述符对应两块内存, 一块内存是读缓冲区, 一块内存是写缓冲区`
	- 读数据: `通过文件描述符将内存中的数据读出, 这块内存称之为读缓冲区`
	- 写数据: `通过文件描述符将数据写入到某块内存中, 这块内存称之为写缓冲区`
- 监听的文件描述符:
	- 客户端的连接请求会发送到服务器端监听的文件描述符的读缓冲区中
	- 读缓冲区中有数据, 说明有新的客户端连接
	- 调用`accept()`函数, 这个函数会检测监听文件描述符的读缓冲区
		- 检测不到数据, 该函数阻塞
		- 如果检测到数据, 解除阻塞, 新的连接建立
- 通信的文件描述符:
	- 客户端和服务器端都有通信的文件描述符
	- 发送数据：调用函数 `write() / send()`,数据进入到内核中
		- 数据并没有被发送出去, 而是将数据写入到了通信的文件描述符对应的写缓冲区中
		- 内核检测到通信的文件描述符写缓冲区中有数据, 内核会将数据发送到网络中
	- 接收数据: 调用的函数 `read() / recv(),` 从内核读数据
		- 数据如何进入到内核程序猿不需要处理, 数据进入到通信的文件描述符的读缓冲区中
		- 数据进入到内核, 必须使用通信的文件描述符, 将数据从读缓冲区中读出即可

> 基于tcp的服务器端通信代码:

```c
// server.c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>

int main()
{
    // 1. 创建监听的套接字
    int lfd = socket(AF_INET, SOCK_STREAM, 0);
    if(lfd == -1)
    {
        perror("socket");
        exit(0);
    }

    // 2. 将socket()返回值和本地的IP端口绑定到一起
    struct sockaddr_in addr;
    addr.sin_family = AF_INET;
    addr.sin_port = htons(10000);   // 大端端口
    // INADDR_ANY代表本机的所有IP, 假设有三个网卡就有三个IP地址
    // 这个宏可以代表任意一个IP地址
    // 这个宏一般用于本地的绑定操作
    addr.sin_addr.s_addr = INADDR_ANY;  // 这个宏的值为0 == 0.0.0.0
//    inet_pton(AF_INET, "192.168.237.131", &addr.sin_addr.s_addr);
    int ret = bind(lfd, (struct sockaddr*)&addr, sizeof(addr));
    if(ret == -1)
    {
        perror("bind");
        exit(0);
    }

    // 3. 设置监听
    ret = listen(lfd, 128);
    if(ret == -1)
    {
        perror("listen");
        exit(0);
    }

    // 4. 阻塞等待并接受客户端连接
    struct sockaddr_in cliaddr;
    int clilen = sizeof(cliaddr);
    int cfd = accept(lfd, (struct sockaddr*)&cliaddr, &clilen);
    if(cfd == -1)
    {
        perror("accept");
        exit(0);
    }
    // 打印客户端的地址信息
    char ip[24] = {0};
    printf("客户端的IP地址: %s, 端口: %d\n",
           inet_ntop(AF_INET, &cliaddr.sin_addr.s_addr, ip, sizeof(ip)),
           ntohs(cliaddr.sin_port));

    // 5. 和客户端通信
    while(1)
    {
        // 接收数据
        char buf[1024];
        memset(buf, 0, sizeof(buf));
        int len = read(cfd, buf, sizeof(buf));
        if(len > 0)
        {
            printf("客户端say: %s\n", buf);
            write(cfd, buf, len);
        }
        else if(len  == 0)
        {
            printf("客户端断开了连接...\n");
            break;
        }
        else
        {
            perror("read");
            break;
        }
    }

    close(cfd);
    close(lfd);

    return 0;
}
```


### 客户端的通信流程

1. 创建一个通信的套接字

```c
int cfd = socket();
```

2. 连接服务器, 需要知道服务器绑定的IP和端口 ---> 类似于拨打电话号码

```c
connect();
```

3. 通信

```c
// 接收数据
read(); / recv();
// 发送数据
write(); / send();
```

4. 断开连接, 关闭文件描述符(套接字)

```c
close();
```


> 基于tcp通信的客户端通信代码:

```c
// client.c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>

int main()
{
    // 1. 创建通信的套接字
    int fd = socket(AF_INET, SOCK_STREAM, 0);
    if(fd == -1)
    {
        perror("socket");
        exit(0);
    }

    // 2. 连接服务器
    struct sockaddr_in addr;
    addr.sin_family = AF_INET;
    addr.sin_port = htons(10000);   // 大端端口
    inet_pton(AF_INET, "192.168.237.131", &addr.sin_addr.s_addr);

    int ret = connect(fd, (struct sockaddr*)&addr, sizeof(addr));
    if(ret == -1)
    {
        perror("connect");
        exit(0);
    }

    // 3. 和服务器端通信
    int number = 0;
    while(1)
    {
        // 发送数据
        char buf[1024];
        sprintf(buf, "你好, 服务器...%d\n", number++);
        write(fd, buf, strlen(buf)+1);
        
        // 接收数据
        memset(buf, 0, sizeof(buf));
        int len = read(fd, buf, sizeof(buf));
        if(len > 0)
        {
            printf("服务器say: %s\n", buf);
        }
        else if(len  == 0)
        {
            printf("服务器断开了连接...\n");
            break;
        }
        else
        {
            perror("read");
            break;
        }
        sleep(1);   // 每隔1s发送一条数据
    }

    close(fd);

    return 0;
}
```
## 扩展阅读

在Window中也提供了套接字通信的API,这些API函数与Linux平台的API函数几乎相同,以至于很多人认为套接字通信的API函数库只有一套,下面来看一下这些Windows平台的套接字函数：

### 初始化套接字环境

使用Windows中的套接字函数需要额外包含对应的头文件以及加载响应的动态库：

```c
// 使用包含的头文件 
include <winsock2.h>
// 使用的套接字库 
ws2_32.dll       
```

在Windows中使用套接字需要先加载套接字库(套接字环境),最后需要释放套接字资源。
```c++
// 初始化Winsock库
// 返回值: 成功返回0,失败返回SOCKET_ERROR。
WSAStartup(WORD wVersionRequested, LPWSADATA lpWSAData);
```

- 参数:
- wVersionRequested: 使用的Windows Socket的版本, 一般使用的版本是 2.2
	- 初始化这个 `MAKEWORD(2, 2)`;参数
- lpWSAData：一个WSADATA结构指针, 这是一个传入参数
	- 创建一个 WSADATA 类型的变量, 将地址传递给该函数的第二个参数

注销Winsock相关库,函数调用成功返回0,失败返回 SOCKET_ERROR。

```c++
int WSACleanup (void);
```

使用举例：

```c
WSAData wsa;
// 初始化套接字库
WSAStartup(MAKEWORD(2, 2), &wsa);

// .......
```

### 套接字通信函数

> 基于Linux的套接字通信流程是最全面的一套通信流程,如果是在某个框架中进行套接字通信,通信流程只会更简单,直接使用window的套接字api进行套接字通信,和Linux平台上的通信流程完全相同。

#### 结构体

```c
///////////////////////////////////////////////////////////////////////
/////////////////////////////// Windows ///////////////////////////////
///////////////////////////////////////////////////////////////////////
typedef struct in_addr {
　　union {
　　	struct{ unsigned char s_b1,s_b2, s_b3,s_b4;} S_un_b;
　　	struct{ unsigned short s_w1, s_w2;} S_un_w;
　　	unsigned long S_addr;	// 存储IP地址
　　} S_un;
}IN_ADDR;

struct sockaddr_in {
　　short int sin_family; /* Address family */
　　unsigned short int sin_port; /* Port number */
　　struct in_addr sin_addr; /* Internet address */
　　unsigned char sin_zero[8]; /* Same size as struct sockaddr */
};

///////////////////////////////////////////////////////////////////////
//////////////////////////////// Linux ////////////////////////////////
///////////////////////////////////////////////////////////////////////
typedef unsigned short  uint16_t;
typedef unsigned int    uint32_t;
typedef uint16_t in_port_t;
typedef uint32_t in_addr_t;
typedef unsigned short int sa_family_t;

struct in_addr
{
    in_addr_t s_addr;
};  

// sizeof(struct sockaddr) == sizeof(struct sockaddr_in)
struct sockaddr_in
{
    sa_family_t sin_family;     /* 地址族协议: AF_INET */
    in_port_t sin_port;         /* 端口, 2字节-> 大端  */
    struct in_addr sin_addr;    /* IP地址, 4字节 -> 大端  */
    /* 填充 8字节 */
    unsigned char sin_zero[sizeof (struct sockaddr) - sizeof(sin_family) -
                      sizeof (in_port_t) - sizeof (struct in_addr)];
};  
```

#### 大小端转换函数
```c
// 主机字节序 -> 网络字节序
u_short htons (u_short hostshort );
u_long htonl ( u_long hostlong);

// 网络字节序 -> 主机字节序
u_short ntohs (u_short netshort );
u_long ntohl ( u_long netlong);

// linux函数, window上没有这两个函数
inet_ntop(); 
inet_pton();

// windows 和 linux 都使用, 只能处理ipv4的ip地址
// 点分十进制IP -> 大端整形
unsigned long inet_addr (const char FAR * cp);	// windows
in_addr_t     inet_addr (const char *cp);			// linux

// 大端整形 -> 点分十进制IP
// window, linux相同
char* inet_ntoa(struct in_addr in);
```

#### 套接字函数

> `window的api中套接字对应的类型是 SOCKET 类型, linux中是 int 类型, 本质是一样的`

```c
// 创建一个套接字
// 返回值: 成功返回套接字, 失败返回INVALID_SOCKET
SOCKET socket(int af,int type,int protocal);
参数:
    - af: 地址族协议
        - ipv4: AF_INET (windows/linux)
        - PF_INET (windows)
        - AF_INET == PF_INET
   - type: 和linux一样
       	- SOCK_STREAM
        - SOCK_DGRAM
   - protocal: 一般写0 即可
       - 在windows上的另一种写法
           - IPPROTO_TCP, 使用指定的流式协议中的tcp协议
           - IPPROTO_UDP, 使用指定的报式协议中的udp协议

 // 关键字: FAR NEAR, 这两个关键字在32/64位机上是没有意义的, 指定的内存的寻址方式
// 套接字绑定本地IP和端口
// 返回值: 成功返回0,失败返回SOCKET_ERROR
int bind(SOCKET s,const struct sockaddr FAR* name, int namelen);

// 设置监听
// 返回值: 成功返回0,失败返回SOCKET_ERROR
int listen(SOCKET s,int backlog);

// 等待并接受客户端连接
// 返回值: 成功返回用于的套接字,失败返回INVALID_SOCKET。
SOCKET accept ( SOCKET s, struct sockaddr FAR* addr, int FAR* addrlen );

// 连接服务器
// 返回值: 成功返回0,失败返回SOCKET_ERROR
int connect (SOCKET s,const struct sockaddr FAR* name,int namelen );

// 在Qt中connect用户信号槽的连接, 如果要使用windows api 中的 connect 需要在函数名前加::
::connect(sock, (struct sockaddr*)&addr, sizeof(addr));

// 接收数据
// 返回值: 成功时返回接收的字节数,收到EOF时为0,失败时返回SOCKET_ERROR。
//		==0 代表对方已经断开了连接
int recv (SOCKET s,char FAR* buf,int len,int flags);

// 发送数据
// 返回值: 成功返回传输字节数,失败返回SOCKET_ERROR。
int send (SOCKET s,const char FAR * buf, int len,int flags);

// 关闭套接字
// 返回值: 成功返回0,失败返回SOCKET_ERROR
int closesocket (SOCKET s);		// 在linux中使用的函数是: int close(int fd);

//----------------------- udp 通信函数 -------------------------
// 接收数据
int recvfrom(SOCKET s,char FAR *buf,int len,int flags,
         struct sockaddr FAR *from,int FAR *fromlen);
// 发送数据
int sendto(SOCKET s,const char FAR *buf,int len,int flags,
       const struct sockaddr FAR *to,int tolen);
```

## TCP的粘包、拆包

### 什么是粘包？

指TCP协议中,当客户端发送多个数据包给服务器时，服务器底层的tcp接收缓冲区收到的数据为粘连在一起的，如下图所示
TCP是面向字节流的协议,就是没有界限的一串数据,本没有“包”的概念,“粘包”和“拆包”一说是为了有助于形象地理解这两种现象。


<img src="https://cdn.llfc.club/9714C5D1A5B9.png" alt="image.png" style="zoom:80%;" />

**为什么UDP没有粘包**？
粘包拆包问题在数据链路层、网络层以及传输层都有可能发生。日常的网络应用开发大都在传输层进行,由于UDP有消息保护边界,不会发生粘包拆包问题,因此粘包拆包问题只发生在TCP协议中。

**粘包拆包发生场景**

- 粘包和拆包是在TCP通信中常见的问题，由于TCP是面向字节流的，因此无法保证接收端能够精确地分辨出发送端每次发送的数据包边界。这导致了在实际通信中可能出现的粘包和拆包问题。

- 粘包通常发生在一次发送的数据量比较小的情况下，如果发送端连续发送多个数据包，并且这些数据包在发送过程中被合并到了一个 TCP 报文中，那么接收端就可能在一次接收中收到多个数据包，导致粘包问题。例如，发送端依次发送了 "loveu" 和 "hello"，但由于 TCP 缓冲区的优化策略或者发送端发送速度过快，这两个数据包被合并到了同一个 TCP 报文中发送给接收端，接收端在一次接收中就会收到 "loveuhello"，无法准确区分每个数据包的边界。

- 拆包则通常发生在一次发送的数据量比较大的情况下，如果发送端发送的数据包大小超过了 TCP 缓冲区的大小，TCP 会将这些数据包拆分成多个 TCP 报文进行发送，这可能导致接收端在一次接收中只能接收到部分数据包，从而出现了拆包问题。
关于粘包和拆包可以参考下图的几种情况：

<img src="https://img-blog.csdnimg.cn/img_convert/c91d6ead62335dcdd7a735f4948f1a6a.png" alt="image.png" style="zoom:60%;" />

粘包/拆包
上图中演示了以下几种情况：
- 正常的理想情况,两个包恰好满足TCP缓冲区的大小或达到TCP等待时长,分别发送两个包；
- 粘包：两个包较小,间隔时间短,发生粘包,合并成一个包发送；
- 拆包：一个包过大,超过缓存区大小,拆分成两个或多个包发送；
- 拆包和粘包：Packet1过大,进行了拆包处理,而拆出去的一部分又与Packet2进行粘包处理。


这是最好理解的粘包问题的产生原因。还有一些其他的原因比如  
1. 客户端的发送频率远高于服务器的接收频率，就会导致数据在服务器的tcp接收缓冲区滞留形成粘连，比如客户端1s内连续发送了两个hello world！,服务器过了2s才接收数据，那一次性读出两个hello world！。  
2. tcp底层的安全和效率机制不允许字节数特别少的小包发送频率过高，tcp会在底层累计数据长度到一定大小才一起发送，比如连续发送1字节的数据要累计到多个字节才发送，可以了解下tcp底层的`Nagle`算法。  
3. 再就是我们提到的最简单的情况，发送端缓冲区有上次未发送完的数据或者接收端的缓冲区里有未取出的数据导致数据粘连。


### 常见的解决方案
对于粘包和拆包问题,常见的解决方案有四种：
- 发送端将每个包都**封装成固定的长度**,比如100字节大小。如果不足100字节可通过补0或空等进行填充到指定长度；
- 发送端在每个包的末尾**使用固定的分隔符**,例如`\r\n`。如果发生拆包需等待多个包发送过来之后再找到其中的`\r\n`进行合并；例如,FTP协议；
- 将消息分为头部和消息体,**头部中保存整个消息的长度**,只有读取到足够长度的消息之后才算是读到了一个完整的消息；
- 通过**自定义协议**进行粘包和拆包的处理。
	- 主要采用应用层定义收发包格式的方式，这个过程俗称切包处理，常用的协议被称为tlv协议(消息id+消息长度+消息内容)，如下图
	- <img src="https://cdn.llfc.club/1683367901552.jpg" alt="image.png" style="zoom:60%;" />
	


# TCP状态转换

在TCP进行三次握手,或者四次挥手的过程中,通信的服务器和客户端内部会发送状态上的变化,发生的状态变化在程序中是看不到的,这个状态的变化也不需要程序猿去维护,但是在某些情况下进行程序的调试会去查看相关的状态信息,先来看三次握手过程中的状态转换。

<img src="https://www.subingwen.cn/linux/tcp-status/20141015155713390.png" alt="image.png" style="zoom:70%;" />


## 三次握手
```c
在第一次握手之前,服务器端必须先启动,并且已经开始了监听
  - 服务器端先调用了 listen() 函数, 开始监听
  - 服务器启动监听前后的状态变化: 没有状态 ---> LISTEN
```

当服务器监听启动之后,由客户端发起的三次握手过程中状态转换如下：

第一次握手:

- 客户端：调用了`connect()` 函数,状态变化：`没有状态 -> SYN_SENT`
- 服务器：收到连接请求SYN,状态变化：`LISTEN -> SYN_RCVD`

第二次握手:

- 服务器：给客户端回复ACK,并且请求和客户端建立连接,状态无变化,依然是 SYN_RCVD
- 客户端：接收数据,收到了ACK,状态变化：`SYN_SENT -> ESTABLISHED`

第三次握手:

- 客户端：给服务器回复ACK,同意建立连接,状态没有变化,还是 `ESTABLISHED`
- 服务器：收到了ACK,状态变化：`SYN_RCVD -> ESTABLISHED`

三次握手完成之后,客户端和服务器都变成了同一种状态,这种状态叫：`ESTABLISHED`,表示双向连接已经建立, 可以通信了。在通过过程中,正常的通信状态就是 `ESTABLISHED`。

## 四次挥手

关于四次挥手对于客户端和服务器哪段先断开连接没有要求,根据实际情况处理即可。下面根据上图中的实例描述一下四次挥手过程中TCP的状态转换(上图中主动断开连接的一方是客户端)：

第一次挥手:
- 客户端：调用close() 函数,将tcp协议中的FIN设置为1,请求和服务器断开连接,
- 状态变化:`ESTABLISHED -> FIN_WAIT_1`
- 服务器：收到断开连接请求,状态变化: `ESTABLISHED -> CLOSE_WAIT`

第二次挥手:
- 服务器：回复ACK,同意断开连接的请求,状态没有变化,还是 `CLOSE_WAIT`
- 客户端：收到ACK,状态变化：`FIN_WAIT_1 -> FIN_WAIT_2`

第三次挥手:

- 服务器端：调用close() 函数,发送FIN给客户端,请求断开连接,状态变化：`CLOSE_WAIT -> LAST_ACK`
- 客户端：收到FIN,状态变化：`FIN_WAIT_2 -> TIME_WAIT`

第四次挥手:

- 客户端：回复ACK给服务器,状态是没有变化的,状态变化：`TIME_WAIT -> 没有状态`
- 服务器端：收到ACK,双向连接断开,状态变化：`LAST_ACK -> 无状态(没有了)`

## 状态转换
在下图中同样是描述TCP通信过程中的客户端和服务器端的状态转,看起来比较乱,其实只需要看两条主线：红色实线和绿色虚线。关于黑色的实线对应的是一些特殊情况下的状态切换,在此不做任何分析。

因为三次握手是由客户端发起的,据此分析红色的实线表示的客户端的状态,绿色虚线表示的是服务器端的状态。


<img src="https://www.subingwen.cn/linux/tcp-status/wKiom1ZLIP2CRdNUAAlNCKgqwI0818.jpg" alt="image.png" style="zoom:50%;" />
- 客户端：
	- 第一次握手：发送SYN,`没有状态 -> SYN_SENT`
	- 第二次握手：收到回复的ACK,`SYN_SENT -> ESTABLISHED`
	- 主动断开连接,第一次挥手发送FIN,`状态ESTABLISHED -> FIN_WAIT_1`
	- 第二次挥手,收到ACK,`状态FIN_WAIT_1 -> FIN_WAIT_2`
	- 第三次挥手,收到FIN,`状态FIN_WAIT_2 -> TIME_WAIT`
	- 第四次挥手,回复ACK,等待2倍报文时长之后,状态`TIME_WAIT -> 没有状态`
- 服务器端：
	- 启动监听,`没有状态 -> LISTEN`
	- 第一次握手,收到SYN,`状态LISTEN -> SYN_RCVD`
	- 第三次握手,收到ACK,`状态SYN_RCVD -> ESTABLISHED`
	- 收到断开连接请求,第一次挥手状态 `ESTABLISHED -> CLOSE_WAIT`
	- 第三次挥手,发送FIN请求和客户端断开连接,状态`CLOSE_WAIT -> LAST_ACK`
	- 第四次挥手,收到ACK,状态`LAST_ACK -> 无状态(没有了)`

在TCP通信的时候,当主动断开连接的一方接收到被动断开连接的一方发送的FIN和最终的ACK后(第三次挥手完成),连接的主动关闭方必须处于`TIME_WAIT`状态并持续`2MSL(Maximum Segment Lifetime)`时间,这样就能够让TCP连接的主动关闭方在它发送的ACK丢失的情况下重新发送最终的ACK。

一倍报文寿命(MSL)大概时长为30s,因此两倍报文寿命一般在1分钟作用。

`主动关闭方重新发送的最终ACK,是因为被动关闭方重传了它的FIN。事实上,被动关闭方总是重传FIN直到它收到一个最终的ACK。`

## 相关命令

```shell
$ netstat 参数
$ netstat -apn	| grep 关键字
```

- 参数:
	- `-a` (all)显示所有选项
	- `-p` 显示建立相关链接的程序名
	- `-n` 拒绝显示别名,能显示数字的全部转化成数字。
	- `-l` 仅列出有在 Listen (监听) 的服务状态
	- `-t` (tcp)仅显示tcp相关选项
	- `-u` (udp)仅显示udp相关选项

# shutdown() 与 close()的区别

在网络编程中,套接字是一个重要的概念。套接字是一种用于网络通信的接口,它可以实现进程之间的通信和数据传输。在使用套接字进行网络编程时,关闭套接字是一个必要的操作。关闭套接字可以释放资源,避免程序出现内存泄漏等问题。在关闭套接字时,我们通常会用到close()和shutdown()函数。这两个函数虽然都能够关闭套接字,但是它们的使用方式和作用有所不同

## close()

**原型**
```c
#include<unistd.h> 
int close(int sockfd); //返回成功为0,出错为-1.
```

close 一个套接字的默认行为是**把套接字标记为已关闭**,然后立即返回到调用进程,该套接字描述符**不能再由调用进程使用**,也就是说它不能再作为read或write的第一个参数,close终止了数据传送的两个方向。
在多进程并发服务器中,父子进程**共享着套接字**,套接字描述符引用计数记录着共享着的进程个数,当父进程或某一子进程close掉套接字时,描述符**引用计数会相应的减一**,当引用计数仍大于零时,这个close调用就不会引发TCP的四路握手断连过程。

**作用**：
- `close()`函数用于关闭套接字,释放套接字占用的资源。在网络编程中,使用`close()`函数可以防止套接字资源泄漏,提高程序的稳定性和可靠性。

**关闭方式**
- `close()`函数会关闭套接字的所有功能,包括读、写和监听。它会立即关闭套接字,并释放所有相关资源。如果在关闭套接字之前还有未发送的数据,这些数据会被丢弃。
## shutdown()函数

**原型**
```c
#include<sys/socket.h> 
int shutdown(int sockfd,int howto); //返回成功为0,出错为-1.</span>
```

- 参数howto说明
	- SHUT_RD：值为0,关闭连接的读端。
	- SHUT_WR：值为1,关闭连接的写这一半。
	- SHUT_RDWR：值为2,连接的读和写都关闭。

**终止网络连接的通用方法是调用close函数。但使用shutdown能更好的控制断连过程(使用第二个参数)。**

**作用**
`shutdown()`函数用于关闭套接字的读或写功能。它可以在套接字关闭之前,确保所有的数据都已经发送或接收完毕。此外,>shutdown() 函数还可以控制套接字的关闭方式,包括关闭读、写或读写两个方向。

**关闭方式**
shutdown() 函数有三种关闭方式：
- `SHUT_RD`：关闭套接字的读功能,即禁止从套接字中读取数据。
- `SHUT_WR`：关闭套接字的写功能,即禁止向套接字中写入数据。
- `SHUT_RDWR`：关闭套接字的读写功能,即禁止从套接字中读取数据和向套接字中写入数据。

**影响范围**
shutdown() 函数只会影响当前套接字,不会影响其他套接字。关闭套接字的读、写或读写功能后,其他套接字仍然可以正常工作。
shutdown则不管引用计数，直接使得该套接字不可用，如果有别的进程企图使用该套接字，将会受到影响

**产生的信号**
调用`shutdown()`函数可能会产生SIGPIPE信号,这个信号表示向一个已经关闭写功能的套接字中写入数据时,会触发一个异常条件。如果不希望程序因为SIGPIPE信号而终止,可以使用 signal() 函数忽略该信号。


# 端口复用
在网络通信中,一个端口只能被一个进程使用,不能多个进程共用同一个端口。我们在进行套接字通信的时候,如果按顺序执行如下操作：先启动服务器程序,再启动客户端程序,然后关闭服务器进程,再退出客户端进程,最后再启动服务器进程,就会出如下的错误提示信息：`bind error: Address already in use`

```shell
# 第二次启动服务器进程
$ ./server 
bind error: Address already in use

$ netstat -apn|grep 9999
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 127.0.0.1:9999          127.0.0.1:50178         TIME_WAIT   -   
```

通过`netstat`查看TCP状态,发现上一个服务器进程其实还没有真正退出。因为服务器进程是主动断开连接的进程, 最后状态变成了 `TIME_WAIT`状态,这个进程会等待2msl(大约1分钟)才会退出,如果该进程不退出,其绑定的端口就不会释放,再次启动新的进程还是使用这个未释放的端口,端口被重复使用,就是提示`bind error: Address already in use`这个错误信息。

如果想要解决上述问题,就必须要设置端口复用,使用的函数原型如下：
```c
// 这个函数是一个多功能函数, 可以设置套接字选项
int setsockopt(int sockfd, int level, int optname, const void *optval, socklen_t optlen);
```

- 参数:
- `sockfd`：用于监听的文件描述符
- `level`：设置端口复用需要使用 SOL_SOCKET 宏
- `optname`：要设置什么属性(下边的两个宏都可以设置端口复用)
	- `SO_REUSEADDR`
	- `SO_REUSEPORT`
- `optval`：设置是去除端口复用属性还是设置端口复用属性,实际应该使用 int 型变量
	- 0：不设置
	- 1：设置
- optlen：optval指针指向的内存大小 sizeof(int)



```bash
optname定义如下：
 
#define SO_DEBUG 1            -- 打开或关闭调试信息
#define SO_REUSEADDR 2        -- 打开或关闭地址复用功能
#define SO_TYPE 3             -- 
#define SO_ERROR 4
#define SO_DONTROUTE 5
#define SO_BROADCAST 6
#define SO_SNDBUF 7           -- 设置发送缓冲区的大小
#define SO_RCVBUF 8           -- 设置接收缓冲区的大小
#define SO_KEEPALIVE 9        -- 套接字保活
#define SO_OOBINLINE 10
#define SO_NO_CHECK 11
#define SO_PRIORITY 12        -- 设置在套接字发送的所有包的协议定义优先权
#define SO_LINGER 13
#define SO_BSDCOMPAT 14
#define SO_REUSEPORT 15
#define SO_PASSCRED 16
#define SO_PEERCRED 17
#define SO_RCVLOWAT 18
#define SO_SNDLOWAT 19
#define SO_RCVTIMEO 20       -- 设置接收超时时间
#define SO_SNDTIMEO 21       -- 设置发送超时时间
#define SO_ACCEPTCONN 30
#define SO_SNDBUFFORCE 32
#define SO_RCVBUFFORCE 33
#define SO_PROTOCOL 38
#define SO_DOMAIN 39
```


这个函数应该添加到服务器端代码中,具体应该放到什么位置呢？答：在绑定之前设置端口复用
1. 创建监听的套接字
2. 设置端口复用
3. 绑定
4. 设置监听
5. 等待并接受客户端连接
6. 通信
7. 断开连接

参考代码如下:
```c++
#include <stdio.h>
#include <ctype.h>
#include <unistd.h>
#include <stdlib.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <string.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <sys/select.h>

// server
int main(int argc, const char* argv[])
{
    // 创建监听的套接字
    int lfd = socket(AF_INET, SOCK_STREAM, 0);
    if(lfd == -1)
    {
        perror("socket error");
        exit(1);
    }

    // 绑定
    struct sockaddr_in serv_addr;
    memset(&serv_addr, 0, sizeof(serv_addr));
    serv_addr.sin_family = AF_INET;
    serv_addr.sin_port = htons(9999);
    serv_addr.sin_addr.s_addr = htonl(INADDR_ANY);  // 本地多有的ＩＰ
    // 127.0.0.1
    // inet_pton(AF_INET, "127.0.0.1", &serv_addr.sin_addr.s_addr);
    
    // 设置端口复用
    int opt = 1;
    setsockopt(lfd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

    // 绑定端口
    int ret = bind(lfd, (struct sockaddr*)&serv_addr, sizeof(serv_addr));
    if(ret == -1)
    {
        perror("bind error");
        exit(1);
    }

    // 监听
    ret = listen(lfd, 64);
    if(ret == -1)
    {
        perror("listen error");
        exit(1);
    }

    fd_set reads, tmp;
    FD_ZERO(&reads);
    FD_SET(lfd, &reads);

    int maxfd = lfd;

    while(1)
    {
        tmp = reads;
        int ret = select(maxfd+1, &tmp, NULL, NULL, NULL);
        if(ret == -1)
        {
            perror("select");
            exit(0);
        }

        if(FD_ISSET(lfd, &tmp))
        {
            int cfd = accept(lfd, NULL, NULL);
            FD_SET(cfd, &reads);
            maxfd = cfd > maxfd ? cfd : maxfd;
        }
        for(int i=lfd+1; i<=maxfd; ++i)
        {
            if(FD_ISSET(i, &tmp))
            {
                char buf[1024];
                int len = read(i, buf, sizeof(buf));
                if(len > 0)
                {
                    printf("client say: %s\n", buf);
                    write(i, buf, len);
                }
                else if(len == 0)
                {
                    printf("客户端断开了连接\n");
                    FD_CLR(i, &reads);
                    close(i);
                }
                else
                {
                    perror("read");
                    exit(0);
                }
            }
        }
    }

    return 0;
}
```

与之相对有`getsockopt()`函数用于获取任意类型、任意状态套接口的选项当前值,并把结果存入optval。

```c
#include <sys/socket.h>
int getsockopt(int sockfd, int level, int optname, void *optval, socklen_t *optlen);
/*
sockfd：一个标识套接口的描述字。
level：选项定义的级别。例如,支持的级别有SOL_SOCKET、IPPROTO_TCP等。
optname：需获取的套接口选项。
optval：指针,指向存放所获得选项值的缓冲区。
optlen：指针,指向optval缓冲区的长度值。
*/
```


例子；通过 `getsockopt` 获取具体的错误信息
```c
if (e->events & EPOLLERR) {
    int error;
    socklen_t len = sizeof(error);
    int code = getsockopt(e->fd, SOL_SOCKET, SO_ERROR, &error, &len);
    const char *err = NULL;
    if (code < 0) {
        // 获取错误信息失败
        err = strerror(errno);
    } else if (error != 0) {
        // 文件描述符上的确有错误发生
        err = strerror(error);
    }
    if (err != NULL) {
        printf("Socket error: %s\n", err);
    }
}
```


# 五种I/O模型

## 阻塞IO 

在《UNIX网络编程》一书中,总结归纳了5种IO模型：
- 阻塞IO(Blocking IO)
- 非阻塞IO(Nonblocking IO)
- IO多路复用(IO Multiplexing)
- 信号驱动IO(Signal Driven IO)
- 异步IO(Asynchronous IO)

应用程序想要去读取数据,他是无法直接去读取磁盘数据的,他需要先到内核里边去**等待内核操作硬件拿到数据**,这个过程就是1,是需要等待的,等到内核从磁盘上把数据加载出来之后,再把这个数据写给用户的缓存区,这个过程是2,如果是阻塞IO,那么整个过程中,用户从发起读请求开始,一直到读取到数据,**都是一个阻塞状态**。

<img src="https://article.biliimg.com/bfs/article/06a4ed7513a0385c50975ba47f9b9893715afe7c.png" alt="image.png" style="zoom:60%;" />

具体流程如下图：
用户去读取数据时,会去先发起`recvform`一个命令,去尝试从内核上加载数据,如果内核没有数据,那么用户就会等待,此时内核会去从硬件上读取数据,内核读取数据之后,会把数据拷贝到用户态,并且返回ok,整个过程,都是阻塞等待的,这就是阻塞IO
总结如下：
顾名思义,阻塞IO就是两个阶段都必须阻塞等待：

<img src="https://article.biliimg.com/bfs/article/762f3608dcf504b64beaa57f463b927d1d8bc12f.png" alt="1653897270074" style="zoom:67%;" />

**阶段一：**
- 用户进程尝试读取数据(比如网卡数据)
- 此时数据尚未到达,内核需要等待数据
- 此时用户进程也处于阻塞状态
    
**阶段二：**
- 数据到达并拷贝到内核缓冲区,代表已就绪
- 将内核数据拷贝到用户缓冲区
- 拷贝过程中,用户进程依然阻塞等待
- 拷贝完成,用户进程解除阻塞,处理数据

## 非阻塞IO 

顾名思义,非阻塞IO的recvfrom操作会立即返回结果而不是阻塞用户进程。
阶段一：
- 用户进程尝试读取数据(比如网卡数据)
- 此时数据尚未到达,内核需要等待数据
- 返回异常给用户进程
- 用户进程拿到error后,再次尝试读取
- 循环往复,直到数据就绪
阶段二：
- 将内核数据拷贝到用户缓冲区 
- 拷贝过程中,用户进程依然阻塞等待
- 拷贝完成,用户进程解除阻塞,处理数据



<img src="https://article.biliimg.com/bfs/article/e24d29fe43fec6c152e78ad50fcaac47dc60d462.png" alt="1653897490116" style="zoom:67%;" />

- 可以看到,非阻塞IO模型中,**用户进程在第一个阶段是非阻塞**,**第二个阶段是阻塞状态**。虽然是非阻塞,但性能并没有得到提高。而且忙等机制会导致CPU空转,CPU使用率暴增。


## IO多路转接(复用)


> I/O复用是最常使用的I/O通知机制。它指的是,应用程序通过I/O复用函数向内核注册一组事件,内核通过I/O复用函数把其中就绪的事件通知给应用程序,I/O复用函数本身是阻塞的,它们能提高程序效率的原因在于它们具有同时监听多个I/O事件的能力。

IO多路转接也称为IO多路复用,它是一种网络通信的手段(机制),通过这种方式可以**同时监测**多个[[零散知识点#文件描述符|文件描述符]]并且这个过程是阻塞的,一旦检测到有文件描述符**就绪**(可以读数据或者可以写数据)程序的阻塞就会被解除,之后就可以基于这些(一个或多个)就绪的文件描述符进行通信了。通过这种方式在单线程/进程的场景下也可以在服务器端实现并发。常见的IO多路转接方式有：`select、poll、epoll`。

下面先对多线程/多进程并发和IO多路转接的并发处理流程进行对比(服务器端)：

- 多线程/多进程并发
	- 主线程/父进程：调用 `accept()`监测客户端连接请求
		- 如果没有新的客户端的连接请求,当前线程/进程会阻塞
		- 如果有新的客户端连接请求解除阻塞,建立连接
	- 子线程/子进程：和建立连接的客户端通信
		- 调用 `read() / recv()` 接收客户端发送的通信数据,如果没有通信数据,当前线程/进程会阻塞,数据到达之后阻塞自动解除
		- 调用 `write() / send()` 给客户端发送数据,如果写缓冲区已满,当前线程/进程会阻塞,否则将待发送数据写入写缓冲区中
- IO多路转接并发
	- 使用IO多路复用函数**委托内核检测**服务器端所有的文件描述符(通信和监听两类),这个检测过程会导致进程/线程的阻塞,如果检测到已就绪的文件描述符阻塞解除,并将这些已就绪的文件描述符传出
	- 根据类型对传出的所有已就绪文件描述符进行判断,并做出不同的处理
		- 监听的文件描述符：和客户端建立连接
			- 此时调用`accept()`是不会导致程序阻塞的,因为监听的文件描述符是已就绪的(有新请求)
		- 通信的文件描述符：调用通信函数和已建立连接的客户端通信
			- 调用 `read() / recv()` 不会阻塞程序,因为通信的文件描述符是就绪的,读缓冲区内已有数据
			- 调用 `write() / send()` 不会阻塞程序,因为通信的文件描述符是就绪的,写缓冲区不满,可以往里面写数据
- 对这些文件描述符继续进行下一轮的检测(循环往复。。。)

<font color=#ff0000>与多进程和多线程技术相比,I/O多路复用技术的最大优势是系统开销小,系统不必创建进程/线程,也不必维护这些进程/线程,从而大大减小了系统的开销</font>。

### select
#### 函数原型
使用select这种IO多路转接方式需要调用一个同名函数`select`,这个函数是跨平台的,Linux、Mac、Windows都是支持的。程序猿通过调用这个函数可以委托内核帮助我们检测若干个文件描述符的状态,其实就是检测这些文件描述符**对应的读写缓冲区的状态**：

- 读缓冲区：检测里边有没有数据,如果有数据该缓冲区对应的文件描述符就绪
	- 套接口缓冲区有数据可读
	- 连接的读一半关闭,即接收到FIN段,读操作将返巨0
	- 如果是监听套接口,已完成连接队列不为空时。
	- 套接口上发生了一个错误待处理,错误可以通过getsockopt指定`SO_ERROR`选项来获取。
- 写缓冲区：检测写缓冲区是否可以写(有没有容量),如果有容量可以写,缓冲区对应的文件描述符就绪
	- 套接口发送缓冲区有空间容纳数据。
	- 连接的写一半关闭。即收到RST段之后,再次调用write操作。
	- 套接口上发牛了一个错误待处理,错误可以通讨getsockopt指定`SO_ERROR`选项来获取。
- 读写异常：检测读写缓冲区是否有异常,如果有该缓冲区对应的文件描述符就绪

委托检测的文件描述符被遍历检测完毕之后,已就绪的这些满足条件的文件描述符会通过`select()`的参数分3个集合传出,程序猿得到这几个集合之后就可以分情况依次处理了。

下面来看一下这个函数的函数原型：

```c
#include <sys/select.h>
struct timeval {
    time_t      tv_sec;         /* seconds */
    suseconds_t tv_usec;        /* microseconds */
};

int select(int nfds, fd_set *readfds, fd_set *writefds,
           fd_set *exceptfds, struct timeval * timeout);
```

- 函数参数：
	- nfds：委托内核检测的这三个集合中最大的文件描述符 + 1
		- 内核需要线性遍历这些集合中的文件描述符,这个值是循环结束的条件
		- 在Window中这个参数是无效的,指定为-1即可
	- readfds：文件描述符的集合, 内核只检测这个集合中文件描述符对应的读缓冲区
		- `传入传出参数`,读集合一般情况下都是需要检测的,这样才知道通过哪个文件描述符接收数据
	- writefds：文件描述符的集合, 内核只检测这个集合中文件描述符对应的写缓冲区
		- `传入传出参数`,如果不需要使用这个参数可以指定为NULL
	- exceptfds：文件描述符的集合, 内核检测集合中文件描述符是否有异常状态
		- `传入传出参数`,如果不需要使用这个参数可以指定为NULL
	- timeout：超时时长,用来强制解除select()函数的阻塞的
		- NULL：函数检测不到就绪的文件描述符会一直阻塞。
		- 等待固定时长(秒)：函数检测不到就绪的文件描述符,在指定时长之后强制解除阻塞,函数返回0
		- 不等待：函数不会阻塞,直接将该参数对应的结构体初始化为0即可。
- 函数返回值：
	- 大于0：成功,返回集合中已就绪的文件描述符的总个数
	- 等于-1：函数调用失败
	- 等于0：超时,没有检测到就绪的文件描述符

另外初始化`fd_set`类型的参数还需要使用相关的一些列操作函数,具体如下：

```c
// 将文件描述符fd从set集合中删除 == 将fd对应的标志位设置为0        
void FD_CLR(int fd, fd_set *set);
// 判断文件描述符fd是否在set集合中 == 读一下fd对应的标志位到底是0还是1
int  FD_ISSET(int fd, fd_set *set);
// 将文件描述符fd添加到set集合中 == 将fd对应的标志位设置为1
void FD_SET(int fd, fd_set *set);
// 将set集合中, 所有文件文件描述符对应的标志位设置为0, 集合中没有添加任何文件描述符
void FD_ZERO(fd_set *set);
```

##### 细节描述
在`select()`函数中第2、3、4个参数都是`fd_set`类型,它表示一个文件描述符的集合,类似于信号集 sigset_t,这个类型的数据有128个字节,也就是1024个标志位,和内核中文件描述符表中的文件描述符个数是一样的。
```c
sizeof(fd_set) = 128 字节 * 8 = 1024 bit      // int [32]
```

这并不是巧合,而是故意为之。这块内存中的每一个bit 和 文件描述符表中的每一个文件描述符是一一对应的关系,这样就可以使用最小的存储空间将要表达的意思描述出来了。

下图中的fd_set中存储了要委托内核检测读缓冲区的文件描述符集合。

- 如果集合中的标志位为`0`代表`不检测`这个文件描述符状态
- 如果集合中的标志位为`1`代表`检测`这个文件描述符状态

<img src="https://www.subingwen.cn/linux/select/image-20210324233252448.png" alt="image.png" style="zoom:30%;" />
内核在遍历这个读集合的过程中,如果被检测的文件描述符对应的读缓冲区中没有数据,内核将修改这个文件描述符在读集合`fd_set`中对应的标志位,改为`0`,如果有数据那么这个标志位的值不变,还是`1`。

<img src="https://www.subingwen.cn/linux/select/image-20210324234324388.png" alt="image.png" style="zoom:30%;" />
当`select()`函数解除阻塞之后,被内核修改过的读集合通过参数传出,此时集合中只要标志位的值为`1`,那么它对应的文件描述符肯定是就绪的,我们就可以基于这个文件描述符和客户端建立新连接或者通信了。

##### 套接字I/O超时设置的方法

1. [[C语言#alarm|alarm]]
	- 但是alarm可以被用于其他地方,会造成冲突,所以不建议使用
2. 套接字选项
	- SO_SNDTIMEO
		- 用于设置套接字发送操作的超时时间。如果在指定的时间内数据没有被成功发送(例如,因为网络拥堵或对方不接收),发送函数将返回一个错误。
	- SO_RCVTIMEO
		- 用于设置套接字接收操作的超时时间。这意味着如果在指定时间内没有接收到任何数据,接收函数将返回一个错误。

	```c
	struct timeval tv; tv.tv_sec = 5; // 5秒 tv.tv_usec = 0; // 0微秒
	setsockopt(sock,SOL_SOCKET,SO_RCVTIMEO,(const char*)&tv, sizeof(tv));
	int ret=read(sock,buf,sizeof buf);
	if(ret==-1&&errno == EAGAIN || errno == EWOULDBLOCK){
		printf("Operation timed out.\n");
	}
	```
	- 这个也不经常使用,因为有些平台不支持

3. 用select实现超时
	- read_timeout函数封装
	- write_timeout函数封装
	- accept_timeout函数封装
	- connect_timeout函数封装



#### 并发处理
##### 处理流程

如果在服务器基于select实现并发,其处理流程如下：

1. 创建监听的套接字 `lfd = socket()`;
2. 将监听的套接字和本地的IP和端口绑定 `bind()`
3. 给监听的套接字设置监听 `listen()`
4. 创建一个文件描述符集合 `fd_set`,用于存储需要**检测读事件**的所有的文件描述符
	- 通过 `FD_ZERO()` 初始化
	- 通过 `FD_SET()` 将监听的文件描述符放入检测的读集合中
5. 循环调用`select()`,周期性的对所有的文件描述符进行检测
6. `select()` 解除阻塞返回,得到内核传出的满足条件的就绪的文件描述符集合
	- 通过`FD_ISSET()` 判断集合中的标志位是否为 1
		- 如果这个文件描述符是监听的文件描述符,调用 `accept()` 和客户端建立连接
			- 将得到的新的通信的文件描述符,通过`FD_SET()` 放入到检测集合中
		- 如果这个文件描述符是通信的文件描述符,调用通信函数和客户端通信
			- 如果客户端和服务器断开了连接,使用`FD_CLR()`将这个文件描述符从检测集合中删除
			- 如果没有断开连接,正常通信即可
7. 重复第6步

<img src="https://www.subingwen.cn/linux/select/image-20210324235635304.png" alt="image.png" style="zoom:40%;" />

##### 通信代码
**服务器端代码如下：**

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>

int main()
{
    // 1. 创建监听的fd
    int lfd = socket(AF_INET, SOCK_STREAM, 0);

    // 2. 绑定
    struct sockaddr_in addr;
    addr.sin_family = AF_INET;
    addr.sin_port = htons(9999);
    addr.sin_addr.s_addr = INADDR_ANY;
    bind(lfd, (struct sockaddr*)&addr, sizeof(addr));

    // 3. 设置监听
    listen(lfd, 128);

    // 将监听的fd的状态检测委托给内核检测
    int maxfd = lfd;
    // 初始化检测的读集合
    fd_set rdset;
    fd_set rdtemp;
    // 清零
    FD_ZERO(&rdset);
    // 将监听的lfd设置到检测的读集合中
    FD_SET(lfd, &rdset);
    // 通过select委托内核检测读集合中的文件描述符状态, 检测read缓冲区有没有数据
    // 如果有数据, select解除阻塞返回
    // 应该让内核持续检测
    while(1)
    {
        // 默认阻塞
        // rdset 中是委托内核检测的所有的文件描述符
        rdtemp = rdset;
        int num = select(maxfd+1, &rdtemp, NULL, NULL, NULL);
        // rdset中的数据被内核改写了, 只保留了发生变化的文件描述的标志位上的1, 没变化的改为0
        // 只要rdset中的fd对应的标志位为1 -> 缓冲区有数据了
        // 判断
        // 有没有新连接
        if(FD_ISSET(lfd, &rdtemp))
        {
            // 接受连接请求, 这个调用不阻塞
            struct sockaddr_in cliaddr;
            int cliLen = sizeof(cliaddr);
            int cfd = accept(lfd, (struct sockaddr*)&cliaddr, &cliLen);

            // 得到了有效的文件描述符
            // 通信的文件描述符添加到读集合
            // 在下一轮select检测的时候, 就能得到缓冲区的状态
            FD_SET(cfd, &rdset);
            // 重置最大的文件描述符
            maxfd = cfd > maxfd ? cfd : maxfd;
        }

        // 没有新连接, 通信
        for(int i=0; i<maxfd+1; ++i)
        {
			// 判断从监听的文件描述符之后到maxfd这个范围内的文件描述符是否读缓冲区有数据
            if(i != lfd && FD_ISSET(i, &rdtemp))
            {
                // 接收数据
                char buf[10] = {0};
                // 一次只能接收10个字节, 客户端一次发送100个字节
                // 一次是接收不完的, 文件描述符对应的读缓冲区中还有数据
                // 下一轮select检测的时候, 内核还会标记这个文件描述符缓冲区有数据 -> 再读一次
                // 	循环会一直持续, 知道缓冲区数据被读完位置
                int len = read(i, buf, sizeof(buf));
                if(len == 0)
                {
                    printf("客户端关闭了连接...\n");
                    // 将检测的文件描述符从读集合中删除
                    FD_CLR(i, &rdset);
                    close(i);
                }
                else if(len > 0)
                {
                    // 收到了数据
                    // 发送数据
                    write(i, buf, strlen(buf)+1);
                }
                else
                {
                    // 异常
                    perror("read");
                }
            }
        }
    }

    return 0;
}
```

> 在上面的代码中,创建了两个fd_set变量,用于保存要检测的读集合：

```c
// 初始化检测的读集合
fd_set rdset;
fd_set rdtemp;
```

> `rdset`用于保存要检测的原始数据,这个变量不能作为参数传递给select函数,因为在函数内部这个变量中的值会被内核修改,函数调用完毕返回之后,里边就不是原始数据了,大部分情况下是值为1的标志位变少了,不可能每一轮检测,所有的文件描述符都是就行的状态。因此需要通过`rdtemp`变量将原始数据传递给内核,`select()` 调用完毕之后再将内核数据传出,这两个变量的功能是不一样的。

客户端代码:

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>

int main()
{
    // 1. 创建用于通信的套接字
    int fd = socket(AF_INET, SOCK_STREAM, 0);
    if(fd == -1)
    {
        perror("socket");
        exit(0);
    }

    // 2. 连接服务器
    struct sockaddr_in addr;
    addr.sin_family = AF_INET;     // ipv4
    addr.sin_port = htons(9999);   // 服务器监听的端口, 字节序应该是网络字节序
    inet_pton(AF_INET, "127.0.0.1", &addr.sin_addr.s_addr);
    int ret = connect(fd, (struct sockaddr*)&addr, sizeof(addr));
    if(ret == -1)
    {
        perror("connect");
        exit(0);
    }

    // 通信
    while(1)
    {
        // 读数据
        char recvBuf[1024];
        // 写数据
        // sprintf(recvBuf, "data: %d\n", i++);
        fgets(recvBuf, sizeof(recvBuf), stdin);
        write(fd, recvBuf, strlen(recvBuf)+1);
        // 如果客户端没有发送数据, 默认阻塞
        read(fd, recvBuf, sizeof(recvBuf));
        printf("recv buf: %s\n", recvBuf);
        sleep(1);
    }

    // 释放资源
    close(fd); 

    return 0;
}
```

客户端不需要使用IO多路转接进行处理,因为客户端和服务器的对应关系是 1：N,也就是说客户端是比较专一的,只能和一个连接成功的服务器通信。

虽然使用select这种IO多路转接技术可以降低系统开销,提高程序效率,但是它也有局限性：

1. 待检测集合(第2、3、4个参数)需要频繁的在用户区和内核区之间进行数据的拷贝,效率低
2. 内核对于select传递进来的待检测集合的检测方式是**线性**的
	- 如果集合内待检测的文件描述符很多,检测效率会比较低
	- 如果集合内待检测的文件描述符相对较少,检测效率会比较高
3. `使用select能够检测的最大文件描述符个数有上限,默认是1024,这是在内核中被写死了的`。
4. 一个进程能打开的最大文件描述符限制。这可以通过 调整内核参数。

### poll

#### poll函数

poll的机制与select类似,与select在本质上没有多大差别,使用方法也类似,下面的是对于二者的对比：

- 内核对应文件描述符的检测也是以线性的方式进行轮询,根据描述符的状态进行处理
- poll和select检测的文件描述符集合会在检测过程中频繁的进行用户区和内核区的拷贝,它的开销随着文件描述符数量的增加而线性增大,从而效率也会越来越低。
- select检测的文件描述符个数上限是1024,poll没有最大文件描述符数量的限制
- select可以跨平台使用,poll只能在**Linux平台使用**

poll函数的函数原型如下：

```c
#include <poll.h>
// 每个委托poll检测的fd都对应这样一个结构体
struct pollfd{
    int   fd;         /* 委托内核检测的文件描述符 */
    short events;     /* 委托内核检测文件描述符的什么事件 */
    short revents;    /* 文件描述符实际发生的事件 -> 传出 */
}

struct pollfd myfd[100];
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```


- 函数参数：
	- fds: 这是一个`struct pollfd`类型的数组, 里边存储了待检测的文件描述符的信息,这个数组中有三个成员：
		- fd：委托内核检测的文件描述符
		- events：委托内核检测的fd事件(输入、输出、错误),每一个事件有多个取值
		- revents：这是一个传出参数,数据由内核写入,存储内核检测之后的结果

		<img src="https://www.subingwen.cn/linux/poll/1558308141721.png" alt="image.png" style="zoom:60%;" />
	- nfds: 这是第一个参数数组中最后一个有效元素的下标 + 1(也可以指定参数1数组的元素总个数)
	- timeout: 指定poll函数的阻塞时长
		- -1：一直阻塞,直到检测的集合中有就绪的文件描述符(有事件产生)解除阻塞
		- 0：不阻塞,不管检测集合中有没有已就绪的文件描述符,函数马上返回
		- 大于0：阻塞指定的毫秒(ms)数之后,解除阻塞
- 函数返回值：
	- 失败： 返回-1
	- 成功：返回一个大于0的整数,表示检测的集合中已就绪的文件描述符的总个数

#### 测试代码
**服务器端**
```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>
#include <sys/select.h>
#include <poll.h>

int main()
{
    // 1.创建套接字
    int lfd = socket(AF_INET, SOCK_STREAM, 0);
    if(lfd == -1)
    {
        perror("socket");
        exit(0);
    }
    // 2. 绑定 ip, port
    struct sockaddr_in addr;
    addr.sin_port = htons(9999);
    addr.sin_family = AF_INET;
    addr.sin_addr.s_addr = INADDR_ANY;
    int ret = bind(lfd, (struct sockaddr*)&addr, sizeof(addr));
    if(ret == -1)
    {
        perror("bind");
        exit(0);
    }
    // 3. 监听
    ret = listen(lfd, 100);
    if(ret == -1)
    {
        perror("listen");
        exit(0);
    }
    
    // 4. 等待连接 -> 循环
    // 检测 -> 读缓冲区, 委托内核去处理
    // 数据初始化, 创建自定义的文件描述符集
    struct pollfd fds[1024];
    // 初始化
    for(int i=0; i<1024; ++i)
    {
        fds[i].fd = -1;
        fds[i].events = POLLIN;
    }
    fds[0].fd = lfd;

    int maxfd = 0;
    while(1)
    {
        // 委托内核检测
        ret = poll(fds, maxfd+1, -1);
        if(ret == -1)
        {
            perror("select");
            exit(0);
        }

        // 检测的度缓冲区有变化
        // 有新连接
        if(fds[0].revents & POLLIN)
        {
            // 接收连接请求
            struct sockaddr_in sockcli;
            int len = sizeof(sockcli);
            // 这个accept是不会阻塞的
            int connfd = accept(lfd, (struct sockaddr*)&sockcli, &len);
            // 委托内核检测connfd的读缓冲区
            int i;
            for(i=0; i<1024; ++i)
            {
                if(fds[i].fd == -1)
                {
                    fds[i].fd = connfd;
                    break;
                }
            }
            maxfd = i > maxfd ? i : maxfd;
        }
        // 通信, 有客户端发送数据过来
        for(int i=1; i<=maxfd; ++i)
        {
            // 如果在集合中, 说明读缓冲区有数据
            if(fds[i].revents & POLLIN)
            {
                char buf[128];
                int ret = read(fds[i].fd, buf, sizeof(buf));
                if(ret == -1)
                {
                    perror("read");
                    exit(0);
                }
                else if(ret == 0)
                {
                    printf("对方已经关闭了连接...\n");
                    close(fds[i].fd);
                    fds[i].fd = -1;
                }
                else
                {
                    printf("客户端say: %s\n", buf);
                    write(fds[i].fd, buf, strlen(buf)+1);
                }
            }
        }
    }
    close(lfd);
    return 0;
}
```

从上面的测试代码可以得知,使用poll和select进行IO多路转接的处理思路是完全相同的,但是使用poll编写的代码看起来会更直观一些,select使用的位图的方式来标记要委托内核检测的文件描述符(每个比特位对应一个唯一的文件描述符),并且对这个`fd_set`类型的位图变量进行读写还需要借助一系列的宏函数,操作比较麻烦。而`poll`直接将要检测的文件描述符的相关信息封装到了一个结构体`struct pollfd`中,我们可以直接读写这个结构体变量。
并且poll没有像`fd_set`那样的`FD_SETSIZE`的限制,另外poll的第二个参数有两种赋值方式,但是都和第一个参数的数组有关系：

- 使用参数1数组的元素个数
- 使用参数1数组中存储的最后一个有效元素对应的下标值 + 1
内核会根据第二个参数传递的值对参数1数组中的文件描述符进行线性遍历,这一点和select也是类似的。

客户端

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>

int main()
{
    // 1. 创建用于通信的套接字
    int fd = socket(AF_INET, SOCK_STREAM, 0);
    if(fd == -1)
    {
        perror("socket");
        exit(0);
    }

    // 2. 连接服务器
    struct sockaddr_in addr;
    addr.sin_family = AF_INET;  // ipv4
    addr.sin_port = htons(9999);   // 服务器监听的端口, 字节序应该是网络字节序
    inet_pton(AF_INET, "127.0.0.1", &addr.sin_addr.s_addr);
    int ret = connect(fd, (struct sockaddr*)&addr, sizeof(addr));
    if(ret == -1)
    {
        perror("connect");
        exit(0);
    }

    // 通信
    while(1)
    {
        // 读数据
        char recvBuf[1024];
        // 写数据
        // sprintf(recvBuf, "data: %d\n", i++);
        fgets(recvBuf, sizeof(recvBuf), stdin);
        write(fd, recvBuf, strlen(recvBuf)+1);
        // 如果客户端没有发送数据, 默认阻塞
        read(fd, recvBuf, sizeof(recvBuf));
        printf("recv buf: %s\n", recvBuf);
        sleep(1);
    }
    // 释放资源
    close(fd); 
    return 0;
}
```


### epoll
#### 概述

epoll 全称 event poll,是 linux 内核实现IO多路转接/复用(IO multiplexing)的一个实现。IO多路转接的意思是在一个操作里同时监听多个输入输出源,在其中一个或多个输入输出源可用的时候返回,然后对其的进行读写操作。epoll是select和poll的升级版,相较于这两个前辈,epoll改进了工作方式,因此它更加高效。

- `对于待检测集合select和poll是基于线性方式处理的,epoll是基于红黑树来管理待检测集合的`。
- `select和poll每次都会线性扫描整个待检测集合,集合越大速度越慢,epoll使用的是回调机制,效率高,处理效率也不会随着检测集合的变大而下降`
- `select和poll工作过程中存在内核/用户空间数据的频繁拷贝问题,在epoll中内核和用户区使用的是共享内存(基于mmap内存映射区实现),省去了不必要的内存拷贝`。
- `程序猿需要对select和poll返回的集合进行判断才能知道哪些文件描述符是就绪的,通过epoll可以直接得到已就绪的文件描述符集合,无需再次检测`
- 使用epoll没有最大文件描述符的限制,仅受系统中进程能打开的最大文件数目限制

当多路复用的文件数量庞大、IO流量频繁的时候,一般不太适合使用select()和poll(),这种情况下select()和poll()表现较差,推荐使用epoll()。

#### 操作函数
在epoll中一共提供是三个API函数,分别处理不同的操作,函数原型如下：
```c
#include <sys/epoll.h>
// 创建epoll实例,通过一棵红黑树管理待检测集合
int epoll_create(int size);
// flags 参数用于控制 epoll 实例的行为。目前,这个参数可以是 0(表示和 epoll_create 相同的行为)或 EPOLL_CLOEXEC。EPOLL_CLOEXEC 标志是 epoll_create1 特有的,用于设置当执行 exec 系列函数时,返回的文件描述符将被自动关闭。这是一个用于文件描述符“继承”行为控制的安全特性,可以避免在多进程程序中由于文件描述符泄露导致的资源管理问题。
int epoll_create1(int flags);
// 管理红黑树上的文件描述符(添加、修改、删除)
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
// 检测epoll树中是否有就绪的文件描述符
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

select/poll低效的原因之一是将“添加/维护待检测任务”和“阻塞进程/线程”两个步骤合二为一。每次调用select都需要这两步操作,然而大多数应用场景中,需要监视的socket个数相对固定,并不需要每次都修改。epoll将这两个操作分开,先用`epoll_ctl()`维护等待队列,再调用`epoll_wait()`阻塞进程(解耦)。通过下图的对比显而易见,epoll的效率得到了提升。

<img src="https://www.subingwen.cn/linux/epoll/image-20210403181746358.png" alt="image.png" style="zoom:30%;" />
`epoll_create()`函数的作用是创建一个红黑树模型的实例,用于管理待检测的文件描述符的集合。
```c
int epoll_create(int size);
```

- 函数参数 size：在Linux内核2.6.8版本以后,这个参数是被忽略的,只需要指定一个大于0的数值就可以了。
- 函数返回值：
	- 失败：返回-1
	- 成功：返回一个有效的文件描述符,通过这个文件描述符就可以访问创建的epoll实例了

`epoll_ctl()`函数的作用是管理红黑树实例上的节点,可以进行添加、删除、修改操作。

```c
// 联合体, 多个变量共用同一块内存        
typedef union epoll_data
{
  void *ptr; // 成员存储指针 我们在ctl函数存储,在wait函数传出就可以取出这个数据
  int fd; // 存储文件描述符
  uint32_t u32;
  uint64_t u64;
} epoll_data_t;

struct epoll_event {
	uint32_t     events;      /* Epoll events */
	epoll_data_t data;        /* User data variable */
};
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
```

- 函数参数：
	- `epfd：epoll_create()` 函数的返回值,通过这个参数找到epoll实例
	- `op`：这是一个枚举值,控制通过该函数执行什么操作
		- `EPOLL_CTL_ADD`：往epoll模型中添加新的节点
		- `EPOLL_CTL_MOD`：修改epoll模型中已经存在的节点
		- `EPOLL_CTL_DEL`：删除epoll模型中的指定的节点
	- fd：文件描述符,即要添加/修改/删除的文件描述符
	- `event`：epoll事件,用来修饰第三个参数对应的文件描述符的,指定检测这个文件描述符的什么事件
		- `events`：委托epoll检测的事件 
			- `EPOLLIN`：读事件, 接收数据, 检测读缓冲区,如果有数据该文件描述符就绪
			- `EPOLLOUT`：写事件, 发送数据, 检测写缓冲区,如果可写该文件描述符就绪
			- `EPOLLERR`：异常事件
	- `data`：用户数据变量,这是一个联合体类型,通常情况下使用里边的fd成员,用于存储待检测的文件描述符的值,在调用epoll_wait()函数的时候这个值会被传出。
- 函数返回值：
	- 失败：返回-1
	- 成功：返回0

`epoll_wait()`函数的作用是检测创建的epoll实例中有没有就绪的文件描述符。

```c
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

- 函数参数：
	- `epfd：epoll_create()` 函数的返回值, 通过这个参数找到epoll实例
	- `events`：传出参数, 这是一个结构体数组的地址, 里边存储了已就绪的文件描述符的信息
	- `maxevents`：修饰第二个参数, 结构体数组的容量(元素个数)
	- `timeout`：如果检测的epoll实例中没有已就绪的文件描述符,该函数阻塞的时长, 单位ms 毫秒
	- 0：函数不阻塞,不管epoll实例中有没有就绪的文件描述符,函数被调用后都直接返回
	- 大于0：如果epoll实例中没有已就绪的文件描述符,函数阻塞对应的毫秒数再返回
	- -1：函数一直阻塞,直到epoll实例中有已就绪的文件描述符之后才解除阻塞
- 函数返回值：
	- 成功：
		- 等于0：函数是阻塞被强制解除了, 没有检测到满足条件的文件描述符
		- 大于0：检测到的已就绪的文件描述符的总个数
	- 失败：返回-1

#### epoll的使用
##### 操作步骤

在服务器端使用epoll进行IO多路转接的操作步骤如下：

1. 创建监听的套接字
	```c
	int lfd = socket(AF_INET, SOCK_STREAM, 0);
	```
1. 设置端口复用(可选)
	```c
	int opt = 1;
	setsockopt(lfd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));
	```
3. 使用本地的IP与端口和监听的套接字进行绑定
	```c
	int ret = bind(lfd, (struct sockaddr*)&serv_addr, sizeof(serv_addr));
	```
4. 给监听的套接字设置监听
	```c
	listen(lfd, 128);
	```
5. 创建epoll实例对象
	```c
	int epfd = epoll_create(100);
	```
6. 将用于监听的套接字添加到epoll实例中
	```c
	struct epoll_event ev;
	ev.events = EPOLLIN;    // 检测lfd读读缓冲区是否有数据
	ev.data.fd = lfd;
	int ret = epoll_ctl(epfd, EPOLL_CTL_ADD, lfd, &ev);
	```
7. 检测添加到epoll实例中的文件描述符是否已就绪,并将这些已就绪的文件描述符进行处理
	```c
	int num = epoll_wait(epfd, evs, size, -1);
	```
	- 如果是监听的文件描述符,和新客户端建立连接,将得到的文件描述符添加到epoll实例中
	```c
	int cfd = accept(curfd, NULL, NULL);
	ev.events = EPOLLIN;
	ev.data.fd = cfd;
	// 新得到的文件描述符添加到epoll模型中, 下一轮循环的时候就可以被检测了
	epoll_ctl(epfd, EPOLL_CTL_ADD, cfd, &ev);
	```
	- 如果是通信的文件描述符,和对应的客户端通信,如果连接已断开,将该文件描述符从epoll实例中删除
	```c
	int len = recv(curfd, buf, sizeof(buf), 0);
	if(len == 0)
	{
	    // 将这个文件描述符从epoll模型中删除
	    epoll_ctl(epfd, EPOLL_CTL_DEL, curfd, NULL);
	    close(curfd);
	}
	else if(len > 0)
	{
	    send(curfd, buf, len, 0);
	}
```
8. 重复第7步的操作

##### 示例代码

```c
#include <stdio.h>
#include <ctype.h>
#include <unistd.h>
#include <stdlib.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <string.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <sys/epoll.h>

// server
int main(int argc, const char* argv[])
{
    // 创建监听的套接字
    int lfd = socket(AF_INET, SOCK_STREAM, 0);
    if(lfd == -1)
    {
        perror("socket error");
        exit(1);
    }

    // 绑定
    struct sockaddr_in serv_addr;
    memset(&serv_addr, 0, sizeof(serv_addr));
    serv_addr.sin_family = AF_INET;
    serv_addr.sin_port = htons(9999);
    serv_addr.sin_addr.s_addr = htonl(INADDR_ANY);  // 本地多有的ＩＰ
    
    // 设置端口复用
    int opt = 1;
    setsockopt(lfd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

    // 绑定端口
    int ret = bind(lfd, (struct sockaddr*)&serv_addr, sizeof(serv_addr));
    if(ret == -1)
    {
        perror("bind error");
        exit(1);
    }

    // 监听
    ret = listen(lfd, 64);
    if(ret == -1)
    {
        perror("listen error");
        exit(1);
    }

    // 现在只有监听的文件描述符
    // 所有的文件描述符对应读写缓冲区状态都是委托内核进行检测的epoll
    // 创建一个epoll模型
    int epfd = epoll_create(100);
    if(epfd == -1)
    {
        perror("epoll_create");
        exit(0);
    }

    // 往epoll实例中添加需要检测的节点, 现在只有监听的文件描述符
    struct epoll_event ev;
    ev.events = EPOLLIN;    // 检测lfd读读缓冲区是否有数据
    ev.data.fd = lfd;
    ret = epoll_ctl(epfd, EPOLL_CTL_ADD, lfd, &ev);
    if(ret == -1)
    {
        perror("epoll_ctl");
        exit(0);
    }

    struct epoll_event evs[1024];
    int size = sizeof(evs) / sizeof(struct epoll_event);
    // 持续检测
    while(1)
    {
        // 调用一次, 检测一次
        int num = epoll_wait(epfd, evs, size, -1);
        for(int i=0; i<num; ++i)
        {
            // 取出当前的文件描述符
            int curfd = evs[i].data.fd;
            // 判断这个文件描述符是不是用于监听的
            if(curfd == lfd)
            {
                // 建立新的连接
                int cfd = accept(curfd, NULL, NULL);
                // 新得到的文件描述符添加到epoll模型中, 下一轮循环的时候就可以被检测了
                ev.events = EPOLLIN;    // 读缓冲区是否有数据
                ev.data.fd = cfd;
                ret = epoll_ctl(epfd, EPOLL_CTL_ADD, cfd, &ev);
                if(ret == -1)
                {
                    perror("epoll_ctl-accept");
                    exit(0);
                }
            }
            else
            {
                // 处理通信的文件描述符
                // 接收数据
                char buf[1024];
                memset(buf, 0, sizeof(buf));
                int len = recv(curfd, buf, sizeof(buf), 0);
                if(len == 0)
                {
                    printf("客户端已经断开了连接\n");
                    // 将这个文件描述符从epoll模型中删除
                    epoll_ctl(epfd, EPOLL_CTL_DEL, curfd, NULL);
                    close(curfd);
                }
                else if(len > 0)
                {
                    printf("客户端say: %s\n", buf);
                    send(curfd, buf, len, 0);
                }
                else
                {
                    perror("recv");
                    exit(0);
                } 
            }
        }
    }

    return 0;
}
```

当在服务器端循环调用`epoll_wait()`的时候,就会得到一个就绪列表,并通过该函数的第二个参数传出：
```c
struct epoll_event evs[1024];
int num = epoll_wait(epfd, evs, size, -1);
```

每当`epoll_wait()`函数返回一次,在`evs`中最多可以存储`size`个已就绪的文件描述符信息,但是在这个数组中实际存储的有效元素个数为`num`个,如果在这个`epoll`实例的红黑树中已就绪的文件描述符很多,并且`evs`数组无法将这些信息全部传出,那么这些信息会在下一次`epoll_wait()`函数返回的时候被传出。

通过`evs`数组被传递出的每一个有效元素里边都包含了已就绪的文件描述符的相关信息,这些信息并不是凭空得来的,这取决于我们在往epoll实例中添加节点的时候,往节点中初始化了哪些数据：
```c
struct epoll_event ev;
// 节点初始化
ev.events = EPOLLIN;    
ev.data.fd = lfd;	// 使用了联合体中 fd 成员
// 添加待检测节点到epoll实例中
int ret = epoll_ctl(epfd, EPOLL_CTL_ADD, lfd, &ev);
```

在添加节点的时候,需要对这个`struct epoll_event`类型的节点进行初始化,当这个节点对应的文件描述符变为已就绪状态,这些被传入的初始化信息就会被原样传出,这个对应关系必须要搞清楚。


#### epoll的工作模式
##### 水平模式
水平模式可以简称为LT模式,`LT(level triggered)是缺省的工作方式,并且同时支持block和no-block socket`。在这种做法中,内核通知使用者哪些文件描述符已经就绪,之后就可以对这些已就绪的文件描述符进行IO操作了。如果我们不作任何操作,内核还是会继续通知使用者。

**水平模式的特点**：

- 读事件：<font color=#ff0000>如果文件描述符对应的读缓冲区还有数据,读事件就会被触发,epoll_wait()解除阻塞</font>
	- 当读事件被触发,`epoll_wait()`解除阻塞,之后就可以接收数据了
	- 如果接收数据的buf很小,不能全部将缓冲区数据读出,那么读事件会继续被触发,直到数据被全部读出,如果接收数据的内存相对较大,读数据的效率也会相对较高(减少了读数据的次数)
	- 因为读数据是被动的,必须要通过读事件才能知道有数据到达了,因此对于读事件的检测是必须的
- 写事件：如果文件描述符对应的写缓冲区可写,写事件就会被触发,`epoll_wait()`解除阻塞
	- 当写事件被触发,`epoll_wait()`解除阻塞,之后就可以将数据写入到写缓冲区了
	- `写事件的触发发生在写数据之前而不是之后`,被写入到写缓冲区中的数据是由内核自动发送出去的
	- 如果写缓冲区没有被写满,写事件会一直被触发
	- `因为写数据是主动的,并且写缓冲区一般情况下都是可写的(缓冲区不满),因此对于写事件的检测不是必须的`

#### 边沿模式
边沿模式可以简称为ET模式,`ET(edge-triggered)是高速工作方式,只支持no-block socket`。在这种模式下,`当文件描述符从未就绪变为就绪时,内核会通过epoll通知使用者。然后它会假设使用者知道文件描述符已经就绪,并且不会再为那个文件描述符发送更多的就绪通知(only once)`。如果我们对这个文件描述符做IO操作,从而导致它再次变成未就绪,当这个未就绪的文件描述符再次变成就绪状态,内核会再次进行通知,并且还是只通知一次。`ET模式在很大程度上减少了epoll事件被重复触发的次数,因此效率要比LT模式高`。

**边沿模式的特点**:

- 读事件：<font color=#ff0000>当读缓冲区有新的数据进入,读事件被触发一次,没有新数据不会触发该事件</font>
	- 如果有新数据进入到读缓冲区,读事件被触发,`epoll_wait()`解除阻塞
	- 读事件被触发,可以通过调用`read()/recv()`函数将缓冲区数据读出
		- 如果数据没有被全部读走,并且没有新数据进入,读事件不会再次触发,只通知一次
		- 如果数据被全部读走或者只读走一部分,此时有新数据进入,读事件被触发,并且只通知一次
- 写事件：<font color=#ff0000>当写缓冲区状态可写,写事件只会触发一次</font>
	- 如果写缓冲区被检测到可写,写事件被触发,`epoll_wait()`解除阻塞
	- 写事件被触发,就可以通过调用`write()/send()`函数,将数据写入到写缓冲区中
		- 写缓冲区从不满到被写满,期间写事件只会被触发一次
		- 写缓冲区从满到不满,状态变为可写,写事件只会被触发一次

综上所述：epoll的边沿模式下 `epoll_wait()`检测到文件描述符有新事件才会通知,如果不是新的事件就不通知,通知的次数比水平模式少,效率比水平模式要高。

##### ET模式的设置
边沿模式不是默认的epoll模式,需要额外进行设置。epoll设置边沿模式是非常简单的,epoll管理的红黑树示例中每个节点都是`struct epoll_event`类型,只需要将`EPOLLET`添加到结构体的`events`成员中即可：

```c
struct epoll_event ev;
ev.events = EPOLLIN | EPOLLET;	// 设置边沿模式
```

示例代码如下：

```c
int num = epoll_wait(epfd, evs, size, -1);
for(int i=0; i<num; ++i)
{
    // 取出当前的文件描述符
    int curfd = evs[i].data.fd;
    // 判断这个文件描述符是不是用于监听的
    if(curfd == lfd)
    {
        // 建立新的连接
        int cfd = accept(curfd, NULL, NULL);
        // 新得到的文件描述符添加到epoll模型中, 下一轮循环的时候就可以被检测了
        // 读缓冲区是否有数据, 并且将文件描述符设置为边沿模式
        struct epoll_event ev;
        ev.events = EPOLLIN | EPOLLET;   
        ev.data.fd = cfd;
        ret = epoll_ctl(epfd, EPOLL_CTL_ADD, cfd, &ev);
        if(ret == -1)
        {
            perror("epoll_ctl-accept");
            exit(0);
        }
    }
}
```

##### 设置非阻塞
对于写事件的触发一般情况下是不需要进行检测的,因为写缓冲区大部分情况下都是有足够的空间可以进行数据的写入。对于读事件的触发就必须要检测了,因为服务器也不知道客户端什么时候发送数据,如果使用epoll的边沿模式进行读事件的检测,有新数据达到只会通知一次,那么<font color=#ff0000>必须要保证得到通知后将数据全部从读缓冲区中读出</font>。那么,应该如何读这些数据呢？

- 方式1：准备一块特别大的内存,用于存储从读缓冲区中读出的数据,但是这种方式有很大的弊端：
	- 内存的大小没有办法界定,太大浪费内存,太小又不够用
	- 系统能够分配的最大堆内存也是有上限的,栈内存就更不必多言了
- 方式2：循环接收数据
```c
int len = 0;
while((len = recv(curfd, buf, sizeof(buf), 0)) > 0)
{
    // 数据处理...
}
```

这样做也是有弊端的,因为套接字操作默认是阻塞的,当读缓冲区数据被读完之后,读操作就阻塞了也就是调用的`read()/recv()`函数被阻塞了,当前进程/线程被阻塞之后就无法处理其他操作了。

要解决阻塞问题,就需要将套接字默认的阻塞行为修改为非阻塞,需要使用`fcntl()`函数进行处理：

```c
// 设置完成之后, 读写都变成了非阻塞模式
int flag = fcntl(cfd, F_GETFL);
flag |= O_NONBLOCK;                                                        
fcntl(cfd, F_SETFL, flag);
```

[[C语言#fcntl|fcntl函数的使用详解]]

通过上述分析就可以得出一个结论：<font color=#ff0000>epoll在边沿模式下,必须要将套接字设置为非阻塞模式</font>,但是,这样就会引发另外的一个bug,在非阻塞模式下,循环地将读缓冲区数据读到本地内存中,当缓冲区数据被读完了,调用的`read()/recv()`函数还会继续从缓冲区中读数据,此时函数调用就失败了,返回-1,对应的全局变量 errno 值为 `EAGAIN` 或者 `EWOULDBLOCK`如果打印错误信息会得到如下的信息：`Resource temporarily unavailable`

```c
// 非阻塞模式下recv() / read()函数返回值 len == -1
int len = recv(curfd, buf, sizeof(buf), 0);
if(len == -1)
{
    if(errno == EAGAIN)
    {
        printf("数据读完了...\n");
    }
    else
    {
        perror("recv");
        exit(0);
    }
}
```

#### 示例代码

```c
#include <stdio.h>
#include <ctype.h>
#include <unistd.h>
#include <stdlib.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <string.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <fcntl.h>
#include <errno.h>

// server
int main(int argc, const char* argv[])
{
    // 创建监听的套接字
    int lfd = socket(AF_INET, SOCK_STREAM, 0);
    if(lfd == -1)
    {
        perror("socket error");
        exit(1);
    }

    // 绑定
    struct sockaddr_in serv_addr;
    memset(&serv_addr, 0, sizeof(serv_addr));
    serv_addr.sin_family = AF_INET;
    serv_addr.sin_port = htons(9999);
    serv_addr.sin_addr.s_addr = htonl(INADDR_ANY);  // 本地多有的ＩＰ
    // 127.0.0.1
    // inet_pton(AF_INET, "127.0.0.1", &serv_addr.sin_addr.s_addr);
    
    // 设置端口复用
    int opt = 1;
    setsockopt(lfd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

    // 绑定端口
    int ret = bind(lfd, (struct sockaddr*)&serv_addr, sizeof(serv_addr));
    if(ret == -1)
    {
        perror("bind error");
        exit(1);
    }

    // 监听
    ret = listen(lfd, 64);
    if(ret == -1)
    {
        perror("listen error");
        exit(1);
    }

    // 现在只有监听的文件描述符
    // 所有的文件描述符对应读写缓冲区状态都是委托内核进行检测的epoll
    // 创建一个epoll模型
    int epfd = epoll_create(100);
    if(epfd == -1)
    {
        perror("epoll_create");
        exit(0);
    }

    // 往epoll实例中添加需要检测的节点, 现在只有监听的文件描述符
    struct epoll_event ev;
    ev.events = EPOLLIN;    // 检测lfd读读缓冲区是否有数据
    ev.data.fd = lfd;
    ret = epoll_ctl(epfd, EPOLL_CTL_ADD, lfd, &ev);
    if(ret == -1)
    {
        perror("epoll_ctl");
        exit(0);
    }


    struct epoll_event evs[1024];
    int size = sizeof(evs) / sizeof(struct epoll_event);
    // 持续检测
    while(1)
    {
        // 调用一次, 检测一次
        int num = epoll_wait(epfd, evs, size, -1);
        printf("==== num: %d\n", num);

        for(int i=0; i<num; ++i)
        {
            // 取出当前的文件描述符
            int curfd = evs[i].data.fd;
            // 判断这个文件描述符是不是用于监听的
            if(curfd == lfd)
            {
                // 建立新的连接
                int cfd = accept(curfd, NULL, NULL);
                // 将文件描述符设置为非阻塞
                // 得到文件描述符的属性
                int flag = fcntl(cfd, F_GETFL);
                flag |= O_NONBLOCK;
                fcntl(cfd, F_SETFL, flag);
                // 新得到的文件描述符添加到epoll模型中, 下一轮循环的时候就可以被检测了
                // 通信的文件描述符检测读缓冲区数据的时候设置为边沿模式
                ev.events = EPOLLIN | EPOLLET;    // 读缓冲区是否有数据
                ev.data.fd = cfd;
                ret = epoll_ctl(epfd, EPOLL_CTL_ADD, cfd, &ev);
                if(ret == -1)
                {
                    perror("epoll_ctl-accept");
                    exit(0);
                }
            }
            else
            {
                // 处理通信的文件描述符
                // 接收数据
                char buf[5];
                memset(buf, 0, sizeof(buf));
                // 循环读数据
                while(1)
                {
                    int len = recv(curfd, buf, sizeof(buf), 0);
                    if(len == 0)
                    {
                        // 非阻塞模式下和阻塞模式是一样的 => 判断对方是否断开连接
                        printf("客户端断开了连接...\n");
                        // 将这个文件描述符从epoll模型中删除
                        epoll_ctl(epfd, EPOLL_CTL_DEL, curfd, NULL);
                        close(curfd);
                        break;
                    }
                    else if(len > 0)
                    {
                        // 通信
                        // 接收的数据打印到终端
                        write(STDOUT_FILENO, buf, len);
                        // 发送数据
                        send(curfd, buf, len, 0);
                    }
                    else
                    {
                        // len == -1
                        if(errno == EAGAIN)
                        {
                            printf("数据读完了...\n");
                            break;
                        }
                        else
                        {
                            perror("recv");
                            exit(0);
                        }
                    }
                }
            }
        }
    }

    return 0;
}
```


表9-2 select、poll和 epoll的区别

|        系统调用         |                                    select                                    |                                      poll                                       |                                            epoll                                            |
| :-----------------: | :--------------------------------------------------------------------------: | :-----------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------: |
|        事件集合         | 用户通过3个参数传入感兴趣的可读、可写及异常等事件,内核通过对这些参数的在线修改来反馈其中的就绪事件。这使得用户每次调用select都要重置这3个参数。 | 用户通过pollfd_events传入感兴趣的事件,内核通过修改pollfd_revents反馈其中就绪的事件。统一处理所有事件类型,因此只需一个事件集参数。 | 内核通过一个事件表直接管理用户感兴趣的所有事件。因此每次调用epoll_wait时,无须反复传人用户感兴趣的事件。epoll_wait系统调用的参数events仅用来反馈就绪的事件。 |
| 应用程序索引就绪文件描述符的时间复杂度 |                                     O(n)                                     |                                      O(n)                                       |                                            O(1)                                             |
|     最大支持文件描述符数      |                                   -般有最大值限制                                   |                                      65535                                      |                                            65535                                            |
|        工作模式         |                                      LT                                      |                                       LT                                        |                                          支持ET高效模式                                           |
|      内核实现和工作效率      |                          采用轮询方式来检测就绪事件,算法时间复杂度为O(n)                          |                           采用轮询方式来检测就绪事件,算法时间复杂度为O(n)                            |                                 采用回调方式来检测就绪事件,算法时间复杂度为O(1)                                  |


## 信号驱动IO 

> SIGIO信号:
> 原理:我们为目标文件描述符指定一个宿主进程,宿主进程会捕获到SIGIO信号,当目标文件描述符上有事件发生时,SIGIO信号的信号处理函数将被触发,于是在该信号处理函数中,我们就可以进行非阻塞I/O操作.

信号驱动IO是与内核建立**SIGIO的信号**关联并设置回调,当内核有FD就绪时,会发出SIGIO信号通知用户,期间用户应用可以执行其它业务,无需**阻塞等待**。

<img src="https://article.biliimg.com/bfs/article/8b3c9f0e6b2a6971384805233f5807a18607d41e.png" style="zoom:67%;" />

阶段一：
* 用户进程调用`sigaction`,注册信号处理函数
* 内核返回成功,开始监听FD
* 用户进程不阻塞等待,可以执行其它业务
* 当内核数据就绪后,回调用户进程的SIGIO处理函数

阶段二：
* 收到SIGIO回调信号
* 调用`recvfrom`,读取
* 内核将数据拷贝到用户空间
* 用户进程处理数据

当有大量IO操作时,信号较多,SIGIO处理函数不能及时处理可能**导致信号队列溢出**,而且内核空间与用户空间的频繁信号交互性能也较低。

## 异步IO 


> 阻塞I/O、I/O复用和信号驱动I/O都是同步I/O模型。因为在这三种I/O模型中,**I/O的读写操作,都是在I/O事件发生之后**,由应用程序来完成的。而POSIX规范所定义的异步I/O模型则不同。对异步I/O而言,**用户可以直接对I/O执行读写操作**,这些操作**告诉内核用户读写缓冲区的位置**,以及I/O操作完成之后内核如何通知应用程序。异步I/O的读写操作总是立即返回,而不论I/O是否是阻塞的,因为真正的读写操作已经由内核接管。也就是说,同步I/O模型要求用户代码自行执行I/O操作(将数据从内核缓冲区读入用户缓冲区,或将数据从用户缓冲区写入内核缓冲区),而异步I/O机制则由内核来执行I/O操作(数据在内核缓冲区和用户缓冲区之间的移动是由内核在“后台”完成的)。可以这样认为,同步I/O向应用程序通知的是I/O就绪事件,而异步I/O向应用程序通知的是I/O完成事件。
> 
<img src="https://article.biliimg.com/bfs/article/e340cc6f459cd3709ecb438b815d8fa74bb5d823.png" alt="image.png" style="zoom:60%;" />

- 这种方式,不仅仅是用户态在试图读取数据后,不阻塞,而且当内核的数据准备完成后,也不会阻塞
- 他会由内核将所有数据处理完成后,由内核将数据写入到用户态中,然后才算完成,所以性能极高,不会有任何阻塞,全部都由内核完成,可以看到,异步IO模型中,用户进程在两个阶段都是非阻塞状态。
- 这里aio_read发送的请求有一个应用层的缓冲区,内核把请求的数据推到这个缓冲区,用信号通知应用进程,应用不用调用read方法了,所以第二阶段也不阻塞


缺点:
1. 在高并发情况下,因为用户进程不阻塞,会持续收到用户请求,会进行很多系统调用,**内核会积累很多IO读写任务**,内存会占用过多,而崩溃
2. 所以在高并发的异步IO要做**并发访问的限流**,代码的实现复杂度就高了


 ### io_uring

#### Linux I/O 发展

<img src="https://pic4.zhimg.com/80/v2-302393c0ee213af0d31b10308066f513_1440w.webp" alt="image.png" style="zoom:60%;" />
- 基于fd的**阻塞式I/O**
	```c
	ssize_t read(int fd, void *buf, size_t count);
	ssize_t write(int fd, const void *buf, size_t count);
	```

阻塞式系统调用：程序调用这些函数时会进入sleep状态，然后被调度出去，直到 I/O 操作完成。随着存储设备越来越快，程序越来越复杂，阻塞式（blocking）I/O 性能难以满足要求。

Linux 2.6 内核引入了 libaio：
- 用户通过 `io_submit()` 提交 I/O 请求，
- 过一会再调用 `io_getevents()` 来检查哪些 events已经ready了。
- 使用户能编写异步的代码。

libaio 的缺陷：
- **系统调用开销大**：`io_submit()` 和 `io_getevents()` 通过系统调用完成，而触发系统调用时，需要进行**上下文切换**。在高IOPS的情况下，进行上下文切换也会消耗大量的 CPU 时间。
- **仅支持 Direct I/O（直接 I/O）**：在使用原生 AIO 的时候，只能指定 `O_DIRECT` 标识位（直接 I/O），不能借助文件系统的[页缓存](https://zhida.zhihu.com/search?content_id=241955504&content_type=Article&match_order=1&q=%E9%A1%B5%E7%BC%93%E5%AD%98&zhida_source=entity)（page cache）来缓存当前的 I/O 请求，只适用于数据库系统。
- **对数据有大小对齐限制**：所有写操作的数据大小必须是文件系统块大小（一般为 4KB）的倍数，而且要与内存页大小对齐。
- **扩展性差：** 接口在设计时并未考虑扩展性。

#### io_uring

1. 在设计上是原生异步的。应用程序只需要将请求放入队列，不需要其他任何等待，请求完成之后会出现在结果队列。
2. 支持多种类型的 I/O：cached files、direct-access files 等。
3. 灵活、可扩展：基于 `io_uring` 可以对 Linux 的系统调用进行重写。

#### Design

应用程序与内核通过**共享内存**进行通信：

<img src="https://pic4.zhimg.com/80/v2-581450dc30fa36d36df0fc6853273221_1440w.webp" alt="image.png" style="zoom:60%;" />

io_uring 主要创建了 3 块共享内存：

- **提交队列（Submission Queue, SQ）**：一整块连续的内存空间存储的环形队列，用于存放将执行 I/O 操作的数据（指向提交队列项数组的索引）。
- **完成队列（Completion Queue, CQ）**：一整块连续的内存空间存储的环形队列，用于存放 I/O 操作完成后返回的结果。
- **提交队列项数组（Submission Queue Entry，SQE）**：提交队列中的一项。

<img src="https://pic3.zhimg.com/80/v2-a59b10c286d37d4c00dc1405bbd26e58_1440w.webp" alt="image.png" style="zoom:60%;" />
提交队列 SQ

```c
struct io_uring_sq {
    unsigned *khead;    //队头
    unsigned *ktail;    //队尾
    // Deprecated: use `ring_mask` instead of `*kring_mask`
    unsigned *kring_mask;
    // Deprecated: use `ring_entries` instead of `*kring_entries`
    unsigned *kring_entries;
    unsigned *kflags;
    unsigned *kdropped;
    unsigned *array;
    struct io_uring_sqe *sqes;  //SQE指针数组

    unsigned sqe_head;
    unsigned sqe_tail;

    size_t ring_sz;
    void *ring_ptr;

    unsigned ring_mask;
    unsigned ring_entries;

    unsigned pad[2];
};
```

<img src="https://pic2.zhimg.com/80/v2-3c43a4d75925b9c8f016e79f7d8cedc7_1440w.webp" alt="image.png" style="zoom:60%;" />
应用程序直接向 `io_sq_ring` 结构的环形队列中提交 I/O 操作，无需通过系统调用来提交，避免了上下文切换的发生。[内核线程](https://zhida.zhihu.com/search?content_id=241955504&content_type=Article&match_order=1&q=%E5%86%85%E6%A0%B8%E7%BA%BF%E7%A8%8B&zhida_source=entity)从 `io_sq_ring` 结构的环形队列中获取到要进行的 I/O 操作，并且发起 I/O 请求。

提交队列项 SQE

```c
/*
 * IO submission data structure (Submission Queue Entry)
 */
struct io_uring_sqe {
    __u8    opcode;     /* type of operation for this sqe */
    __u8    flags;      /* IOSQE_ flags */
    __u16   ioprio;     /* ioprio for the request */
    __s32   fd;     /* file descriptor to do IO on */
    union {
        __u64   off;    /* offset into file */
        __u64   addr2;
        struct {
            __u32   cmd_op;
            __u32   __pad1;
        };
    };
    union {
        __u64   addr;   /* pointer to buffer or iovecs */
        __u64   splice_off_in;
    };
    __u32   len;        /* buffer size or number of iovecs */
    ...
};
```

当用户调用 `io_uring_setup()` 系统调用创建一个 `io_ring` 对象时，内核将会创建一个类型为 `io_uring_sqe` 结构的数组。 应用程序提交 I/O 操作时，先要从 `提交队列项数组` 中获取一个空闲的项 `io_uring_sqe`，然后向此项填充数据（如 I/O 操作码、要进行 I/O 操作的文件句柄等），然后将此项在 `提交队列项数组` 的索引写入 `提交队列` 中。

完成队列 CQ
当内核完成 I/O 操作后，会将 I/O 操作的结果保存到 完成队列 中。

```c
struct io_uring_cq {
    unsigned *khead;
    unsigned *ktail;
    // Deprecated: use `ring_mask` instead of `*kring_mask`
    unsigned *kring_mask;
    // Deprecated: use `ring_entries` instead of `*kring_entries`
    unsigned *kring_entries;
    unsigned *kflags;
    unsigned *koverflow;
    struct io_uring_cqe *cqes;

    size_t ring_sz;
    void *ring_ptr;

    unsigned ring_mask;
    unsigned ring_entries;

    unsigned pad[2];
};
```

<img src="https://pic2.zhimg.com/80/v2-73afdc287d669d48ca4ba6c3f0c7431d_1440w.webp" alt="image.png" style="zoom:60%;" />

SQ 线程

在内核轮询模式下，内核将会创建一个名为 io_uring-sq 的内核线程（称为 SQ 线程），此内核线程会不断从 提交队列 中读取 I/O 操作，并且发起 I/O 请求。
当 I/O 请求完成以后，SQ 线程将会把 I/O 操作的结果写入到 完成队列 中，应用程序就可以从 完成队列 中读取 I/O 操作的结果。

<img src="https://pica.zhimg.com/80/v2-e7e407ef1c391d8649bb2374e543deac_1440w.webp" alt="image.png" style="zoom:80%;" />

简要步骤

`io_uring` 的基本操作流程：

- **第一步**：应用程序通过向 `io_uring` 的 `提交队列` 提交 I/O 操作。
- **第二步**：SQ 内核线程从 `提交队列` 中读取 I/O 操作。
- **第三步**：SQ 内核线程发起 I/O 请求。
- **第四步**：I/O 请求完成后，SQ 内核线程会将 I/O 请求的结果写入到 `io_uring` 的 `完成队列` 中。
- **第五步**：应用程序可以通过从 `完成队列` 中读取到 I/O 操作的结果。

Demo

```c
/* SPDX-License-Identifier: MIT */
/*
 * Simple app that demonstrates how to setup an io_uring interface,
 * submit and complete IO against it, and then tear it down.
 *
 * gcc -Wall -O2 -D_GNU_SOURCE -o io_uring-test io_uring-test.c -luring
 */
#include <stdio.h>
#include <fcntl.h>
#include <string.h>
#include <stdlib.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <unistd.h>
#include "liburing.h"

#define QD  4

int main(int argc, char *argv[])
{
    struct io_uring ring;
    int i, fd, ret, pending, done;
    struct io_uring_sqe *sqe;
    struct io_uring_cqe *cqe;
    struct iovec *iovecs;
    struct stat sb;
    ssize_t fsize;
    off_t offset;
    void *buf;

    if (argc < 2) {
        printf("%s: file\n", argv[0]);
        return 1;
    }

// 1. 初始化一个 io_uring 实例
    ret = io_uring_queue_init(QD, &ring, 0);
    if (ret < 0) {
        fprintf(stderr, "queue_init: %s\n", strerror(-ret));
        return 1;
    }

//2. 获取文件描述符，指定O_DIRECT flag，内核轮询模式需要O_DIRECT flag
    fd = open(argv[1], O_RDONLY | O_DIRECT);
    if (fd < 0) {
        perror("open");
        return 1;
    }

    if (fstat(fd, &sb) < 0) {
        perror("fstat");
        return 1;
    }

    printf("file size=%lu\n",sb.st_size);

// 3. 初始化 4 个读缓冲区
    fsize = 0;
    iovecs = calloc(QD, sizeof(struct iovec));
    for (i = 0; i < QD; i++) {
        if (posix_memalign(&buf, 4096, 4096))
            return 1;
        iovecs[i].iov_base = buf;
        iovecs[i].iov_len = 4096;
        fsize += 4096;
    }

// 4. 准备 4 个 SQE 读请求，指定将随后读入的数据写入 iovecs 
    offset = 0;
    i = 0;
    do {
        sqe = io_uring_get_sqe(&ring);
        if (!sqe)
            break;

        printf("prepare sqe %d\n",i);

        // 指定将随后读入的数据写入 iovecs 
        io_uring_prep_readv(sqe, fd, &iovecs[i], 1, offset);
        offset += iovecs[i].iov_len;
        i++;
        if (offset > sb.st_size)
            break;
    } while (1);

// 5. 提交 SQE 读请求
    ret = io_uring_submit(&ring);
    if (ret < 0) {
        fprintf(stderr, "io_uring_submit: %s\n", strerror(-ret));
        return 1;
    } else if (ret != i) {
        fprintf(stderr, "io_uring_submit submitted less %d\n", ret);
        return 1;
    }

// 6. 等待读请求完成（CQE）
    done = 0;
    pending = ret;
    fsize = 0;

    printf("pending=%d\n",pending);

    for (i = 0; i < pending; i++) {
        ret = io_uring_wait_cqe(&ring, &cqe); // 等待系统返回一个读完成事件
        if (ret < 0) {
            fprintf(stderr, "io_uring_wait_cqe: %s\n", strerror(-ret));
            return 1;
        }

        done++;
        ret = 0;
        if (cqe->res != 4096 && cqe->res + fsize != sb.st_size) {
            fprintf(stderr, "ret=%d, wanted 4096\n", cqe->res);
            ret = 1;
        }
        fsize += cqe->res;

        printf("iteration %d\n",i);
        printf("ret=%d\tcqe->res=%d\n",ret,cqe->res);
        printf("%s\n",iovecs[i].iov_base);

        io_uring_cqe_seen(&ring, cqe); // 释放一个io_uring_cqe entry
        if (ret)
            break;
    }

    printf("Submitted=%d, completed=%d, bytes=%lu\n", pending, done,
                        (unsigned long) fsize);
    close(fd);
    io_uring_queue_exit(&ring);
    return 0;
}
```


### 对比
最后用一幅图,来说明他们之间的区别

<img src="http://article.biliimg.com/bfs/article/443d0a35430d6e9213891a52af04cbceacd31752.png" style="zoom:67%;" />

# 两种高效的事件处理模式

## Reactor模型


![[reactor模型是如何处理网络IO.km]]


**1. Reactor模式简介**  
Reactor模式也叫Dispatcher模式,大多数IO相关组件如redis也在使用该模式。 reactor模式原理是I/O多路复用[监听](https://so.csdn.net/so/search?q=%E7%9B%91%E5%90%AC&spm=1001.2101.3001.7020)事件,收到事件后,根据事件类型分配(Dispatch)给某个进程/线程。**其实就是将对io的操作转化为对事件的处理**。

- Reactor模式主要由Reactor和处理资源池这两个核心部分组成,它俩负责的事情如下：
	- Reactor负责**监听**和**分发**事件,事件类型包含**连接事件、读写事件**；
	- 处理资源池负责处理事件,如 read -> 业务逻辑 -> send；

在Reactor模式中:
- 主线程:
	- 一直在监听文件描述符上是否有事件发生,有就通知工作线程
- 工作线程:
	- 接受新的连接
	    - 读数据
	    - 处理客户请求
	    - 写数据

Reactor 模式是灵活多变的,可以应对不同的业务场景,灵活在于：
- Reactor 的数量可以只有一个,也可以有多个；
- 处理资源池可以是单个进程/线程,也可以是多个进程/线程；

将上面的两个因素排列组设一下,理论上就可以有 4 种方案选择：
- 单 Reactor 单进程 / 线程；
- 单 Reactor 多进程 / 线程；
- ❌ 多 Reactor 单进程 / 线程；**不仅复杂而且也没有性能优势,因此实际中并没有应用**。
- 多 Reactor 多进程 / 线程；


**单 Reactor 单进程 / 线程**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%8D%95%E8%BF%9B%E7%A8%8B.png" alt="image.png" style="zoom:40%;" />

可以看到进程里有 Reactor、Acceptor、Handler这三个对象：
- Reactor对象的作用是监听和分发事件；
- Acceptor对象的作用是获取连接；
- Handler对象的作用是处理业务；


介绍下「单 Reactor 单进程」这个方案：
- Reactor对象通过select(IO 多路复用接口)监听事件,收到事件后通过 dispatch 进行分发,具体分发给 Acceptor 对象还是 Handler 对象,还要看收到的事件类型；
- 如果是连接建立的事件,则交由 Acceptor 对象进行处理,Acceptor 对象会通过 accept 方法 获取连接,并创建一个 Handler 对象来处理后续的响应事件；
- 如果不是连接建立事件,则交由当前连接对应的 Handler 对象来进行响应；
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

这种方案存在 2 个缺点：
- 第一个缺点,因为只有一个进程,**无法充分利用多核CPU的性能**；
- 第二个缺点,Handler对象在业务处理时,整个进程是无法处理其他连接的事件的,**如果业务处理耗时比较长,那么就造成响应的延迟**；

所以,单 Reactor 单进程的方案**不适用计算机密集型的场景,只适用于业务处理非常快速的场景**。


**单 Reactor 多线程 / 多进程**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png" alt="image.png" style="zoom:40%;" />

步骤:
- Reactor对象通过select(IO 多路复用接口) 监听事件,收到事件后通过dispatch进行分发,具体分发给Acceptor对象还是Handler对象,还要看收到的事件类型；
- 如果是连接建立的事件,则交由Acceptor对象进行处理,Acceptor对象会通过accept方法获取连接,并创建一个Handler对象来处理后续的响应事件；
- 如果不是连接建立事件,则交由当前连接对应的Handler对象来进行响应；

接下来的步骤就开始不一样了：
- Handler对象不再负责业务处理,**只负责数据的接收和发送**,Handler对象通过read读取到数据后,会将数据发给子线程里的Processor对象进行业务处理；
- 子线程里的Processor对象就进行业务处理,处理完后,将结果发给主线程中的Handler对象,接着由Handler通过send方法将响应结果发送给client；

单Reator多线程的方案优势在于**能够充分利用多核CPU的能力**,那既然引入多线程,那么自然就带来了多线程竞争资源的问题。
子线程完成业务处理后,要把结果传递给主线程的 Handler 进行发送,这里涉及共享数据的竞争。那就需要在操作共享资源前加上互斥锁
「单 Reactor」的模式还有个问题,**因为一个Reactor对象承担所有事件的监听和响应,而且只在主线程中运行,在面对瞬间高并发的场景时,容易成为性能的瓶颈的地方**。



**多 Reactor 多进程 / 线程**

<img src="https://img-blog.csdnimg.cn/7e2b0008a65e458bb9d26d8f3d66d388.png" alt="image.png" style="zoom:60%;" />

- 主线程中的 MainReactor 对象通过 select 监控连接建立事件,收到事件后通过 Acceptor 对象中的 accept 获取连接,将新的连接分配给某个子线程；
- 子线程中的 SubReactor 对象将 MainReactor 对象分配的连接加入 select **继续进行监听**,并创建一个Handler用于处理连接的响应事件。
- 如果有新的事件发生时,SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应。
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

多 Reactor 多线程的方案虽然看起来复杂的,但是实际实现时比单 Reactor 多线程的方案要简单的多,原因如下：

- 主线程和子线程分工明确,主线程只负责接收新连接,子线程负责完成后续的业务处理。
- 主线程和子线程的交互很简单,主线程只需要把新连接传给子线程,子线程无须返回数据,直接就可以在子线程将处理结果发送给客户端。

## Proactor

前面提到的Reactor是非阻塞同步网络模式,而Proactor是异步网络模式。异步I/O的概念看[[网络编程#异步IO|这里]]

#面试点 
- **Reactor 是非阻塞同步网络模式,感知的是就绪可读写事件**。在每次感知到有事件发生(比如可读就绪事件)后,就需要应用进程主动调用read方法来完成数据的读取,也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中,这个过程是同步的,读取完数据后应用进程才能处理数据。
- **Proactor 是异步网络模式, 感知的是已完成的读写事件**。在发起异步读写请求时,需要传入数据缓冲区的地址(用来存放结果数据)等信息,这样系统内核才可以自动帮我们把数据的读写工作完成,这里的读写工作全程由操作系统来做,并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据,操作系统完成读写工作后,就会通知应用进程直接处理数据。

因此,**Reactor 可以理解为「来了事件操作系统通知应用进程,让应用进程来处理」**,而 **Proactor 可以理解为「来了事件操作系统来处理,处理完再通知应用进程」**。这里的「事件」就是有新连接、有数据可读、有数据可写的这些 I/O 事件这里的「处理」包含从驱动读取到内核以及从内核读取到用户空间。

无论是 Reactor,还是 Proactor,都是一种基于「事件分发」的网络编程模式,区别在于 **Reactor 模式是基于「待完成」的 I/O 事件,而 Proactor 模式则是基于「已完成」的 I/O 事件**。


<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/Proactor.png" alt="image.png" style="zoom:60%;" />

介绍一下 Proactor 模式的工作流程：

- Proactor Initiator 负责创建Proactor和Handler对象,并将Proactor和Handler都通过Asynchronous Operation Processor注册到内核；
- Asynchronous Operation Processor 负责处理注册请求,并处理 I/O 操作；
- Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor；
- Proactor 根据不同的事件类型**回调不同的Handler**进行业务处理；
- Handler 完成业务处理；

#面试点 
> 看起来Proactor这么好⽤,那你为什么不⽤？

在Linux下的异步I/O是不完善的,`aio` 系列函数是由POSIX定义的异步操作接口,不是真正的操作系统级别支持的,而是在用户空间模拟出来的异步,并且仅仅支持基于本地文件的aio异步操作,网络编程中的socket是不支持的,这也使得基于Linux的高性能网络程序都是使用Reactor方案。

而Windows里实现了一套完整的支持socket的异步编程接口,这套接口就是`IOCP`,是由操作系统级别实现的异步I/O,真正意义上异步I/O,因此在Windows里实现高性能网络程序可以使用效率更高的Proactor方案。

# 基于UDP的套接字通信

- udp是一个面向无连接的,不安全的,报式传输层协议,udp的通信过程默认也是阻塞的。
- UDP通信不需要建立连接 ,因此不需要进行connect()操作
- UDP通信过程中,每次都需要指定数据接收端的IP和端口,和发快递差不多
- UDP不对收到的数据进行排序,在UDP报文的首部中并没有关于数据顺序的信息
- UDP对接收到的数据报不回复确认信息,发送端不知道数据是否被正确接收,也不会重发数据。
- 如果发生了数据丢失,不存在丢一半的情况,如果丢当前这个数据包就全部丢失了
- UDP协议数据**报文截断**,如果接受到的数据包大于我们接受的缓冲区,报文就会截断被丢弃了,下次再接受就被丢弃了
- UDP存在[[计算机网络#网际控制报文协议|ICMP]]异步错误
	- 根据TCP/IP协议规范,异步ICMP错误消息通常不会被直接传递给无连接的套接字(如UDP)。因此,即使ICMP错误消息指出发送的数据包未能到达,应用程序通过UDP套接字通常无法直接获得这些错误信息。
	- 解决办法是udp也可以采用connect,但在UDP上使用connect并不是为了建立一个真正的连接,而是为了指定一个默认的远程地址和端口。

## 通信流程
使用UDP进行通信,服务器和客户端的处理步骤比TCP要简单很多,并且两端是对等的 (通信的处理流程几乎是一样的),也就是说并没有严格意义上的客户端和服务器端。UDP的通信流程如下：

<img src="https://www.subingwen.cn/linux/udp/udp.jpg" alt="image.png" style="zoom:60%;" />

### 服务器端
**假设服务器端是接收数据的角色：**

1. 创建通信的套接字
	```c
	// 第二个参数是 SOCK_DGRAM, 第三个参数0表示使用报式协议中的udp
	int fd = socket(AF_INET, SOCK_DGRAM, 0);
	```

2. 使用通信的套接字和本地的IP和端口绑定,IP和端口需要转换为大端(可选)

	```c
	bind();
	```

3. 通信
	```c
	// 接收数据
	recvfrom();
	// 发送数据
	sendto();
	```

4. 关闭套接字(文件描述符)
	```c
	close(fd);
	```

### 客户端
**假设客户端是发送数据的角色：**

1. 创建通信的套接字
	```c
	// 第二个参数是 SOCK_DGRAM, 第三个参数0表示使用报式协议中的udp
	int fd = socket(AF_INET, SOCK_DGRAM, 0);
	```

2. 通信
	```c
	// 接收数据
	recvfrom();
	// 发送数据
	sendto();
	```

3. 关闭套接字(文件描述符)
	```c
	close(fd);
	```

在UDP通信过程中,`哪一端是接收数据的角色,那么这个接收端就必须绑定一个固定的端口`,如果某一端不需要接收数据,这个绑定操作就可以省略不写了,通信的套接字会自动绑定一个随机端口。


## 通信函数
基于UDP进行套接字通信,创建套接字的函数还是`socket()`但是第二个参数的值需要指定为`SOCK_DGRAM`,通过该参数指定要创建一个基于报式传输协议的套接字,最后一个参数指定为0表示使用报式协议中的UDP协议。
```c
int socket(int domain, int type, int protocol);
```

- 参数:
	- `domain`：地址族协议,AF_INET -> IPv4,AF_INET6-> IPv6
	- `type`：使用的传输协议类型,报式传输协议需要指定为 SOCK_DGRAM
	- `protocol`：指定为0,表示使用的默认报式传输协议为 UDP
- 返回值：函数调用成功返回一个可用的文件描述符(大于0),调用失败返回-1

另外进行UDP通信,通信过程虽然默认还是阻塞的,但是通信函数和TCP不同,操作函数原型如下：
```c
// 接收数据, 如果没有数据,该函数阻塞
ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags,
                 struct sockaddr *src_addr, socklen_t *addrlen);
```

- 参数:
	- `sockfd`: 基于udp的通信的文件描述符
	- `buf`: 指针指向的地址用来存储接收的数据
	- `len`: buf指针指向的内存的容量, 最多能存储多少字节
	- `flags`: 设置套接字属性,一般使用默认属性,指定为0即可
	- `src_addr`: 发送数据的一端的地址信息,IP和端口都存储在这里边, 是大端存储的
		- 如果这个参数中的信息对当前业务处理没有用处, 可以指定为NULL, 不保存这些信息
	- `addrlen`: 类似于accept() 函数的最后一个参数, 是一个传入传出参数
		- 传入的是src_addr参数指向的内存的大小, 传出的也是这块内存的大小
		- 如果src_addr参数指定为NULL, 这个参数也指定为NULL即可
- 返回值：成功返回接收的字节数,失败返回-1,可以返回0,只要我们发送端发送0字节

```c
// 发送数据函数
ssize_t sendto(int sockfd, const void *buf, size_t len, int flags,
               const struct sockaddr *dest_addr, socklen_t addrlen);
```

- 参数:
	- sockfd: 基于udp的通信的文件描述符
	- buf: 这个指针指向的内存中存储了要发送的数据
	- len: 要发送的数据的实际长度
	- flags: 设置套接字属性,一般使用默认属性,指定为0即可
	- dest_addr: 接收数据的一端对应的地址信息, 大端的IP和端口
	- addrlen: 参数 dest_addr 指向的内存大小
- 返回值：函数调用成功返回实际发送的字节数,调用失败返回-1
- 含义：调用sendto发送数据时,数据确实是从应用层缓冲区复制到操作系统的网络套接字缓冲区中。这个过程**并不保证数据能成功到达目的地**。数据包的发送只意味着它被交付给了操作系统来处理,进一步的传输由下层的网络协议栈负责。
	- 由于UDP是无连接的,发送操作(sendto)通常不会等待网络层的反馈来确认数据包是否成功送达。这意味着,即使目的地不可达或其他网络问题阻碍了数据包的传输,sendto调用也可能返回成功。

## 通信代码
在UDP通信过程中,服务器和客户端都可以作为数据的发送端和数据接收端,假设服务器端是被动接收数据,客户端是主动发送数据,那么在服务器端就必须绑定固定的端口了。

### 服务器端
```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>

int main()
{
    // 1. 创建通信的套接字
    int fd = socket(AF_INET, SOCK_DGRAM, 0);
    if(fd == -1)
    {
        perror("socket");
        exit(0);
    }

    // 2. 通信的套接字和本地的IP与端口绑定
    struct sockaddr_in addr;
    addr.sin_family = AF_INET;
    addr.sin_port = htons(9999);    // 大端
    addr.sin_addr.s_addr = INADDR_ANY;  // 0.0.0.0
    int ret = bind(fd, (struct sockaddr*)&addr, sizeof(addr));
    if(ret == -1)
    {
        perror("bind");
        exit(0);
    }

    char buf[1024];
    char ipbuf[64];
    struct sockaddr_in cliaddr;
    int len = sizeof(cliaddr);
    // 3. 通信
    while(1)
    {
        // 接收数据
        memset(buf, 0, sizeof(buf));
        int rlen = recvfrom(fd, buf, sizeof(buf), 0, (struct sockaddr*)&cliaddr, &len);
        printf("客户端的IP地址: %s, 端口: %d\n",
               inet_ntop(AF_INET, &cliaddr.sin_addr.s_addr, ipbuf, sizeof(ipbuf)),
               ntohs(cliaddr.sin_port));
        printf("客户端say: %s\n", buf);

        // 回复数据
        // 数据回复给了发送数据的客户端
        sendto(fd, buf, rlen, 0, (struct sockaddr*)&cliaddr, sizeof(cliaddr));
    }

    close(fd);

    return 0;
}
```

作为数据接收端,服务器端通过`bind()`函数绑定了固定的端口,然后基于这个固定的端口通过`recvfrom()`函数接收客户端发送的数据,同时通过这个函数也得到了数据发送端的地址信息(`recvfrom`的第三个参数),这样就可以通过得到的地址信息通过`sendto()`函数给客户端回复数据了。

### 客户端

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>

int main()
{
    // 1. 创建通信的套接字
    int fd = socket(AF_INET, SOCK_DGRAM, 0);
    if(fd == -1)
    {
        perror("socket");
        exit(0);
    }
    
    // 初始化服务器地址信息
    struct sockaddr_in seraddr;
    seraddr.sin_family = AF_INET;
    seraddr.sin_port = htons(9999);    // 大端
    inet_pton(AF_INET, "192.168.1.100", &seraddr.sin_addr.s_addr);

    char buf[1024];
    char ipbuf[64];
    struct sockaddr_in cliaddr;
    int len = sizeof(cliaddr);
    int num = 0;
    // 2. 通信
    while(1)
    {
        sprintf(buf, "hello, udp %d....\n", num++);
        // 发送数据, 数据发送给了服务器
        sendto(fd, buf, strlen(buf)+1, 0, (struct sockaddr*)&seraddr, sizeof(seraddr));

        // 接收数据
        memset(buf, 0, sizeof(buf));
        recvfrom(fd, buf, sizeof(buf), 0, NULL, NULL);
        printf("服务器say: %s\n", buf);
        sleep(1);
    }

    close(fd);

    return 0;
}
```

作为数据发送端,客户端不需要绑定固定端口,客户端使用的端口是随机绑定的(也可以调用`bind()函数`手动进行绑定)。客户端在接收服务器端回复的数据的时候需要调用`recvfrom()函数`,因为客户端在发送数据之前就已经知道服务器绑定的固定的IP和端口信息了,所以接收服务器数据的时候就可以不保存服务器端的地址信息,直接将函数的最后两个参数指定为NULL即可。

# UDP特性之广播

## 广播的特点
广播的UDP的特性之一,`通过广播可以向子网中多台计算机发送消息,并且子网中所有的计算机都可以接收到发送方发送的消息`,每个广播消息都包含一个特殊的IP地址,这个IP中子网内主机标志部分的二进制全部为1 (即点分十进制IP的最后一部分是255)。点分十进制的IP地址每一部分是1字节,最大值为255,比如：`192.168.1.100`

- 前两部分`192.168`表示当前网络是局域网
- 第三部分`1`表示局域网中的某一个网段,最大值为 255
- 第四部分`100`用于标记当前网段中的某一台主机,最大值为255
- 每个网段都有一个特殊的广播地址,即：`192.168.xxx.255`

广播分为两端,即数据发送端和数据接收端,通过广播的方式发送数据,发送端和接收端的关系是 1:N

- `发送广播消息的一端,通过广播地址,可以将消息同时发送到局域网的多台主机上(数据接收端)`
- `在发送广播消息的时候,必须要把数据发送到广播地址上`
- `广播只能在局域网内使用,广域网是无法使用UDP进行广播的`
- 只要发送端在发送广播消息,数据接收端就能收到广播消息,消息的接收是无法拒绝的,除非将接收端的进程关闭,就接收不到了。

UDP的广播和日常生活中的广播是一样的,都是一种快速传播消息的方式,因此广播的开销很小,发送端使用一个广播地址,就可以将数据发送到多个接收数据的终端上,如果不使用广播,就需要进行多次发送才能将数据分别发送到不同的主机上。

## 设置广播属性
基于UDP虽然可以进行数据的广播,但是这个属性默认是关闭的,如果需要对数据进行广播,那么需要在广播端代码中开启广播属性,需要通过套接字选项函数进行设置,该函数原型为：

```c
int setsockopt(int sockfd, int level, int optname, 	const void *optval, socklen_t optlen);
```

- 参数:
	- `sockfd`：进行UDP通信的文件描述符
	- `level`: 套接字级别,需要设置为 `SOL_SOCKET`
	- `optname`：选项名,此处要设置udp的广播属性,该参数需要指定为：`SO_BROADCAST`
	- `optval`：如果是设置广播属性,该指针实际指向一块int类型的内存
		- 该整型值为0：关闭广播属性
		- 该整形值为1：打开广播属性
	- optlen：optval指针指向的内存大小,即：`sizeof(int)`
- 返回值：函数调用成功返回0,失败返回-1

## 广播通信流程
如果使用UDP在局域网范围内进行消息的广播,一般情况下广播端只发送数据,接收端只接受广播消息。因此在数据接收端需要绑定固定的端口,广播端则不需要手动绑定固定端口,自动随机绑定即可。

<img src="https://www.subingwen.cn/linux/broadcast/image-20210407000319507.png" alt="image.png" style="zoom:30%;" />


- 数据发送端
	1. 创建通信的套接字
		```c
		// 第二个参数是 SOCK_DGRAM, 第三个参数0表示使用报式协议中的udp
		int fd = socket(AF_INET, SOCK_DGRAM, 0);
		```
	2. 主动发送数据不需要手动绑定固定端口(自动随机分配就可以了),因此直接设置广播属性
		```c
		int opt  = 1;
		setsockopt(fd, SOL_SOCKET, SO_BROADCAST, &opt, sizeof(opt));
		```
	3. 使用广播地址发送广播数据到接收端绑定的固定端口上
		```c
		sendto();
		```
	4. 关闭套接字(文件描述符)
		```c
		close(fd);
		```
- **数据接收端**
	1. 创建通信的套接字
	```c
	// 第二个参数是 SOCK_DGRAM, 第三个参数0表示使用报式协议中的udp
	int fd = socket(AF_INET, SOCK_DGRAM, 0);
	```
	2. 因为是被动接收数据的一端,所以必须要绑定固定的端口和本地IP地址
	```c
	bind();
	```
	3. 接收广播消息
	```c
	recvfrom();
	```
	4. 关闭套接字(文件描述符)
	```c
	close(fd);
	```


## 通信代码
**广播端**

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>

int main()
{
    // 1. 创建通信的套接字
    int fd = socket(AF_INET, SOCK_DGRAM, 0);
    if(fd == -1)
    {
        perror("socket");
        exit(0);
    }

    // 2. 设置广播属性
    int opt  = 1;
    setsockopt(fd, SOL_SOCKET, SO_BROADCAST, &opt, sizeof(opt));

    char buf[1024];
    struct sockaddr_in cliaddr;
    int len = sizeof(cliaddr);
    cliaddr.sin_family = AF_INET;
    cliaddr.sin_port = htons(9999); // 接收端需要绑定9999端口
    // 只要主机在237网段, 并且绑定了9999端口, 这个接收端就能收到广播消息
    inet_pton(AF_INET, "192.168.237.255", &cliaddr.sin_addr.s_addr);
    // 3. 通信
    int num = 0;
    while(1)
    {
        sprintf(buf, "hello, client...%d\n", num++);
        // 数据广播
        sendto(fd, buf, strlen(buf)+1, 0, (struct sockaddr*)&cliaddr, len);
        printf("发送的广播的数据: %s\n", buf);
        sleep(1);
    }

    close(fd);

    return 0;
}
```

<font color=#ff0000>注意事项：发送广播消息一端必须要开启UDP的广播属性,并且发送消息的地址必须是当前发送端所在网段的广播地址,这样才能通过调用一个消息发送函数将消息同时发送N台接收端主机上。</font>

**接收端**
```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>

int main()
{
    // 1. 创建通信的套接字
    int fd = socket(AF_INET, SOCK_DGRAM, 0);
    if(fd == -1)
    {
        perror("socket");
        exit(0);
    }

    // 2. 通信的套接字和本地的IP与端口绑定
    struct sockaddr_in addr;
    addr.sin_family = AF_INET;
    addr.sin_port = htons(9999);    // 大端
    addr.sin_addr.s_addr = INADDR_ANY;  // 0.0.0.0
    int ret = bind(fd, (struct sockaddr*)&addr, sizeof(addr));
    if(ret == -1)
    {
        perror("bind");
        exit(0);
    }

    char buf[1024];
    // 3. 通信
    while(1)
    {
        // 接收广播消息
        memset(buf, 0, sizeof(buf));
        // 阻塞等待数据达到
        recvfrom(fd, buf, sizeof(buf), 0, NULL, NULL);
        printf("接收到的广播消息: %s\n", buf);
    }

    close(fd);

    return 0;
}
```

<font color=#ff0000>对于接收广播消息的一端,必须要绑定固定的端口,并由广播端将广播消息发送到这个端口上,因此所有接收端都应绑定相同的端口,这样才能同时收到广播数据。</font>


# UDP特性之组播(多播)

## 组播的特点
组播也可以称之为多播这也是UDP的特性之一。`组播是主机间一对多的通讯模式,是一种允许一个或多个组播源发送同一报文到多个接收者的技术`。组播源将一份报文发送到特定的组播地址,组播地址不同于单播地址,它并不属于特定某个主机,而是属于一组主机。一个组播地址表示一个群组,需要接收组播报文的接收者都加入这个群组。

- 广播只能在局域网访问内使用,组播既可以在局域网中使用,也可以用于广域网
- 在发送广播消息的时候,连接到局域网的客户端不管想不想都会接收到广播数据,组播可以控制发送端的消息能够被哪些接收端接收,更灵活和人性化。
- 广播使用的是广播地址,组播需要使用组播地址。
- 广播和组播属性默认都是关闭的,如果使用需要通过setsockopt()函数进行设置。

组播需要使用组播地址,在 IPv4 中它的范围从 `224.0.0.0` 到 `239.255.255.255`,并被划分为局部链接多播地址、预留多播地址和管理权限多播地址三类:

|  IP地址                       |  说明                                                                                                     |
|:---------------------------:|:-------------------------------------------------------------------------------------------------------:|
|    224.0.0.0~224.0.0.255    |  局部链接多播地址：是为路由协议和其它用途保留的地址,只能用于局域网中,路由器是不会转发的地址 224.0.0.0不能用,是保留地址                                      |
|    224.0.1.0~224.0.1.255    |  为用户可用的组播地址(临时组地址),可以用于 Internet 上的。                                                                    |
|  224.0.2.0~238.255.255.255  |  用户可用的组播地址(临时组地址),全网范围内有效                                                                               |
|   239.0.0.0~239.255.255     |  为本地管理组播地址,仅在特定的本地范围内有效                                                                                 |  

组播地址不属于任何服务器或个人,它有点类似一个微信群号,任何成员(**组播源**)往微信群(**组播IP**)发送消息(组播数据),这个群里的成员(**组播接收者**)都会接收到此消息。

## 设置组播属性
如果使用组播进行数据的传输,不管是消息发送端还是接收端,都需要进行相关的属性设置,设置函数使用的是同一个,即：`setsockopt()`。

### 发送端
发送组播消息的一端需要设置组播属性,具体的设置方式如下：
```c
int setsockopt(int sockfd, int level, int optname, const void *optval, socklen_t optlen);
```

- 参数:
	- sockfd：用于UDP通信的套接字
	- level：套接字级别,设置组播属性需要将该参数指定为：`IPPTOTO_IP`
	- optname: 套接字选项名,设置组播属性需要将该参数指定为：`IP_MULTICAST_IF`
	- optval：设置组播属性,这个指针需要指向一个`struct in_addr{}` 类型的结构体地址,这个结构体地址用于存储组播地址,并且组播IP地址的存储方式是大端的。
	```c
	struct in_addr
	{
	    in_addr_t s_addr;	// unsigned int
	}; 
	```

- optlen：optval指针指向的内存大小,即：`sizeof(struct in_addr)`
- 返回值：函数调用成功返回0,调用失败返回-1

### 接收端
因为一个组播地址表示一个群组,所以需要接收组播报文的接收者都加入这个群组,和想要接收群消息就必须要先入群是一个道理。加入到这个组播群组的方式如下：

```c
int setsockopt(int sockfd, int level, int optname, const void *optval, socklen_t optlen);
```

- 参数:
	- `sockfd`：基于udp的通信的套接字
	- `level`：套接字级别,加入到多播组该参数需要指定为：`IPPTOTO_IP`
	- `optname`：套接字选项名,加入到多播组该参数需要指定为：`IP_ADD_MEMBERSHIP`
	- `optval`：加入到多播组,这个指针应该指向一个`struct ip_mreqn{}`类型的结构体地址

	```c
	typedef unsigned int  uint32_t;
	typedef uint32_t in_addr_t;
	struct sockaddr_in addr;
	
	struct in_addr
	{
	    in_addr_t s_addr;	// unsigned int
	};
	
	struct ip_mreqn
	{
	    struct in_addr imr_multiaddr;   // 组播地址/多播地址
	    struct in_addr imr_address;     // 本地地址
	    int   imr_ifindex;              // 网卡的编号, 每个网卡都有一个编号
	};
	// 必须通过网卡名字才能得到网卡的编号: 可以通过 ifconfig 命令查看网卡名字
	#include <net/if.h>
	// 将网卡名转换为网卡的编号, 参数是网卡的名字, 比如: "ens33"
	// 返回值就是网卡的编号
	unsigned int if_nametoindex(const char *ifname);
	```

	- optlen：optval指向的内存大小,即：`sizeof(struct ip_mreqn)`

## 组播通信流程
发送组播消息的一端需要将数据发送到组播地址和固定的端口上,想要接收组播消息的终端需要绑定对应的固定端口然后加入到组播的群组,最终就可以实现数据的共享。

<img src="https://www.subingwen.cn/linux/multicast/image-20210407110753006.png" alt="image.png" style="zoom:30%;" />
### 发送端
1. 创建通信的套接字
```c
// 第二个参数是 SOCK_DGRAM, 第三个参数0表示使用报式协议中的udp
int fd = socket(AF_INET, SOCK_DGRAM, 0);
```

2. 主动发送数据的一端不需要手动绑定端口(自动随机分配就可以了),设置UDP组播属性

```c
// 设置组播属性
struct in_addr opt;
// 将组播地址初始化到这个结构体成员中
inet_pton(AF_INET, "239.0.1.10", &opt.s_addr);
setsockopt(fd, IPPROTO_IP, IP_MULTICAST_IF, &opt, sizeof(opt));
```

3. 使用组播地址发送组播消息到固定的端口(接收端需要绑定这个端口)
```c
sendto();
```

4. 关闭套接字(文件描述符)
```c
close(fd);
```

### 接收端
1. 创建通信的套接字

```c
// 第二个参数是 SOCK_DGRAM, 第三个参数0表示使用报式协议中的udp
int fd = socket(AF_INET, SOCK_DGRAM, 0);
```

2. 绑定固定的端口,发送端应该将数据发送到接收端绑定的端口上
```c
bind();
```

3. 加入到组播的群组中,入群之后就可以接受组播消息了。
```c
// 加入到多播组
struct ip_mreqn opt;
// 要加入到哪个多播组, 通过组播地址来区分
inet_pton(AF_INET, "239.0.1.10", &opt.imr_multiaddr.s_addr);
opt.imr_address.s_addr = INADDR_ANY;
opt.imr_ifindex = if_nametoindex("ens33");
setsockopt(fd, IPPROTO_IP, IP_ADD_MEMBERSHIP, &opt, sizeof(opt));
```

4. 接收组播数据
```c
recvfrom();
```
5. 关闭套接字(文件描述符)
```c
close(fd);
```

## 通信代码
### 发送端

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>

int main()
{
    // 1. 创建通信的套接字
    int fd = socket(AF_INET, SOCK_DGRAM, 0);
    if(fd == -1)
    {
        perror("socket");
        exit(0);
    }

    // 2. 设置组播属性
    struct in_addr opt;
    // 将组播地址初始化到这个结构体成员中即可
    inet_pton(AF_INET, "239.0.1.10", &opt.s_addr);
    setsockopt(fd, IPPROTO_IP, IP_MULTICAST_IF, &opt, sizeof(opt));

    char buf[1024];
    struct sockaddr_in cliaddr;
    int len = sizeof(cliaddr);
    cliaddr.sin_family = AF_INET;
    cliaddr.sin_port = htons(9999); // 接收端需要绑定9999端口
    // 发送组播消息, 需要使用组播地址, 和设置组播属性使用的组播地址一致就可以
    inet_pton(AF_INET, "239.0.1.10", &cliaddr.sin_addr.s_addr);
    // 3. 通信
    int num = 0;
    while(1)
    {
        sprintf(buf, "hello, client...%d\n", num++);
        // 数据广播
        sendto(fd, buf, strlen(buf)+1, 0, (struct sockaddr*)&cliaddr, len);
        printf("发送的组播的数据: %s\n", buf);
        sleep(1);
    }

    close(fd);

    return 0;
}
```

<font color=#ff0000>注意事项：在组播数据的发送端,需要先设置组播属性,发送的数据是通过sendto()函数发送到某一个组播地址上,并且在程序中数据发送到了接收端的9999端口,因此接收端程序必须要绑定这个端口才能收到组播消息。</font>

### 接收端
```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>
#include <net/if.h>

int main()
{
    // 1. 创建通信的套接字
    int fd = socket(AF_INET, SOCK_DGRAM, 0);
    if(fd == -1)
    {
        perror("socket");
        exit(0);
    }

    // 2. 通信的套接字和本地的IP与端口绑定
    struct sockaddr_in addr;
    addr.sin_family = AF_INET;
    addr.sin_port = htons(9999);    // 大端
    addr.sin_addr.s_addr = INADDR_ANY;  // 0.0.0.0
    int ret = bind(fd, (struct sockaddr*)&addr, sizeof(addr));
    if(ret == -1)
    {
        perror("bind");
        exit(0);
    }

    // 3. 加入到多播组
    struct ip_mreqn opt;
    // 要加入到哪个多播组, 通过组播地址来区分
    inet_pton(AF_INET, "239.0.1.10", &opt.imr_multiaddr.s_addr);
    opt.imr_address.s_addr = INADDR_ANY;
    opt.imr_ifindex = if_nametoindex("ens33");
    setsockopt(fd, IPPROTO_IP, IP_ADD_MEMBERSHIP, &opt, sizeof(opt));

    char buf[1024];
    // 3. 通信
    while(1)
    {
        // 接收广播消息
        memset(buf, 0, sizeof(buf));
        // 阻塞等待数据达到
        recvfrom(fd, buf, sizeof(buf), 0, NULL, NULL);
        printf("接收到的组播消息: %s\n", buf);
    }

    close(fd);

    return 0;
}
```

<font color=#ff0000>注意事项：作为组播消息的接收端,必须要先绑定一个固定端口(发送端就可以把数据发送到这个固定的端口上了),然后加入到组播的群组中(一个组播地址可以看做是一个群组),这样就可以接收到组播消息了。</font>


# UNIX域协议

- UNIX域套接字与TCP套接字相比较,在同一台主机的传输速度前者是后者的两倍
- UNIX域套接字可以在同一台主机上各进程之间传递描述符。
- UNIX域套接字与传统套接字的区别是用**路径名**来表示协议族的描述。

```c
#define UNIX_PATH_MAX 128

struct sockaddr_un{
	sa_family_t sun_family; /* AF_UNIX 或者 AF_LOCAL */
	char sun_path[UNIX_PATH_MAX]; /* path name */
};
```



```c
  int listenfd;
  if ((listenfd = socket(AF_UNIX, SOCK_STREAM, 0)) < 0) { ERR_EXIT("socket"); }

  unlink("/tmp/test_socket"); // 删除一个名为 "test_socket" 的文件。这个操作一般用于在创建套接字前,确保该套接字文件不存在,以免发生文件名冲突或其他异常情况。
  sockaddr_un servaddr = {};
  servaddr.sun_family = AF_UNIX;
  strcpy(servaddr.sun_path, "/tmp/test_socket")
```


**注意点**

- bind成功将会创建一个文件,权限为0777 & umask
- sun path最好用一个绝对路径
- UNIX域协议支持流式套接口与报式套接口
- UNIX域流式套接字connect发现监听队列满时,会立刻返回一个ECONNREFUSED,这和TCP不同,如果监听队列满,会忽略到来的SYN,这导致对方重传SYN


## socketpair

- 功能:创建一个**全双工**的流管道,类似于[[C语言#匿名管道|匿名管道]]
- 原型
	```c
	int socketpair(int domain, int type, int protocol, int sv[2]);
	```
- 参数
	- **domain**:指定套接字使用的协议族,对于socketpair,通常使用AF_UNIX。
	- **type**:套接字类型
	- **protocol**:协议类型
	- **sv**:一个引用参数,用于返回两个连接的套接字描述符。`sv[0]`和`sv[1]`分别代表套接字对的两端。
- 返回值:
	- 成功时,返回0,并且sv数组中的两个元素被设置为新创建的套接字描述符。
	- 失败时,返回-1,并设置errno以指示错误原因。

- 使用场景
	- socketpair非常适用于需要进行快速、双向通信的场合,比如：
	- **父子进程间的通信**：父进程可以使用socketpair创建一对套接字,然后通过fork创建子进程。父进程和子进程可以关闭各自不需要的套接字端点,然后通过剩下的套接字端点进行通信。
	- 线程间的通信：虽然线程可以直接共享数据和使用其他同步机制,但在某些设计中使用socketpair进行消息传递可以简化线程间通信的逻辑。


## sendmsg 和 recvmsg 函数

最通用的I/O函数,只要设置好参数,read、readv、recv、recvfrom和write、writev、send、sendto等函数都可以对应换成这两个函数来调用。同时,各种输出函数调用也可以替换成sendmsg调用。

```c
#include <sys/socket.h>
ssize_t recvmsg(int sockfd, struct msghdr *msg, int flags);
ssizt_t sendmsg(int sockfd, struct msghdr *msg, int flags);
```

大部分参数都在 msghdr结构中

```c
struct iovec
{                   /* Scatter/gather array items */
    void *iov_base; /* Starting address */
    size_t iov_len; /* Number of bytes to transfer */
};

struct msghdr
{
    void *msg_name;        /* optional address */
    socklen_t msg_namelen; /* size of address */
    struct iovec *msg_iov; /* scatter/gather array */
    size_t msg_iovlen;   /* # elements in msg_iov */
    void *msg_control;   /* ancillary data, see below */
    size_t msg_controllen; /* ancillary data buffer len */
    int msg_flags;         /* flags on received message */
};
```

**struct msghdr** 结构体参数说明：
- msg_name : 指向一个套接字地址结构,用于存放接受者或者发送者的协议地址。无需指明时,置为空 。
- msg_iov,msg_iovlen : 指定输入或输出的缓冲区数组。
- msg_control,msg_controllen : 可选的辅助数据的位置和大小

详细请看这个[博客](https://www.cnblogs.com/mickole/articles/3204406.html)

# system V 标准



## system V消息队列

消息队列是在两个进程之间传递二进制块数据的一种简单有效的方式。每个数据块都有一个特定的类型,接收方可以根据类型来有选择地接收数据,而不一定像管道和命名管道那样必须以先进先出的方式接收数据。

概念可以看[[零散知识点#消息队列|这里]]

内核为每个IPC对象维护一个数据结构 
```c
struct ipc_perm {
    key_t          __key;    /* Key supplied to shmget(), semget(), or msgget() */
    uid_t          uid;      /* Owner's user ID */
    gid_t          gid;      /* Owner's group ID */
    uid_t          cuid;     /* Creator's user ID */
    gid_t          cgid;     /* Creator's group ID */
    mode_t         mode;     /* Read/write permission */
    unsigned short __seq;    /* Sequence number 用于区分具有相同键值的IPC对象。 */
};
```



```c
struct msqid_ds {
    struct ipc_perm msg_perm;     // 消息队列权限 就是ipc对象
    msgqnum_t       msg_qnum;     // 队列中的消息数
    struct msg      *msg_first;     /* ptr to first message on queue */
    struct msg      *msg_last;      /* ptr to last message on queue */
    msglen_t        msg_cbytes;   // 当前的队列的字节数 
    msglen_t        msg_qbytes;   // 队列的最大字节数 一个消息的消息的总和限制
    pid_t           msg_lspid;    // 最后发送消息的进程ID
    pid_t           msg_lrpid;    // 最后接收消息的进程ID
    time_t          msg_stime;    // 最后发送消息的时间
    time_t          msg_rtime;    // 最后接收消息的时间
    time_t          msg_ctime;    // 最后一次修改队列的时间或创建队列的时间
};
```

<img src="https://img-blog.csdnimg.cn/2020041911561241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25pZTIzMTQ1NTA0NDE=,size_16,color_FFFFFF,t_70" alt="image.png" style="zoom:70%;" />

```c
消息队列也有管道一样的不足,就是每个消息的 最大长度是有上限的(MSGMAX),
每个消息队 列的总的字节数是有上限的(MSGMNB),系统上消息队列的总数也有一个上限(MSGMNI)
```


### msgget 函数

msgget 函数用于**创建**一个新的消息队列或**访问**一个已存在的消息队列。

```c
#include <sys/msg.h>
int msgget(key_t key, int oflag);
        // 返回：若成功则为非负标识符,若出错则为-1
```

返回值是一个整数标识符,其他三个msg函数就用它来指代该队列。它是基于指定的key产生的,而key既可以是ftok的返回值,也可以是常值`IPC_PRIVATE`(这种进程的创建方式key可以重复,就代表私有消息队列,不存在共享了)。
oflag读写权限值的组合。它还可以与`IPC_CREAT或IPC_CREAT | IPC_EXCL`按位或。
当创建一个新消息队列时,msqid_ds 结构的如下成员被初始化。
- msg_perm 结构的uid和cuid成员被设置成当前进程的有效用户ID, gid和cgid成员被设置成当前进程的有效组ID。
- oflag中的读写权限位存放在msg_perm.mode中。
- msg_qnum、msg_lspid、msg_lrpid、msg_stime和msg_rtime被置为0。
- msg_ctime被设置成当前时间。
- msg_qbytes被设置成系统限制值。


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240317191626.png" alt="image.png" style="zoom:50%;" />
实例代码


```c
int main() {

  int msgid;
//  msgid = msgget(1234, 0666|IPC_CREAT | IPC_EXCL);
//  msgid = msgget(IPC_PRIVATE, 0666|IPC_CREAT | IPC_EXCL);
  msgid = msgget(1234, 0);

  if (msgid == -1) {
    ERR_EXIT("msgget");
  }

  printf("msgget success\n");
  printf("msgid = %d\n", msgid);


  return EXIT_SUCCESS;
}
```

### msgctl 函数
msgctl 函数提供在一个消息队列上的各种**控制操作**。
```c
#include <sys/msg.h>
int msgctl(int msqid, int cmd, struct msqid_ds *buff);
            // 返回：若成功则为0,若出错则为-1
```

msgctl 函数提供3个命令。

|    命令    |                                                      说明                                                      |
| :------: | :----------------------------------------------------------------------------------------------------------: |
| IPC_RMID |                                    从系统中删除由msqid指定的消息队列。当前在该队列上的任何消息都被丢弃。                                     |
| IPC-SET  | 给所指定的消息队列设置其msqid_ds结构的以下4个成员： msg_perm.uid、msg_perm.gid、msg_perm.mode和msg_qbytes。 它们的值来自由buff参数指向的结构中的相应成员。 |
| IPC-STAT |                                   (通过buff参数)给调用者返回与所指定消息队列对应的当前msgid_ds结构。                                   |
  
示例代码
```c
  int msgid;
  msgid = msgget(1234, 0);
  if (msgid == -1) {
    ERR_EXIT("msgget");
  }
  printf("msgget success\n");
  printf("msgid = %d\n", msgid);
  msgctl(msgid,IPC_RMID,nullptr);
```

```c
  int msgid;
  msgid = msgget(1234, 0);

  if (msgid == -1) {
    ERR_EXIT("msgget");
  }

  printf("msgget success\n");
  printf("msgid = %d\n", msgid);
  msqid_ds buf;

  msgctl(msgid,IPC_STAT,&buf);
  printf("mode = %o\n",buf.msg_perm.mode);
  printf("bytes = %ld\n",buf.__msg_cbytes);
  printf("number = %ld\n",buf.msg_qnum);
  printf("msgmnb = %ld\n",buf.msg_qbytes);
```

```c
  int msgid;
  msgid = msgget(1234, 0);

  if (msgid == -1) {
    ERR_EXIT("msgget");
  }

  msqid_ds buf{};
  msgctl(msgid,IPC_STAT,&buf);

  sscanf("600","%o",&buf.msg_perm.mode);

  msgctl(msgid,IPC_SET,&buf);
```


### msgsnd 函数
使用msgget打开一个消息队列后,我们使用msgsnd往其上**放置一个消息**。

```c
#include <sys/msg.h>
int msgsnd(int msqid, const void *ptr, size_t length, int flag);
    	            // 返回：若成功则为0,若出错则为-1
```

**参数**：
其中msqid是由msgget返回的标识符。ptr是一个结构指针,该结构具有如下的模板,它定义在`<sys/msg.h>`中
- **length**:是msgp指向的消息长度,这个长度不含保存消息类型的那个long int长整型
- **flag**:控制着当前消息队列满或到达系统上限时将要发生的事情

```c
struct msgbuf{
    long mtype;/*消息类型*/
    char mtext[512];/*消息数据*/
}
```

- flag=IPC_NOWAIT表示队列满不等待,返回EAGAIN错误。
- 消息结构在两方面受到制约。首先,它必须小于系统规定的上限值;其次,它必须以一个long int长整数开始,接收者函数将利用这个长整数确定消息的类型

### msgrcv 函数
使用msgrcv函数从某个消息队列中读出一个消息。

```c
#include <sys/msg.h>
ssize_t msgrcv(int msqid, void *ptr, size_t length, long type, int flag);
	// 返回：若成功则为读入缓冲区中数据的字节数,若出错则为-1
```

其中ptr参数指定所接收消息的存放位置。跟msgsnd一样,该指针指向紧挨在真正的消息数据之前返回的长整数类型字段,length指定了由ptr指向的缓冲区中数据部分的大小。这是该函数能返回的最大数据量。该长度不包括长整数类型字段。
type指定希望从所给定的队列中读出什么样的消息。
- 如果type为0,那就返回该队列中的第一个消息。既然每个消息队列都是作为一个FIFO(先进先出)链表维护的,因此ype为0指定返回该队列中最早的消息。
- 如果type大于0,那就返回其类型值为type的第一个消息。
- 如果tpe小于0,那就返回其类型值小于或等于type参数的绝对值的消息中类型值最小的第一个消息。

- 返回值:成功返回实际放到接收缓冲区里去的字符个数,失败返回-1


## system V信号量


`semid_ds`结构是一个在UNIX和类UNIX操作系统中用于信号量集(semaphore set)管理的数据结构。信号量是同步工具,用于控制多个进程对共享资源的访问。

```c
struct semid_ds {
    struct ipc_perm sem_perm;  // Ownership and permissions
    time_t          sem_otime; // Last semop time
    time_t          sem_ctime; // Last change time
    unsigned short  sem_nsems; // No. of semaphores in set
    // ... 可能还有其他的实现特定字段
};
```

- **sem_perm**：一个ipc_perm结构,包含基本的IPC权限。这个结构包括所有者的用户ID和组ID,创建者的用户ID和组ID,以及信号量集的访问权限。
- **sem_nsems**：信号量集中信号量的数量。
- **sem_otime**：最后一次semop操作的时间。semop是一个系统调用,用于在信号量上执行操作(如增加或减少信号量的值)。
- **sem_ctime**：最后一次改变信号量集的时间。这可能是由semctl系统调用造成的,该调用用于控制信号量集的各种属性。

```c
// 用于创建或访问信号量集
int semget(key_t key, int nsems, int semflg);
```

参数
- key：一个键值,用于标识信号量集。不同进程可以通过相同的键值访问同一信号量集。特殊的键值IPC_PRIVATE创建一个私有的信号量集,只能由创建它的进程及其子进程访问。
- nsems：需要的信号量数目。当创建新信号量集时,这个参数指定集合中信号量的数量。如果访问现有的集合,通常将此参数设置为0。
- semflg：一个整数,指定标志和权限。这些标志包括权限位(如IPC_CREAT,IPC_EXCL等),以及用于设置新信号量集的权限的9位权限掩码(类似于文件权限,指定用户、组和其他的读写权限)。
返回值
- 成功时,semget返回信号量集的标识符(一个非负整数),可用于后续的信号量操作(如semop和semctl)。
- 失败时,返回-1,并设置errno以指示错误原因(例如,EACCES权限拒绝,EINVAL参数无效等)。

示例代码如下： 
```c
#include <sys/ipc.h>
#include <sys/sem.h>

// 创建或获取信号量集的示例
int sem_id;
key_t key = 1234; // 示例键值
int nsems = 1; // 信号量集中只有一个信号量

sem_id = semget(key, nsems, IPC_CREAT | 0666);
if (sem_id == -1) {
    // 错误处理
}

```


函数原型
semctl函数的原型定义在<sys/sem.h>头文件中,其形式如下：

```c
int semctl(int semid, int semnum, int cmd, ...);
```

参数
- semid：信号量集的标识符,这是通过semget函数获得的。
- semnum：信号量集中信号量的索引,用于指定要操作的信号量。对于某些命令,此参数可能被忽略。
- cmd：指定要执行的命令。这个参数决定了semctl如何操作以及额外参数的意义。
- ...：可变参数。根据cmd的不同,可能需要提供额外的参数。例如,设置信号量值时,需要提供一个
	```c
	union semun {
	    int val;                    // 用于SETVAL的值
	    struct semid_ds *buf;       // 用于IPC_STAT和IPC_SET的缓冲区
	    unsigned short *array;      // 用于GETALL和SETALL的数组
	    struct seminfo *__buf;      // 用于IPC_INFO(Linux特有)的缓冲区
	};
	```


命令(cmd)
semctl支持多种命令,下面是一些常用命令的简介：
- IPC_STAT：将信号量集的semid_ds结构复制到用户提供的缓冲区中。这可以用来获取信号量集的当前信息。
- IPC_SET：根据用户提供的semid_ds结构中的值来设置信号量集的参数。
- IPC_RMID：立即删除信号量集,释放其所有资源。
- GETVAL：获取单个信号量的值。
- SETVAL：设置单个信号量的值。这通常用于初始化信号量。
- GETALL：获取信号量集中所有信号量的值。
- SETALL：设置信号量集中所有信号量的值。


```c
// 用于对信号量执行操作
int semop(int semid, struct sembuf *sops, size_t nsops);
```

- 参数
	- **semid**：信号量集的标识符,这是通过`semget`函数获得的。
	- **sops**：指向`sembuf`结构数组的指针,每个结构指定了要在单个信号量上执行的操作。
	- **nsops**：`sops`数组中`sembuf`结构的数量,即要执行的操作数。


`sembuf`结构用于指定在单个信号量上执行的操作,定义如下：
```c
struct sembuf {
    unsigned short sem_num; // 信号量集合中的信号量编号
    short          sem_op;  // 对信号量执行的操作
    short          sem_flg; // 操作标志
};
```

- **sem_num**：信号量集合中的信号量编号,从0开始。
- **sem_op**：指定操作类型。如果`sem_op`大于0,信号量的值会增加`sem_op`的数量,这通常用来释放资源。如果`sem_op`为0,操作会阻塞直到信号量的值也为0。如果`sem_op`小于0,那么`sem_op`的绝对值会被尝试从信号量的值中减去,如果信号量的值不够,调用会阻塞直到足够为止。
- **sem_flg**：操作标志,可以是`IPC_NOWAIT`(非阻塞操作,如果操作不能立即执行,则返回错误)或`SEM_UNDO`(在进程退出时自动撤销对信号量的操作)的组合。


代码例子

```c
union semun {
  int val;              // 用于SETVAL的值
  struct semid_ds *buf; // 用于IPC_STAT和IPC_SET的缓冲区
  unsigned short *array;// 用于GETALL和SETALL的数组
  struct seminfo *__buf;// 用于IPC_INFO(Linux特有)的缓冲区
};


void usage();
int sem_create(key_t key) {
  int semid;
  semid = semget(key, 1, IPC_CREAT | IPC_EXCL | 0666);
  if (semid == -1) {
    ERR_EXIT("semget");
  }
  return semid;
}

int sem_open(key_t key) {
  int semid;
  semid = semget(key, 0, 0);
  if (semid == -1) {
    ERR_EXIT("semget");
  }
  return semid;
}
int sem_setval(int semid, int val) {
  semun un{};
  un.val = val;
  int ret = semctl(semid, 0, SETVAL, un);
  if (ret == -1) {
    ERR_EXIT("semctl");
  }
  return 0;
}

int sem_getval(int semid) {

  int ret = semctl(semid, 0, GETVAL);
  if (ret == -1) {
    ERR_EXIT("semctl");
  }
  printf("current value is %d\n", ret);
  return ret;
}

int sem_d(int semid) {
  int ret = semctl(semid, 0, IPC_RMID);
  if (ret == -1) {
    ERR_EXIT("semctl");
  }
  return 0;
}

int sem_p(int semid) {
  sembuf sb = {0, -1, /*0*//*IPC_NOWAIT*/SEM_UNDO};
  int ret = semop(semid, &sb, 1);
  if (ret == -1) {
    ERR_EXIT("semop");
  }
  return ret;
}

int sem_v(int semid) {
  sembuf sb = {0, 1, 0};
  int ret = semop(semid, &sb, 1);
  if (ret == -1) {
    ERR_EXIT("semop");
  }
  return ret;
}

int sem_getmode(int semid) {
  semid_ds buf{};
  semun su{};
  su.buf = &buf;
  int ret = semctl(semid, 0, IPC_STAT, su);
  if (ret == -1) {
    ERR_EXIT("semctl");
  }
  printf("current permissions is %o\n", su.buf->sem_perm.mode);
  return 0;
}

int sem_setmode(int semid, char *mode) {
  semid_ds buf{};
  semun su{};
  su.buf = &buf;
  int ret = semctl(semid, 0, IPC_STAT, su);
  if (ret == -1) {
    ERR_EXIT("semctl");
  }
  sscanf(mode, "%o", &buf.sem_perm.mode);
  ret = semctl(semid, 0, IPC_SET, su);
  if (ret == -1) {
    ERR_EXIT("semctl");
  }
  printf("permissions updated...\n");
  return 0;
}


int main(int argc, char **argv) {

  int opt;
  /*
   * -c 创建
   * -d 删除
   * -g 获取
   * -p 减少
   * -v 增加
   * -s val 设置
   * -f 查看权限
   * -m mode 更改权限
   */
  opt = getopt(argc, argv, "cdgpvs:fm:");

  if (opt == '?') { exit(EXIT_FAILURE); }

  if (opt == -1) {
    usage();
    exit(EXIT_FAILURE);
  }

  // 为了获取一个独一无二的通信对象,必须使用键(可使用ftok()函数生成,返回值key)。
  // 这里的键是用来定位I P C 对象的标识符的
  key_t key = ftok(".", 's');

  int semid;
  switch (opt) {
    case 'c':
      semid = sem_create(key);
      printf("create semid = %d\n", semid);
      break;
    case 'd':
      semid = sem_open(key);
      sem_d(semid);
      printf("delete semid = %d\n", semid);
      break;
    case 'g':
      semid = sem_open(key);
      sem_getval(semid);
      break;
    case 'p':
      semid = sem_open(key);
      sem_p(semid);
      sem_getval(semid);
      break;
    case 'v':
      semid = sem_open(key);
      sem_v(semid);
      sem_getval(semid);
      break;
    case 's':
      semid = sem_open(key);
      sem_setval(semid, atoi(optarg));
      sem_getval(semid);
      break;
    case 'f':
      semid = sem_open(key);
      sem_getmode(semid);
      break;
    case 'm':
      semid = sem_open(key);
      sem_setmode(semid, argv[2]);
      break;
  }


  return EXIT_SUCCESS;
}
void usage() {
  //    创建信号量 -c
  fprintf(stderr, "semtool -c\n");
  //    删除	  -d
  fprintf(stderr, "semtool -d\n");
  //    获得值    -g
  fprintf(stderr, "semtool -g\n");
  //    设置值    -s <val>
  fprintf(stderr, "semtool -s <val>\n");
  //    p操作	 -p
  fprintf(stderr, "semtool -p\n");
  //    v操作     -v
  fprintf(stderr, "semtool -v\n");
  //    查看权限   -f
  fprintf(stderr, "semtool -f\n");
  //    更改权限   -m <mode>
  fprintf(stderr, "semtool -m <mode>\n");
}

```


```c
union semun {
  int val;              // 用于SETVAL的值
  struct semid_ds *buf; // 用于IPC_STAT和IPC_SET的缓冲区
  unsigned short *array;// 用于GETALL和SETALL的数组
  struct seminfo *__buf;// 用于IPC_INFO(Linux特有)的缓冲区
};

int semid;
void philosopher(int no);
void get2fork(int no);
void put2fork(int no);

// 哲学家用餐例子
int main(int argc,char *argv[]) {

  // 1.创建一个信号集
  semid=semget(IPC_PRIVATE,5, IPC_CREAT | 0666);
  if (semid == -1) {
    ERR_EXIT("semget");
  }

  // 2.初始化叉子的状态
  semun su={};
  su.val=1;
  for (int i = 0; i < 5; ++i) {
    semctl(semid,i,SETVAL,su);
  }

  pid_t pid;
  int no=0; // 当前进程的编号

  for (int i = 0; i < 5; ++i) {
    pid=fork();
    if (pid==-1) {
      ERR_EXIT("fork");
    }
    if (pid==0) {
      no=i;
      break;
    }
  }

  philosopher(no);
  return EXIT_SUCCESS;
}

void philosopher(int no) {
  while (true){
    // 思考
    printf("%d号哲学家正在思考\n",no);
    sleep(rand() % 3);
    // 感到饥饿
    printf("%d号哲学家饿了\n",no);
    // 拿两个叉子
    get2fork(no);
    printf("%d号哲学家正在吃饭\n",no);
    sleep(rand() % 3);
    // 放下两个叉子
    put2fork(no);
  }
}

void put2fork(int no) {
  unsigned short int left=no;
  unsigned short int right=(no+1)%5;
  sembuf sembuf[2]={{left,1,0},{right,1,0}};
  int ret=semop(semid,sembuf,2);
  if (ret==-1) {
    ERR_EXIT("put2fork");
  }
}

void get2fork(int no) {
  unsigned short int left=no;
  unsigned short int right=(no+1)%5;
  sembuf sembuf[2]={{left,-1,0},{right,-1,0}};
  int ret=semop(semid,sembuf,2);
  if (ret==-1) {
    ERR_EXIT("get2fork");
  }
}

```

# POSIX 标准

## POSIX 消息队列
功能:用来创建和访问一个消息队列
原型:
```c
mqd_t mq_open(const char *name, int oflag);
mqd_t mq_open(const char *name, int oflag, mode_t mode,struct mq_attr *attr);
```

参数
- name:某个消息队列的名字
- oflag: 与open函数类似,可以是O_RDONLY、O WRONLY.
	- O_RDWR,还可以按位或上O_CREAT、O_EXCL、
	- O_NONBLOCK等。
- mode:如果oflag指定了O_CREAT,需要设置mode。
- attr：(可选)一个指向 mq_attr结构的指针,该结构指定了消息队列的属性。如果此参数为NULL,则使用系统默认属性。
- 返回值:成功返回消息队列文件描述符;失败返回(mqd_t)-1


- 功能:关闭消息队列
- 原型
	```c
	mqd_tmq_close(mqd_t mqdes);
	```
- 参数
	- mqdes:消息队列描述符
- 返回值:成功返回0;失败返回-1


- 功能:删除消息队列
- 原型
	```c
	mqd_tmq_unlink(const thar *name);
	```
- 参数
	- name:消息队列的名字
- 返回值:成功返回0;失败返回-1


- 功能:获取/设置消息队列属性
- 原型
	```c
	mqd_tmq_getattr(mqd_t mqdes, struct mq_attr *attr);
	mqd_tmq_setattr(mqd_t mqdes, struct mq_attr *newattr, struct mq_attr *oldattr);
	```

- 参数
	- mqdes：是消息队列描述符,通常是之前通过调用`mq_open`函数获得的。
	- attr：是指向`mq_attr`结构的指针,该结构用于返回消息队列的属性。`mq_attr`结构体定义如下：

```c
struct mq_attr {
    long mq_flags;   /* 消息队列标志：例如,是否为非阻塞 */
    long mq_maxmsg;  /* 队列中允许的最大消息数量 */
    long mq_msgsize; /* 每个消息的最大字节数 */
    long mq_curmsgs; /* 队列中当前的消息数量 */
};
```
- 返回值:成功返回0;失败返回-1


- 功能:用于向 POSIX 消息队列发送消息。
- 原型
```c
#include <mqueue.h>
int mq_send(mqd_t mqdes, const char *msg_ptr, size_t msg_len, unsigned msg_prio);
```

- 参数说明：
	- `mqdes`：消息队列描述符,是通过之前调用 `mq_open` 函数成功打开一个消息队列后获得的。
	- `msg_ptr`：指向要发送的消息的指针。
	- `msg_len`：消息的长度,以字节为单位。这个长度不能超过在消息队列创建时指定的最大消息长度。
	- `msg_prio`：消息的优先级。在 POSIX 消息队列中,每个消息都有一个优先级,数值越低表示优先级越低。队列按优先级顺序(从最高到最低)排序消息；同优先级的消息则按照先进先出(FIFO)的顺序处理。

- 返回值：
	- 成功时,`mq_send` 返回 `0`。
	- 发生错误时,返回 `-1` 并设置 `errno` 来指示错误的原因。

- 常见的 `errno` 值包括：
	- `EAGAIN`：消息队列已满,且队列被设置为非阻塞模式。
	- `EBADF`：提供的消息队列描述符无效。
	- `EMSGSIZE`：`msg_len` 超过了在消息队列创建时指定的最大消息长度。
	- `EINTR`：调用在完成前被信号中断。



- 功能:用于从 POSIX 消息队列中接收消息。
- 原型
```c
#include <mqueue.h>
ssize_t mq_receive(mqd_t mqdes, char *msg_ptr, size_t msg_len, unsigned *msg_prio);
```

- 参数说明：
	- `mqdes`：消息队列描述符,是通过之前调用 `mq_open` 函数成功打开一个消息队列后获得的。
	- `msg_ptr`：指向缓冲区的指针,该缓冲区用于接收从消息队列中读取的消息。
	- `msg_len`：`msg_ptr` 指向的缓冲区的大小。这个大小应该足够大,能够容纳队列中任意消息的最大长度。
	- `msg_prio`：一个指向 `unsigned int` 的指针,用于接收读取的消息的优先级。如果这个参数不是 `NULL`,则读取的消息的优先级将被存储在 `*msg_prio` 指向的位置。

- 返回值：
	- 成功时,`mq_send` 返回 `0`。
	- 发生错误时,返回 `-1` 并设置 `errno` 来指示错误的原因。

- 常见的 `errno` 值包括：
	- `EAGAIN`：消息队列已满,且队列被设置为非阻塞模式。
	- `EBADF`：提供的消息队列描述符无效。
	- `EMSGSIZE`：`msg_len` 超过了在消息队列创建时指定的最大消息长度。
	- `EINTR`：调用在完成前被信号中断。


- 功能:允许进程注册或取消注册**接收关于消息队列状态变化**的通知。具体来说,它可以用于设置一个通知机制,当消息队列从空变为非空时(也就是说,当消息队列中出现了至少一条消息时),它会通知一个进程或线程。
- 原型
```c
#include <mqueue.h>
int mq_notify(mqd_t mqdes, const struct sigevent *notification);
```

- 参数说明：
	- `mqdes`：是消息队列描述符,是通过之前调用 `mq_open` 函数成功打开一个消息队列后获得的。
	- `notification`：是指向 `sigevent` 结构的指针,该结构指定当消息队列变为非空时如何通知进程。如果此参数为 `NULL`,则取消之前注册的任何通知。

	- `sigevent` 结构可用于指定多种通知类型,最常见的包括：
		- 发送信号(`SIGEV_SIGNAL`)：当消息到达时,向进程发送一个信号。
		- 启动线程(`SIGEV_THREAD`)：当消息到达时,启动一个新线程来处理消息。



```c
struct sigevent {
    int          sigev_notify;             // 通知类型
    int          sigev_signo;              // 信号编号
    union sigval sigev_value;              // 信号值
    void       (*sigev_notify_function)(union sigval); // 通知函数
    pthread_attr_t *sigev_notify_attributes; // 线程属性 
};
```

```c
union sigval {         /* Data passed with notification */
	int  sival_int;    /* Integer value */
	void *sival_ptr;   /* Pointer value */
};
```



- 返回值：
	- 成功时,`mq_notify` 返回 `0`。
	- 发生错误时,返回 `-1` 并设置 `errno` 来指示错误的原因。


- 任何时刻只能有一个进程可以被注册为接收某个给定队列的通知
- 当有一个消息到达某个先前为空的队列,而且已有一个进程被注册为接收该队列的通知时,只有没有任何线程阻塞在该队列的`mq_receive`调用的前提下,通知才会发出。
- 当通知被发送给它的注册进程时,其注册被撤消。进程必须再次调用`mq_notify`以重新注册飞如果需要的话),重新注册要放在从消息队列读出消息之前而不是之后。

## POSIX 共享内存

- 功能:用于创建或打开一个POSIX共享内存对象
- 原型
```c
#include <fcntl.h>
#include <sys/mman.h>
int shm_open(const char *name, int oflag, mode_t mode);
```

- 参数说明：
	- `name`：共享内存对象的名称。这个名称对所有进程都是可见的,并且必须以斜杠(`/`)开始。例如,`"/myshm"`。
	- `oflag`：打开标志。常用的标志包括`O_RDONLY`(只读打开)、`O_RDWR`(读写打开)和`O_CREAT`(如果对象不存在,则创建)。`O_CREAT`时通常会与`O_EXCL`一起使用,后者确保只有当共享内存对象不存在时才创建,避免覆盖已存在的对象。
	- `mode`：如果创建新的共享内存对象,这个参数指定对象的权限。它是一个八进制数,表示文件的访问权限(类似于`chmod`命令)。例如,`0644`表示所有者具有读写权限,而组和其他用户具有只读权限


- 返回值：
	- 成功时,返回共享内存对象的文件描述符,该描述符可用于后续的内存映射(`mmap`)调用。
	- 出错时,返回`-1`,并设置`errno`以指示错误的具体原因。
	- `shm_open`函数通常与`mmap`函数结合使用,后者用于将共享内存对象映射到进程的地址空间。这样,进程就可以通过访问这块内存来共享数据





- 功能:修改共享内存对象大小
- 原型
	```c
	int ftruncate(int fd, off_t length);
	```

- 参数
	- fd:文件描述符
	- length:长度
- 返回值:成功返回0;失败返回-1


- 功能:获取共享内存对象信息
- 原型
```c
int fstat(int fd, struct stat *buf);
```
- 参数
	- fd:文件描述符
	- buf:返回共享内存状态
- 返回值:成功返回0;失败返回-1

# asio网络编程


基本的服务器流程我们在前面的[[网络编程#TCP通信流程|TCP通信流程]]可以看到

<img src="https://cdn.llfc.club/1540562-20190417002428451-62583604.jpg" alt="image.png" style="zoom:80%;" />

## asio之socket的创建和连接

### 终端节点的创建

- 所谓**终端节点**就是用来通信的端对端的节点，可以通过ip地址和端口构造，其的节点可以连接这个终端节点做通信。 
- 如果我们是客户端，我们可以通过对端的ip和端口构造一个endpoint，用这个endpoint和其通信。
	```c++
	int  client_end_point() {
	    // 步骤 1. 假设客户端应用程序已经获得了 IP 地址和协议端口号。
	    std::string raw_ip_address = "127.0.0.1";
	    unsigned short port_num = 3333;
	    // 用于存储在解析原始 IP 地址时发生的错误信息。
	    boost::system::error_code ec;
	    // 步骤 2. 使用 IP 协议版本无关的地址表示。
	    asio::ip::address ip_address =
	        asio::ip::address::from_string(raw_ip_address, ec);
	    if (ec.value() != 0) {
	        // 提供的 IP 地址无效。中断执行。
	        std::cout
	            << "Failed to parse the IP address. Error code = "
	            << ec.value() << ". Message: " << ec.message();
	        return ec.value();
	    }
	    // 步骤 3.
	    asio::ip::tcp::endpoint ep(ip_address, port_num);
	    // 步骤 4. 端点已准备就绪,可以用来指定网络中客户端希望与之通信的特定服务器。
	    return 0;
	}
	```

- 如果是服务端，则只需根据本地地址绑定就可以生成endpoint
	```c++
	int server_end_point(){
	    // 步骤1. 这里我们假设服务器应用程序已经获取了协议端口号。
	    unsigned short port_num = 3333;
	    
	    // 步骤2. 创建 asio::ip::address 类的特殊对象，
	    // 该对象指定主机上可用的所有IP地址。注意
	    // 这里我们假设服务器在IPv6协议上工作。
	    asio::ip::address ip_address = asio::ip::address_v6::any();
	    
	    // 步骤3. 创建 asio::ip::tcp::endpoint 实例。
	    asio::ip::tcp::endpoint ep(ip_address, port_num);
	    
	    // 步骤4. 端点已创建，可以用来
	    // 指定服务器应用程序希望监听传入连接的
	    // IP地址和端口号。
	    return 0;
	}
	```

### 创建socket

- socket进行通信必须要一个参数，这个参数就是上下文
- 上下文是boost的asio的一个核心服务,它的所有的服务都是通过上下文服务来通信的

```c++
int create_tcp_socket() {
    // 步骤1. 需要 'io_service' 类的实例作为套接字构造函数的参数。
    asio::io_context  ios;

    // 步骤2. 创建 'tcp' 类的对象，该对象表示使用 IPv4 作为底层协议的 TCP 协议。
    asio::ip::tcp protocol = asio::ip::tcp::v4();

    // 步骤3. 实例化一个活动的 TCP 套接字对象。
    asio::ip::tcp::socket sock(ios);

    // 用于存储打开套接字时发生的错误信息。
    boost::system::error_code ec;

    // 步骤4. 打开套接字。
    sock.open(protocol, ec);
    if (ec.value() != 0) {
        // 打开套接字失败。
        std::cout
            << "Failed to open the socket! Error code = "
            << ec.value() << ". Message: " << ec.message();
        return ec.value();
    }
    return 0;
}
```


上述socket只是通信的socket，如果是服务端，我们还需要生成一个acceptor的socket，用来接收新的连接。
```c++
int create_acceptor_socket() {
	/*
    // 步骤1：需要一个'io_service'类的实例作为socket构造函数的参数。
    asio::io_context ios;
    // 步骤2：创建一个表示TCP协议的'tcp'类对象，使用IPv6作为底层协议。
    asio::ip::tcp protocol = asio::ip::tcp::v6();
    // 步骤3：实例化一个acceptor socket对象。
    asio::ip::tcp::acceptor acceptor(ios);
    // 用于存储在打开acceptor socket时发生的错误信息。
    boost::system::error_code ec;
    // 步骤4：打开acceptor socket。
    acceptor.open(protocol, ec);
    if (ec.value() != 0) {
        // 无法打开socket。
        std::cout
            << "无法打开acceptor socket！"
            << "错误代码 = "
            << ec.value() << "。消息：" << ec.message();
        return ec.value();
    }
	*/
	// 新版本更简单的方法
	asio::ip::tcp::acceptor a(ios,asio::ip::tcp::endpoint(asio::ip::tcp::v4(),3333));

    return 0;
}
```

### 绑定acceptor
对于acceptor类型的socket，服务器要将其绑定到指定的断点,所有连接这个端点的连接都可以被接收到。
```c++
int bind_acceptor_socket() {
    // 步骤1：假设服务器应用程序已经获取了协议端口号。
    unsigned short port_num = 3333;
    // 步骤2：创建端点。
    asio::ip::tcp::endpoint ep(asio::ip::address_v4::any(), port_num);
    // 'acceptor'类构造函数使用。
    asio::io_context ios;
    // 步骤3：创建并打开一个acceptor socket。
    asio::ip::tcp::acceptor acceptor(ios, ep.protocol());
    boost::system::error_code ec;
    // 步骤4：绑定acceptor socket。
    acceptor.bind(ep, ec);
    // 处理任何错误。
    if (ec.value() != 0) {
        // 无法绑定acceptor socket。中断执行。
        std::cout << "无法绑定acceptor socket。"
            << "错误代码 = " << ec.value() << "。消息："
            << ec.message();
        return ec.value();
    }
    return 0;
}
```

### 连接指定的端点

作为客户端可以连接服务器指定的端点进行连接
```c++
int connect_to_end() {
    // 步骤1：假设客户端应用程序已经获取了目标服务器的IP地址和协议端口号。
    std::string raw_ip_address = "127.0.0.1";
    unsigned short port_num = 3333;
    try {
        // 步骤2：创建一个指定目标服务器应用程序的端点。
        asio::ip::tcp::endpoint ep(asio::ip::address::from_string(raw_ip_address), port_num);
        asio::io_context ios;
        // 步骤3：创建并打开一个socket。
        asio::ip::tcp::socket sock(ios, ep.protocol());
        // 步骤4：连接socket。
        sock.connect(ep);
        // 此时socket 'sock'已经连接到服务器应用程序，可以用于发送或接收数据。
    }
    // 这里使用了asio::ip::address::from_string()和asio::ip::tcp::socket::connect()
    // 的异常重载函数，以处理错误情况。
    catch (system::system_error &e) {
        std::cout << "发生错误！错误代码 = " << e.code()
                  << "。消息：" << e.what();
        return e.code().value();
    }
}
```

```c++
#include <asio.hpp>
#include <iostream>
#include <string>
int dns_connect_to_end(){
  // 定义目标主机和端口
  std::string host = "llfc.club";
  std::string port_num = "3333";

  // 创建 ASIO 的 I/O 上下文对象，所有的异步操作都需要它来运行
  asio::io_context ios;

  // 创建一个 DNS 查询，参数指定了主机名、端口号，以及服务类型（这里指定为数字服务，意味着 port_num 是一个服务的数字标识）
  asio::ip::tcp::resolver::query resolver_query(host, port_num, asio::ip::tcp::resolver::query::numeric_service);

  // 创建解析器对象，用于 DNS 解析
  asio::ip::tcp::resolver resolver(ios);

  try {
    // 执行 DNS 解析，返回一个迭代器，指向解析结果的开始
    asio::ip::tcp::resolver::iterator it = resolver.resolve(resolver_query);

    // 创建一个 TCP 套接字
    asio::ip::tcp::socket sock(ios);

    // 使用解析出的端点信息来连接套接字
    asio::connect(sock, it);
  }
  catch (asio::system_error &e) { // 捕获可能发生的异常，通常是网络错误或 DNS 解析失败
    std::cout << "Error occurred! Error code = " << e.code()
              << ". Message: " << e.what();
    // 返回错误代码
    return e.code().value();
  }
  // 若连接成功，返回 0 表示无错误
  return 0;
}
```


### 服务器接收连接
- 当有客户端连接时，服务器需要接收连接
```cpp
int accept_new_connection(){
    // 用于存储挂起连接请求的队列大小。
    const int BACKLOG_SIZE = 30;
    // 步骤1：假设服务器应用程序已经获取了协议端口号。
    unsigned short port_num = 3333;
    // 步骤2：创建服务器端点。
    asio::ip::tcp::endpoint ep(asio::ip::address_v4::any(), port_num);
    asio::io_context ios;
    try {
        // 步骤3：实例化并打开一个acceptor socket。
        asio::ip::tcp::acceptor acceptor(ios, ep.protocol());
        // 步骤4：将acceptor socket绑定到服务器端点。
        acceptor.bind(ep);
        // 步骤5：开始监听传入的连接请求。
        acceptor.listen(BACKLOG_SIZE);
        // 步骤6：创建一个活动socket。
        asio::ip::tcp::socket sock(ios);
        // 步骤7：处理下一个连接请求并将活动socket连接到客户端。
        acceptor.accept(sock);
        // 此时'sock' socket已连接到客户端应用程序，可以用于发送或接收数据。
    }
    catch (system::system_error& e) {
        std::cout << "发生错误！错误代码 = " << e.code()
            << "。消息：" << e.what();
        return e.code().value();
    }
}
```
## asio socket同步读写
### 关于buffer

- 任何网络库都有提供buffer的数据结构，所谓buffer就是接收和发送数据时缓存数据的结构。  
- `boost::asio`提供了`asio::mutable_buffer`和`asio::const_buffer`这两个结构，他们是一段连续的空间，**首字节存储了后续数据的长度**。  
- `asio::mutable_buffer`用于写服务，`asio::const_buffer`用于读服务。但是这两个结构都没有被asio的api直接使用。  
- 对于api的buffer参数，asio提出了`MutableBufferSequence`和`ConstBufferSequence`概念，他们是由多个`asio::mutable_buffer`和`asio::const_buffer`组成的。也就是说`boost::asio`为了节省空间，将一部分连续的空间组合起来，作为参数交给api使用。  
- 我们可以理解为`MutableBufferSequence`的数据结构为`std::vector<asio::mutable_buffer>`  
结构如下

<img src="https://cdn.llfc.club/1676257797218.jpg" alt="image.png" style="zoom:80%;" />

- 每隔`vector`存储的都是`mutable_buffer`的地址，每个`mutable_buffer`的第一个字节表示数据的长度，后面跟着数据内容。  
- 这么复杂的结构交给用户使用并不合适，所以asio提出了`buffer()`函数，该函数接收多种形式的字节流，该函数返回`asio::mutable_buffers_1 o`或者`asio::const_buffers_1`结构的对象。  
- 如果传递给`buffer()`的参数是一个只读类型，则函数返回`asio::const_buffers_1` 类型对象。  
- 如果传递给`buffer()`的参数是一个可写类型，则返回`asio::mutable_buffers_1` 类型对象。  
- `asio::const_buffers_1`和`asio::mutable_buffers_1`是`asio::mutable_buffer`和`asio::const_buffer`的适配器，提供了符合`MutableBufferSequence`和`ConstBufferSequence`概念的接口，所以他们可以作为`boost::asio`的api函数的参数使用。  
- 简单概括一下，我们可以用`buffer()`函数生成我们要用的缓存存储数据。  
- 比如boost的发送接口send要求的参数为`ConstBufferSequence`类型

```c++
template<typename ConstBufferSequence>
std::size_t send(const ConstBufferSequence & buffers);
```

我们需要将”Hello Word转化为该类型”

```c++
void use_const_buffer() {
    std::string buf = "hello world!";
    asio::const_buffer  asio_buf(buf.c_str(), buf.length());
    std::vector<asio::const_buffer> buffers_sequence;
    buffers_sequence.push_back(asio_buf);
}
```

最终buffers_sequence就是可以传递给发送接口send的类型。但是这太复杂了，可以直接用buffer函数转化为send需要的参数类型
```c++
void use_buffer_str() {
    asio::const_buffers_1 output_buf = asio::buffer("hello world");
}
```

output_buf可以直接传递给该send接口。我们也可以将数组转化为send接受的类型
```c++
void use_buffer_array(){
    const size_t  BUF_SIZE_BYTES = 20;
    std::unique_ptr<char[] > buf(new char[BUF_SIZE_BYTES]);
    auto input_buf = asio::buffer(static_cast<void*>(buf.get()), BUF_SIZE_BYTES);
}
```

对于流式操作，我们可以用streambuf，将输入输出流和streambuf绑定，可以实现流式输入和输出。
```c++
void use_stream_buffer() {
    asio::streambuf buf;
    std::ostream output(&buf);
    // 将消息写入基于流的缓冲区。
    output << "Message1\nMessage2";
    // 现在我们想要从流缓冲区中读取所有数据，直到遇到'\n'分隔符。
    // 实例化一个输入流，它使用我们的流缓冲区。
    std::istream input(&buf);
    // 我们将把数据读入这个字符串中。
    std::string message1;
    std::getline(input, message1);
    // 现在message1字符串中包含'Message1'。
}
```


### 同步写write_some

- `boost::asio`提供了几种同步写的api，`write_som`e可以每次向指定的空间写入**固定**的字节数，如果写缓冲区满了，就**只写一部分**，返回写入的字节数。

```c++
void write_to_socket(asio::ip::tcp::socket& sock) {
    std::string buf = "Hello World!";
    std::size_t  total_bytes_written = 0;
    // 循环发送
    // write_some返回每次写入的字节数
    // total_bytes_written是已经发送的字节数。
    // 每次发送buf.length()- total_bytes_written)字节数据
    while (total_bytes_written != buf.length()) {
        total_bytes_written += sock.write_some(asio::buffer(buf.c_str() + total_bytes_written, buf.length() - total_bytes_written));
    }
}
```


### 同步写send

- `write_some`使用起来比较麻烦，需要多次调用，asio提供了send函数。send函数会一次性将buffer中的内容发送给对端，如果有部分字节因为发送缓冲区满无法发送，则阻塞等待，直到发送缓冲区可用，则继续发送完成。

```c++
int send_data_by_send() {
    std::string raw_ip_address = "127.0.0.1";
    unsigned short port_num = 3333;
    try {
        asio::ip::tcp::endpoint ep(asio::ip::address::from_string(raw_ip_address), port_num);
        asio::io_context ios;
        // 步骤1：分配并打开socket。
        asio::ip::tcp::socket sock(ios, ep.protocol());
        sock.connect(ep);
        std::string buf = "Hello World!";
        int send_length = sock.send(asio::buffer(buf.c_str(), buf.length()));
        if (send_length <= 0) {
            cout << "发送失败" << endl;
            return 0;
        }
    }
    catch (system::system_error& e) {
        std::cout << "发生错误！错误代码 = " << e.code()
                  << "。消息：" << e.what();
        return e.code().value();
    }
    return 0;
}
```

### 同步写write

- 类似send方法，asio还提供了一个write函数，可以一次性将所有数据发送给对端，如果发送缓冲区满了则阻塞，直到发送缓冲区可用，将数据发送完成。
```c++
int send_data_by_wirte() {
    std::string raw_ip_address = "127.0.0.1";
    unsigned short port_num = 3333;
    try {
        asio::ip::tcp::endpoint
            ep(asio::ip::address::from_string(raw_ip_address),
                port_num);
        asio::io_service ios;
        // Step 1. Allocating and opening the socket.
        asio::ip::tcp::socket sock(ios, ep.protocol());
        sock.connect(ep);
        std::string buf = "Hello World!";
        int send_length  = asio::write(sock, asio::buffer(buf.c_str(), buf.length()));
        if (send_length <= 0) {
            cout << "send failed" << endl;
            return 0;
        }
    }
    catch (system::system_error& e) {
        std::cout << "Error occured! Error code = " << e.code()
            << ". Message: " << e.what();
        return e.code().value();
    }
    return 0;
 }
```

### 同步读read_some
- 同步读和同步写类似，提供了读取指定字节数的接口read_some
```c++
std::string read_from_socket(asio::ip::tcp::socket& sock) {
    const unsigned char MESSAGE_SIZE = 7;
    char buf[MESSAGE_SIZE];
    std::size_t total_bytes_read = 0;
    while (total_bytes_read != MESSAGE_SIZE) {
        total_bytes_read += sock.read_some(
            asio::buffer(buf + total_bytes_read,
                MESSAGE_SIZE - total_bytes_read));
    }
    return std::string(buf, total_bytes_read);
}
int read_data_by_read_some() {
    std::string raw_ip_address = "127.0.0.1";
    unsigned short port_num = 3333;
    try {
        asio::ip::tcp::endpoint
            ep(asio::ip::address::from_string(raw_ip_address),
                port_num);
        asio::io_service ios;
        asio::ip::tcp::socket sock(ios, ep.protocol());
        sock.connect(ep);
        read_from_socket(sock);
    }
    catch (system::system_error& e) {
        std::cout << "Error occured! Error code = " << e.code()
            << ". Message: " << e.what();
        return e.code().value();
    }
    return 0;
}
```

### 同步读receive

- 可以一次性同步接收对方发送的数据
```c++
int read_data_by_receive() {
    std::string raw_ip_address = "127.0.0.1";
    unsigned short port_num = 3333;
    try {
        asio::ip::tcp::endpoint
            ep(asio::ip::address::from_string(raw_ip_address),
                port_num);
        asio::io_service ios;
        asio::ip::tcp::socket sock(ios, ep.protocol());
        sock.connect(ep);
        const unsigned char BUFF_SIZE = 7;
         char buffer_receive[BUFF_SIZE];
        int receive_length =  sock.receive(asio::buffer(buffer_receive, BUFF_SIZE));
        if (receive_length <= 0) {
            cout << "receive failed" << endl;
        }
    }
    catch (system::system_error& e) {
        std::cout << "Error occured! Error code = " << e.code()
            << ". Message: " << e.what();
        return e.code().value();
    }
    return 0;
}
```

### 同步读read

- 可以一次性同步读取对方发送的数据

```c++
int read_data_by_read() {
    std::string raw_ip_address = "127.0.0.1";
    unsigned short port_num = 3333;
    try {
        asio::ip::tcp::endpoint
            ep(asio::ip::address::from_string(raw_ip_address),
                port_num);
        asio::io_service ios;
        asio::ip::tcp::socket sock(ios, ep.protocol());
        sock.connect(ep);
        const unsigned char BUFF_SIZE = 7;
        char buffer_receive[BUFF_SIZE];
        int receive_length = asio::read(sock, asio::buffer(buffer_receive, BUFF_SIZE));
        if (receive_length <= 0) {
            cout << "receive failed" << endl;
        }
    }
    catch (system::system_error& e) {
        std::cout << "Error occured! Error code = " << e.code()
            << ". Message: " << e.what();
        return e.code().value();
    }
    return 0;
 }
```

### 读取直到指定字符
- 我们可以一直读取，直到读取指定字符结束

```c++
std::string read_data_by_until(asio::ip::tcp::socket& sock) {
    asio::streambuf buf;
    // 同步从套接字读取数据，直到遇到'\n'符号。
    asio::read_until(sock, buf, '\n');
    std::string message;
    // 因为缓冲区'buf'可能包含'\n'符号后的其他数据，所以我们需要解析缓冲区，并提取分隔符前的数据。
    std::istream input_stream(&buf);
    std::getline(input_stream, message);
    return message;
}
```

## 同步读写的客户端和服务器示例

### 客户端设计

客户端设计基本思路是根据服务器对端的ip和端口创建一个endpoint，然后创建socket连接这个endpoint，之后就可以用同步读写的方式发送和接收数据了。

```c++
#include <iostream>
#include <boost/asio.hpp>
using namespace std;
using namespace boost::asio::ip;
const int MAX_LENGTH = 1024;
int main()
{
    try {
        //创建上下文服务
        boost::asio::io_context   ioc;
        //构造endpoint
        tcp::endpoint  remote_ep(address::from_string("127.0.0.1"), 10086);
        tcp::socket  sock(ioc);
        boost::system::error_code   error = boost::asio::error::host_not_found; ;
        sock.connect(remote_ep, error);
        if (error) {
            cout << "connect failed, code is " << error.value() << " error msg is " << error.message();
            return 0;
        }
        std::cout << "Enter message: ";
        char request[MAX_LENGTH];
        std::cin.getline(request, MAX_LENGTH);
        size_t request_length = strlen(request);
        boost::asio::write(sock, boost::asio::buffer(request, request_length));
        char reply[MAX_LENGTH];
        size_t reply_length = boost::asio::read(sock,
            boost::asio::buffer(reply, request_length));
        std::cout << "Reply is: ";
        std::cout.write(reply, reply_length);
        std::cout << "\n";
    }
    catch (std::exception& e) {
        std::cerr << "Exception: " << e.what() << endl;
    }
    return 0;
}
```

### 服务器
#### session函数
创建session函数，该函数为服务器处理客户端请求，每当我们获取客户端连接后就调用该函数。在session函数里里进行echo方式的读写，所谓echo就是应答式的处理
```c++
void session(socket_ptr sock) {
    try {
        for (;;) {
            char data[max_length];
            memset(data, '\0', max_length);
            boost::system::error_code error;
            size_t length = sock->read_some(boost::asio::buffer(data, max_length), error);
            if (error == boost::asio::error::eof) {
                std::cout << "connection closed by peer" << endl;
                break;
            }
            else if (error) {
                throw boost::system::system_error(error);
            }
            cout << "receive from " << sock->remote_endpoint().address().to_string() << endl;
            cout << "receive message is " << data << endl;
            //回传信息值
            boost::asio::write(*sock, boost::asio::buffer(data, length));
        }
    }
    catch (std::exception& e) {
        std::cerr << "Exception in thread: " << e.what() << "\n" << std::endl;
    }
}
```


#### server函数

server函数根据服务器ip和端口创建服务器acceptor用来接收数据，用socket接收新的连接，然后为这个socket创建session。
```c++
void server(boost::asio::io_context& io_context, unsigned short port) {
    tcp::acceptor a(io_context, tcp::endpoint(tcp::v4(), port));
    for (;;) {
        socket_ptr socket(new tcp::socket(io_context));
        a.accept(*socket);
        auto t =  std::make_shared<std::thread>(session, socket);
        thread_set.insert(t);
    }
}
```

### 同步读写的优劣
1. 同步读写的缺陷在于读写是**阻塞**的，如果客户端对端不发送数据服务器的read操作是阻塞的，这将导致服务器处于阻塞等待状态。
2. 可以通过开辟新的线程为新生成的连接处理读写，但是一个进程开辟的线程是有限的，约为2048个线程，在Linux环境可以通过`unlimit`增加一个进程开辟的线程数，但是线程过多也会**导致切换消耗的时间片**较多。
3. 该服务器和客户端为应答式，实际场景为**全双工通信模式**，发送和接收要独立分开。
4. 该服务器和客户端未考虑粘包处理。
综上所述，是我们这个服务器和客户端存在的问题，为解决上述问题，我们在接下里的文章里做不断完善和改进，主要以异步读写改进上述方案。
当然同步读写的方式也有其优点，比如客户端连接数不多，而且服务器并发性不高的场景，可以使用同步读写的方式。使用同步读写能简化编码难度。


## asio异步读写操作及注意事项

### 异步写操作
在写操作前，我们先封装一个Node结构，用来管理要发送和接收的数据，该结构包含数据域首地址，数据的总长度，以及已经处理的长度(已读的长度或者已写的长度)
```c++
//最大报文接收大小
const int RECVSIZE = 1024;
class  MsgNode {
public :
    MsgNode(const char* msg,  int total_len): _total_len(total_len), _cur_len(0){
        _msg = new char[total_len];
        memcpy(_msg, msg, total_len);
    }
    MsgNode(int total_len) :_total_len(total_len), _cur_len(0) {
        _msg = new char[total_len];
    }
    ~MsgNode(){
        delete[]_msg;
    }
    //消息首地址
    char* _msg;
    //总长度
    int _total_len;
    //当前长度
    int _cur_len;
};
```

写了两个构造函数，两个参数的负责构造写节点，一个参数的负责构造读节点。  
接下来为Session添加异步写操作和负责发送写数据的节点

- 需要一个回调函数`WriteCallBackErr`，其中`bytes_transferred`代表我们写了多少字符
- `WriteToSocketErr`是封装的异步写函数

```c++
class Session{
public:
    void WriteCallBackErr(const boost::system::error_code & ec, std::size_t bytes_transferred,std::shared_ptr<MsgNode>); //
    void WriteToSocketErr(const std::string& buf);
private:
    std::shared_ptr<MsgNode> _send_node;
};
```

WriteToSocketErr函数为我们封装的写操作，WriteCallBackErr为异步写操作回调的函数，为什么会有三个参数呢，我们可以看一下asio源码
```c++
BOOST_ASIO_COMPLETION_TOKEN_FOR(void (boost::system::error_code,
        std::size_t)) WriteToken
          BOOST_ASIO_DEFAULT_COMPLETION_TOKEN_TYPE(executor_type)>
  BOOST_ASIO_INITFN_AUTO_RESULT_TYPE_PREFIX(WriteToken,
      void (boost::system::error_code, std::size_t))
  async_write_some(const ConstBufferSequence& buffers,
      BOOST_ASIO_MOVE_ARG(WriteToken)token
        BOOST_ASIO_DEFAULT_COMPLETION_TOKEN(executor_type))
```

`async_write_some`是异步写的函数，这个异步写函数有两个参数，第一个参数为`ConstBufferSequence`常引用类型的buffers，第二个参数为`WriteToken`类型，而`WriteToken`在上面定义了,是一个函数对象类型,返回值为void,参数为`error_code`和size_t,所以我们为了调用`async_write_some`函数也要传入一个符合WriteToken定义的函数,就是我们声明的`WriteCallBackErr`函数,前两个参数为`WriteToken`规定的参数，第三个参数为MsgNode的智能指针，这样通过智能指针保证我们发送的Node生命周期延长。我们看一下`WriteToSocketErr`函数的具体实现

```c++
void Session::WriteToSocketErr(const std::string& buf) {
    _send_node = make_shared<MsgNode>(buf.c_str(), buf.length());
    //异步发送数据，因为异步所以不会一下发送完
    this->_socket->async_write_some(asio::buffer(_send_node->_msg, 
        _send_node->_total_len),
        std::bind(&Session::WriteCallBackErr,
            this, std::placeholders::_1, std::placeholders::_2, _send_node));
}
```

因为`WriteCallBackErr`函数为三个参数且为成员函数，而`async_write_some`需要的回调函数为两个参数，所以我们通过bind将三个参数转换为两个参数的普通函数。  
我们看看回调函数的实现
```c++
void Session::WriteCallBackErr(const boost::system::error_code& ec, 
    std::size_t bytes_transferred, std::shared_ptr<MsgNode> msg_node) {
    if (bytes_transferred + msg_node->_cur_len 
        < msg_node->_total_len) {
        _send_node->_cur_len += bytes_transferred;
        this->_socket->async_write_some(asio::buffer(_send_node->_msg+_send_node->_cur_len,
            _send_node->_total_len-_send_node->_cur_len),
            std::bind(&Session::WriteCallBackErr,
                this, std::placeholders::_1, std::placeholders::_2, _send_node));
    }
}
```

在`WriteCallBackErr`函数里判断如果已经发送的字节数没有达到要发送的总字节数，那么久更新节点已经发送的长度，然后计算剩余要发送的长度，如果有数据未发送完，再次调用async_write_some函数异步发送。  
但是这个函数并不能投入实际应用，因为`async_write_some`回调函数返回已发送的字节数可能并不是全部长度。比如TCP发送缓存区总大小为8字节，但是有3字节未发送(上一次未发送完)，这样剩余空间为5字节

<img src="https://cdn.llfc.club/1680692232796.jpg" alt="image.png" style="zoom:60%;" />
此时我们调用`async_write_some`发送hello world!实际发送的长度就是为5，也就是只发送了hello，剩余world!通过我们的回调继续发送。  
而实际开发的场景用户是不清楚底层tcp的多路复用调用情况的，用户想发送数据的时候就调用WriteToSocketErr,或者循环调用WriteToSocketErr，很可能在一次没发送完数据还未调用回调函数时再次调用`WriteToSocketErr`，因为boost::asio封装的时epoll和iocp等多路复用模型，当写事件就绪后就发数据，发送的数据按照async_write_some调用的顺序发送，所以回调函数内调用的async_write_some可能并没有被及时调用。  
比如我们如下代码
```c++
//用户发送数据
WriteToSocketErr("Hello World!");
//用户无感知下层调用情况又一次发送了数据
WriteToSocketErr("Hello World!");
```
那么很可能第一次只发送了Hello，后面的数据没发完，第二次发送了Hello World!之后又发送了World!
所以对端收到的数据很可能是”HelloHello World! World!”
那怎么解决这个问题呢，我们可以通过队列保证应用层的发送顺序。我们在Session中定义一个发送队列，然后重新定义正确的异步发送函数和回调处理
```c++
class Session{
public:
    void WriteCallBack(const boost::system::error_code& ec, std::size_t bytes_transferred);
    void WriteToSocket(const std::string &buf);
private:
    std::queue<std::shared_ptr<MsgNode>> _send_queue;
    std::shared_ptr<asio::ip::tcp::socket> _socket;
    bool _send_pending;
};
```


定义了bool变量`_send_pending`，该变量为true表示一个节点还未发送完。
`_send_queue`用来缓存要发送的消息节点，是一个队列。
我们实现异步发送功能

```c++
void Session::WriteToSocket(const std::string& buf){
    //插入发送队列
    _send_queue.emplace(new MsgNode(buf.c_str(), buf.length()));
    //pending状态说明上一次有未发送完的数据
    if (_send_pending) {
        return;
    }
    //异步发送数据，因为异步所以不会一下发送完
    this->_socket->async_write_some(asio::buffer(buf), std::bind(&Session::WriteCallBack, this, std::placeholders::_1, std::placeholders::_2));
    _send_pending = true;
}
void Session::WriteCallBack(const boost::system::error_code & ec,  std::size_t bytes_transferred){
    if (ec.value() != 0) {
        std::cout << "Error , code is " << ec.value() << " . Message is " << ec.message();
        return;
    }
    //取出队首元素即当前未发送完数据
    auto & send_data = _send_queue.front();
    send_data->_cur_len += bytes_transferred;
    //数据未发送完， 则继续发送
    if (send_data->_cur_len < send_data->_total_len) {
        this->_socket->async_write_some(asio::buffer(send_data->_msg + send_data->_cur_len, send_data->_total_len-send_data->_cur_len),
            std::bind(&Session::WriteCallBack,
            this, std::placeholders::_1, std::placeholders::_2));
        return;
    }
    //如果发送完，则pop出队首元素
    _send_queue.pop();
    //如果队列为空，则说明所有数据都发送完,将pending设置为false
    if (_send_queue.empty()) {
        _send_pending = false;
    }
    //如果队列不是空，则继续将队首元素发送
    if (!_send_queue.empty()) {
        auto& send_data = _send_queue.front();
        this->_socket->async_write_some(asio::buffer(send_data->_msg + send_data->_cur_len, send_data->_total_len - send_data->_cur_len),
            std::bind(&Session::WriteCallBack,
                this, std::placeholders::_1, std::placeholders::_2));
    }
}
```

`async_write_some`函数不能保证每次回调函数触发时发送的长度为要总长度，这样我们每次都要在回调函数判断发送数据是否完成，asio提供了一个更简单的发送函数`async_send`，这个函数在发送的长度未达到我们要求的长度时就不会触发回调，所以触发回调函数时要么时发送出错了要么是发送完成了,其内部的实现原理就是帮我们不断的调用`async_write_some`直到完成发送，所以`async_send`不能和`async_write_some`混合使用，我们基于`async_send`封装另外一个发送函数

```c++
//不能与async_write_some混合使用
void Session::WriteAllToSocket(const std::string& buf) {
    //插入发送队列
    _send_queue.emplace(new MsgNode(buf.c_str(), buf.length()));
    //pending状态说明上一次有未发送完的数据
    if (_send_pending) {
        return;
    }
    //异步发送数据，因为异步所以不会一下发送完
    this->_socket->async_send(asio::buffer(buf), 
        std::bind(&Session::WriteAllCallBack, this,
            std::placeholders::_1, std::placeholders::_2));
    _send_pending = true;
}
void Session::WriteAllCallBack(const boost::system::error_code& ec, std::size_t bytes_transferred){
    if (ec.value() != 0) {
        std::cout << "Error occured! Error code = "
            << ec.value()
            << ". Message: " << ec.message();
        return;
    }
    //如果发送完，则pop出队首元素
    _send_queue.pop();
    //如果队列为空，则说明所有数据都发送完,将pending设置为false
    if (_send_queue.empty()) {
        _send_pending = false;
    }
    //如果队列不是空，则继续将队首元素发送
    if (!_send_queue.empty()) {
        auto& send_data = _send_queue.front();
        this->_socket->async_send(asio::buffer(send_data->_msg + send_data->_cur_len, send_data->_total_len - send_data->_cur_len),
            std::bind(&Session::WriteAllCallBack,
                this, std::placeholders::_1, std::placeholders::_2));
    }
}
```

### 异步读操作

接下来介绍异步读操作，异步读操作和异步的写操作类似同样又async_read_some和async_receive函数，前者触发的回调函数获取的读数据的长度可能会小于要求读取的总长度，后者触发的回调函数读取的数据长度等于读取的总长度。  
先基于`async_read_some`封装一个读取的函数`ReadFromSocket`，同样在Session类的声明中添加一些变量
```c++
class Session {
public:
    void ReadFromSocket();
    void ReadCallBack(const boost::system::error_code& ec, std::size_t bytes_transferred);
private:
    std::shared_ptr<asio::ip::tcp::socket> _socket;
    std::shared_ptr<MsgNode> _recv_node;
    bool _recv_pending;
};
```

`_recv_node`用来缓存接收的数据，`_recv_pending`为true表示节点正在接收数据，还未接受完。

```c++
//不考虑粘包情况， 先用固定的字节接收
void Session::ReadFromSocket() {
    if (_recv_pending) {
        return;
    }
    //可以调用构造函数直接构造，但不可用已经构造好的智能指针赋值
    /*auto _recv_nodez = std::make_unique<MsgNode>(RECVSIZE);
    _recv_node = _recv_nodez;*/
    _recv_node = std::make_shared<MsgNode>(RECVSIZE);
    _socket->async_read_some(asio::buffer(_recv_node->_msg, _recv_node->_total_len), std::bind(&Session::ReadCallBack, this,
        std::placeholders::_1, std::placeholders::_2));
    _recv_pending = true;
}
void Session::ReadCallBack(const boost::system::error_code& ec, std::size_t bytes_transferred){
    _recv_node->_cur_len += bytes_transferred;
    //没读完继续读
    if (_recv_node->_cur_len < _recv_node->_total_len) {
        _socket->async_read_some(asio::buffer(_recv_node->_msg+_recv_node->_cur_len,
            _recv_node->_total_len - _recv_node->_cur_len), std::bind(&Session::ReadCallBack, this,
            std::placeholders::_1, std::placeholders::_2));
        return;
    }
    //将数据投递到队列里交给逻辑线程处理，此处略去
    //如果读完了则将标记置为false
    _recv_pending = false;
    //指针置空
    _recv_node = nullptr;    
}
```

我们基于`async_receive`再封装一个接收数据的函数
```c++
void Session::ReadAllFromSocket(const std::string& buf) {
    if (_recv_pending) {
        return;
    }
    //可以调用构造函数直接构造，但不可用已经构造好的智能指针赋值
    /*auto _recv_nodez = std::make_unique<MsgNode>(RECVSIZE);
    _recv_node = _recv_nodez;*/
    _recv_node = std::make_shared<MsgNode>(RECVSIZE);
    _socket->async_receive(asio::buffer(_recv_node->_msg, _recv_node->_total_len), std::bind(&Session::ReadAllCallBack, this,
        std::placeholders::_1, std::placeholders::_2));
    _recv_pending = true;
}
void Session::ReadAllCallBack(const boost::system::error_code& ec, std::size_t bytes_transferred) {
    _recv_node->_cur_len += bytes_transferred;
    //将数据投递到队列里交给逻辑线程处理，此处略去
    //如果读完了则将标记置为false
    _recv_pending = false;
    //指针置空
    _recv_node = nullptr;
}
```

同样`async_read_some`和`async_receive`不能混合使用，否则会出现逻辑问题。

## 异步读写的服务器示例

### Session类
Session类主要是处理客户端消息收发的会话类，为了简单起见，我们不考虑粘包问题，也不考虑支持手动调用发送的接口，只以应答的方式发送和接收固定长度(1024字节长度)的数据。

```c++
class Session
{
public:
    Session(boost::asio::io_context& ioc):_socket(ioc){
    }
    tcp::socket& Socket() {
        return _socket;
    }
    void Start();
private:
    void handle_read(const boost::system::error_code & error, size_t bytes_transfered);
    void handle_write(const boost::system::error_code& error);
    tcp::socket _socket;
    enum {max_length = 1024};
    char _data[max_length];
};
```

1. `_data`用来接收客户端传递的数据
2. `_socket`为单独处理客户端读写的socket。
3. `handle_read`和`handle_write`分别为读回调函数和写回调函数。
接下来我们实现Session类
```c++
void Session::Start(){
    memset(_data, 0, max_length);
    _socket.async_read_some(boost::asio::buffer(_data, max_length),
        std::bind(&Session::handle_read, this, placeholders::_1,
            placeholders::_2)
    );
}
```

在Start方法中我们调用异步读操作，监听对端发送的消息。当对端发送数据后，触发`handle_read`函数
```c++
void Session::handle_read(const boost::system::error_code& error, size_t bytes_transfered) {
    if (!error) {
        cout << "server receive data is " << _data << endl;
        boost::asio::async_write(_socket, boost::asio::buffer(_data, bytes_transfered), 
            std::bind(&Session::handle_write, this, placeholders::_1));
    }
    else {
        delete this;
    }
}
```

`handle_read`函数内将收到的数据发送给对端，当发送完成后触发`handle_write`回调函数。

```c++
void Session::handle_write(const boost::system::error_code& error) {
    if (!error) {
        memset(_data, 0, max_length);
        _socket.async_read_some(boost::asio::buffer(_data, max_length), std::bind(&Session::handle_read,
            this, placeholders::_1, placeholders::_2));
    }
    else {
        delete this;
    }
}
```

`handle_write`函数内又一次监听了读事件，如果对端有数据发送过来则触发handle_read，我们再将收到的数据发回去。从而达到应答式服务的效果。

### Server类

Server类为服务器接收连接的管理类
```c++
class Server {
public:
    Server(boost::asio::io_context& ioc, short port);
private:
    void start_accept();
    void handle_accept(Session* new_session, const boost::system::error_code& error);
    boost::asio::io_context& _ioc;
    tcp::acceptor _acceptor;
};
```

`start_accept`将要接收连接的`acceptor`绑定到服务上，其内部就是将`accpeptor`对应的socket描述符绑定到`epoll`或`iocp`模型上，实现事件驱动。  
`handle_accept`为新连接到来后触发的回调函数。  
下面是具体实现
```c++
Server::Server(boost::asio::io_context& ioc, short port) :_ioc(ioc),
_acceptor(ioc, tcp::endpoint(tcp::v4(), port)) {
    start_accept();
}
void Server::start_accept() {
    Session* new_session = new Session(_ioc);
    _acceptor.async_accept(new_session->Socket(),
        std::bind(&Server::handle_accept, this, new_session, placeholders::_1));
}
void Server::handle_accept(Session* new_session, const boost::system::error_code& error) {
    if (!error) {
        new_session->Start();
    }
    else {
        delete new_session;
    }
    start_accept();
}
```


## 模拟伪闭包实现连接的安全回收

之前的异步服务器为echo模式，但其存在安全隐患，就是在极端情况下客户端关闭导致触发写和读回调函数，二者都进入错误处理逻辑，进而造成二次析构的问题。  
下面我们介绍通过C11智能指针构造成一个伪闭包的状态延长session的生命周期。

### 智能指针管理Session

我们可以通过智能指针的方式管理`Session`类，将`acceptor`接收的链接保存在`Session`类型的智能指针里。由于智能指针会在引用计数为0时自动析构，所以为了防止其被自动回收，也方便`Server`管理`Session`，因为我们后期会做一些重连踢人等业务逻辑，我们在`Server`类中添加成员变量，该变量为一个map类型，`key`为`Session`的uid，value为该`Session`的智能指针。

```c++
class CServer
{
public:
    CServer(boost::asio::io_context& io_context, short port);
    void ClearSession(std::string);
private:
    void HandleAccept(shared_ptr<CSession>, const boost::system::error_code & error);
    void StartAccept();
    boost::asio::io_context &_io_context;
    short _port;
    tcp::acceptor _acceptor;
    std::map<std::string, shared_ptr<CSession>> _sessions;
};
```

通过Server中的`_sessions`这个map管理链接，可以增加Session智能指针的引用计数，只有当Session从这个map中移除后，Session才会被释放。  
所以在接收连接的逻辑里将Session放入map

```c++
void CServer::StartAccept() {
    shared_ptr<CSession> new_session = make_shared<CSession>(_io_context, this);
    _acceptor.async_accept(new_session->GetSocket(), std::bind(&CServer::HandleAccept, this, new_session, placeholders::_1));
}
void CServer::HandleAccept(shared_ptr<CSession> new_session, const boost::system::error_code& error){
    if (!error) {
        new_session->Start();
        _sessions.insert(make_pair(new_session->GetUuid(), new_session));
    }
    else {
        cout << "session accept failed, error is " << error.what() << endl;
    }
    StartAccept();
}
```

`StartAccept`函数中虽然new_session是一个局部变量，但是我们通过bind操作，将`new_session`作为数值传递给bind函数，而bind函数返回的函数对象内部引用了该`new_session`所以引用计数增加1，这样保证了new_session不会被释放。  
在`HandleAccept`函数里调用`session`的start函数监听对端收发数据，并将session放入map中，保证session不被自动释放。  
此外，需要封装一个释放函数，将`session`从map中移除，当其引用计数为0则自动释放
```c++
void CServer::ClearSession(std::string uuid) {
    _sessions.erase(uuid);
}
```

### Session的uuid

关于session的uuid可以通过boost提供的生成唯一id的函数获得，当然你也可以自己实现[[MyBatis-plus#雪花算法|雪花算法]]。
```c++
CSession::CSession(boost::asio::io_context& io_context, CServer* server):
    _socket(io_context), _server(server){
    boost::uuids::uuid  a_uuid = boost::uuids::random_generator()();
    _uuid = boost::uuids::to_string(a_uuid);
}
```

另外我们修改Session中读写回调函数关于错误的处理，当读写出错的时候清除连接

```c++
void CSession::HandleWrite(const boost::system::error_code& error) {
    if (!error) {
        std::lock_guard<std::mutex> lock(_send_lock);
        _send_que.pop();
        if (!_send_que.empty()) {
            auto &msgnode = _send_que.front();
            boost::asio::async_write(_socket, boost::asio::buffer(msgnode->_data, msgnode->_max_len),
                std::bind(&CSession::HandleWrite, this, std::placeholders::_1));
        }
    }
    else {
        std::cout << "handle write failed, error is " << error.what() << endl;
        _server->ClearSession(_uuid);
    }
}
void CSession::HandleRead(const boost::system::error_code& error, size_t  bytes_transferred){
    if (!error) {
        cout << "read data is " << _data << endl;
        //发送数据
        Send(_data, bytes_transferred);
        memset(_data, 0, MAX_LENGTH);
        _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH), std::bind(&CSession::HandleRead, this, std::placeholders::_1, std::placeholders::_2));
    }
    else {
        std::cout << "handle read failed, error is " << error.what() << endl;
        _server->ClearSession(_uuid);
    }
}
```

### 隐患

正常情况下上述服务器运行不会出现问题，但是当我们像上次一样模拟，在服务器要发送数据前打个断点，此时关闭客户端，在服务器就会先触发写回调函数的错误处理，再触发读回调函数的错误处理，这样session就会两次从map中移除，因为map中key唯一，所以第二次map判断没有session的key就不做移除操作了。  
但是这么做还是会有崩溃问题，因为第一次在session写回调函数中移除session，session的引用计数就为0了，调用了session的析构函数，这样在触发session读回调函数时此时session的内存已经被回收了自然会出现崩溃的问题。解决这个问题可以利用智能指针引用计数和bind的特性，实现一个伪闭包的机制延长session的生命周期。


### 如何构造伪闭包

思路：  
- 利用智能指针被复制或使用引用计数加一的原理保证内存不被回收  
- bind操作可以将值绑定在一个函数对象上生成新的函数对象，如果将智能指针作为参数绑定给函数对象，那么智能指针就以值的方式被新函数对象使用，那么智能指针的生命周期将和新生成的函数对象一致，从而达到延长生命的效果。  
我们按照上面的思路改写我们的回调函数

```c++
void HandleRead(const boost::system::error_code& error, 
size_t  bytes_transferred, shared_ptr<CSession> _self_shared);
void HandleWrite(const boost::system::error_code& error, shared_ptr<CSession> _self_shared);
```

以`HandleWrite`举例,在bind时传递`_self_shared`指针增加其引用计数，这样`_self_shared`的生命周期就和`async_write`的第二个参数(也就是asio要求的回调函数对象)生命周期一致了。

```c++
void CSession::HandleWrite(const boost::system::error_code& error, shared_ptr<CSession> _self_shared) {
    if (!error) {
        std::lock_guard<std::mutex> lock(_send_lock);
        _send_que.pop();
        if (!_send_que.empty()) {
            auto &msgnode = _send_que.front();
            boost::asio::async_write(_socket, boost::asio::buffer(msgnode->_data, msgnode->_max_len),
                std::bind(&CSession::HandleWrite, this, std::placeholders::_1, _self_shared));
        }
    }
    else {
        std::cout << "handle write failed, error is " << error.what() << endl;
        _server->ClearSession(_uuid);
    }
}
```

同样道理HandleRead内部也实现了类似的绑定

```c++
void CSession::HandleRead(const boost::system::error_code& error, size_t  bytes_transferred, shared_ptr<CSession> _self_shared){
    if (!error) {
        cout << "read data is " << _data << endl;
        //发送数据
        Send(_data, bytes_transferred);
        memset(_data, 0, MAX_LENGTH);
        _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH), std::bind(&CSession::HandleRead, this, 
            std::placeholders::_1, std::placeholders::_2, _self_shared));
    }
    else {
        std::cout << "handle read failed, error is " << error.what() << endl;
        _server->ClearSession(_uuid);
    }
}
```

除此之外，我们也要在第一次绑定读写回调函数的时候传入智能指针的值,但是要注意传入的方式，不能用两个智能指针管理同一块内存，如下用法是错误的。
```c++
void CSession::Start(){
    memset(_data, 0, MAX_LENGTH);
    _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH), std::bind(&CSession::HandleRead, this, 
        std::placeholders::_1, std::placeholders::_2, shared_ptr<CSession>(this)));
}
```

`shared_ptr<CSession>(this)`生成的新智能指针和this之前绑定的智能指针并不共享引用计数，所以要通过`shared_from_this()`函数返回智能指针，该智能指针和其他管理这块内存的智能指针共享引用计数。

```c++
void CSession::Start(){
    memset(_data, 0, MAX_LENGTH);
    _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH), std::bind(&CSession::HandleRead, this, 
        std::placeholders::_1, std::placeholders::_2, shared_from_this()));
}
```

`shared_from_this()`函数并不是session的成员函数，要使用这个函数需要继承`std::enable_shared_from_this<Session>`

```c++
class CSession:public std::enable_shared_from_this<CSession>
{
public:
    CSession(boost::asio::io_context& io_context, CServer* server);
    tcp::socket& GetSocket();
    std::string& GetUuid();
    void Start();
    void Send(char* msg,  int max_length);
private:
    void HandleRead(const boost::system::error_code& error, size_t  bytes_transferred, shared_ptr<CSession> _self_shared);
    void HandleWrite(const boost::system::error_code& error, shared_ptr<CSession> _self_shared);
    tcp::socket _socket;
    std::string _uuid;
    char _data[MAX_LENGTH];
    CServer* _server;
    std::queue<shared_ptr<MsgNode> > _send_que;
    std::mutex _send_lock;
};
```

同样的道理，我们在发送的时候也要绑定智能指针作为参数, 这里不做赘述。  
再次测试，链接可以安全释放，并不存在二次释放的问题。可以在析构函数内打印析构的信息，发现只析构一次

<img src="https://cdn.llfc.club/20230410154807.png" alt="image.png" style="zoom:60%;" />

## 封装服务器发送队列

前面介绍了通过智能指针实现伪闭包的方式延长了`session`的生命周期，而实际使用的服务器并不是应答式，而是全双工通信方式，服务器一直监听写事件，接收对端数据，可随时发送数据给对端，今天介绍如何封装异步的发送接口，因为多次发送时，异步的发送要保证回调触发后再次发送才能确保数据是有序的，这一点我们已经在前文异步发送函数介绍的时候提到了。

### Server和Session分离

将Server修改为CServer并分离到CServer.h中，然后将`Session`修改为`CSession`分离到`CSession.h`中。  
`CSession.h`中类的声明如下，和之前的`Session`内容一样，就是修改了类名，放在`CSession.h`中

```c++
#include <iostream>
#include <boost/asio.hpp>
#include <map>
#include <boost/uuid/uuid_generators.hpp>
#include <boost/uuid/uuid_io.hpp>
using boost::asio::ip::tcp;
using namespace std;
class CServer;
class CSession :public std::enable_shared_from_this<CSession>
{
public:
    CSession(boost::asio::io_context& ioc, CServer* server) :_socket(ioc), _server(server) {
        boost::uuids::uuid  a_uuid = boost::uuids::random_generator()();
        _uuid = boost::uuids::to_string(a_uuid);
    }
    tcp::socket& Socket() {
        return _socket;
    }
    ~CSession() {
        std::cout << "session destruct delete this " << this << endl;
    }
    void Start();
    std::string& GetUuid();
private:
    void handle_read(const boost::system::error_code& error,
        size_t bytes_transferred, shared_ptr<CSession> _self_shared);
    void handle_write(const boost::system::error_code& error, shared_ptr<CSession> _self_shared);
    tcp::socket _socket;
    enum { max_length = 1024 };
    char _data[max_length];
    CServer* _server;
    std::string _uuid;
};
```

`CServer.h`中声明如下，内容前文没变化，就是将`Server`内容写入`CServer.h`中

```c++
#include <boost/asio.hpp>
#include "CSession.h"
#include <memory.h>
#include <map>
using namespace std;
using boost::asio::ip::tcp;
class CServer
{
public:
    CServer(boost::asio::io_context& io_context, short port);
    void ClearSession(std::string);
private:
    void HandleAccept(shared_ptr<CSession>, const boost::system::error_code & error);
    void StartAccept();
    boost::asio::io_context &_io_context;
    short _port;
    tcp::acceptor _acceptor;
    std::map<std::string, shared_ptr<CSession>> _sessions;
};
```

整体目录变为

<img src="https://cdn.llfc.club/1682157973797.jpg" alt="image.png" style="zoom:80%;" />

### 数据节点设计

我们设计一个数据节点MsgNode用来存储数据

```c++
class MsgNode
{
    friend class CSession;
public:
    MsgNode(char * msg, int max_len) {
        _data = new char[max_len];
        memcpy(_data, msg, max_len);
    }
    ~MsgNode() {
        delete[] _data;
    }
private:
    int _cur_len;
    int _max_len;
    char* _data;
};
```

- `_cur_len`表示数据当前已处理的长度(已经发送的数据或者已经接收的数据长度)，因为一个数据包存在未发送完或者未接收完的情况。  
- `_max_len`表示数据的总长度。  
- `_data`表示数据域，已接收或者已发送的数据都放在此空间内。

### 封装发送接口

首先在`CSession`类里新增一个队列存储要发送的数据，因为我们不能保证每次调用发送接口的时候上一次数据已经发送完，就要把要发送的数据放入队列中，通过回调函数不断地发送。而且我们不能保证发送的接口和回调函数的接口在一个线程，所以要增加一个锁保证发送队列安全性。  
同时我们新增一个发送接口`Send`

```c++
void Send(char* msg,  int max_length);
std::queue<shared_ptr<MsgNode> > _send_que;
std::mutex _send_lock;
```

实现发送接口

```c++
void CSession::Send(char* msg, int max_length) {
    bool pending = false;
    std::lock_guard<std::mutex> lock(_send_lock);
    if (_send_que.size() > 0) {
        pending = true;
    }
    _send_que.push(make_shared<MsgNode>(msg, max_length));
    if (pending) {
        return;
    }
    boost::asio::async_write(_socket, boost::asio::buffer(msg, max_length), 
        std::bind(&CSession::HandleWrite, this, std::placeholders::_1, shared_from_this()));
}
```

发送接口里判断发送队列是否为空，如果不为空说明有数据未发送完，需要将数据放入队列，然后返回。如果发送队列为空，则说明当前没有未发送完的数据，将要发送的数据放入队列并调用`async_write`函数发送数据。  
回调函数实现

```c++
void CSession::HandleWrite(const boost::system::error_code& error, shared_ptr<CSession> _self_shared) {
    if (!error) {
        std::lock_guard<std::mutex> lock(_send_lock);
        _send_que.pop();
        if (!_send_que.empty()) {
            auto &msgnode = _send_que.front();
            boost::asio::async_write(_socket, boost::asio::buffer(msgnode->_data, msgnode->_max_len),
                std::bind(&CSession::HandleWrite, this, std::placeholders::_1, _self_shared));
        }
    }
    else {
        std::cout << "handle write failed, error is " << error.what() << endl;
        _server->ClearSession(_uuid);
    }
}
```

判断发送队列是否为空，为空则发送完，否则不断取出队列数据调用async_write发送，直到队列为空。

### 修改读回调

因为我们要一直监听对端发送的数据，所以要在每次收到数据后继续绑定监听事件


```c++
void CSession::HandleRead(const boost::system::error_code& error, size_t  bytes_transferred, shared_ptr<CSession> _self_shared){
    if (!error) {
        cout << "read data is " << _data << endl;
        //发送数据
        Send(_data, bytes_transferred);
        memset(_data, 0, MAX_LENGTH);
        _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH), std::bind(&CSession::HandleRead, this, 
            std::placeholders::_1, std::placeholders::_2, _self_shared));
    }
    else {
        std::cout << "handle read failed, error is " << error.what() << endl;
        _server->ClearSession(_uuid);
    }
}
```



## 处理网络粘包问题

粘包问题看[[网络编程#什么是粘包？|这里]]


### 完善消息节点

之前我们设计过消息节点的数据结构MsgNode，这里需要完善一下.

```c++
class MsgNode
{
    friend class CSession;
public:
    MsgNode(char * msg, short max_len):_total_len(max_len + HEAD_LENGTH),_cur_len(0){
        _data = new char[_total_len+1]();
        memcpy(_data, &max_len, HEAD_LENGTH);
        memcpy(_data+ HEAD_LENGTH, msg, max_len);
        _data[_total_len] = '\0';
    }
    MsgNode(short max_len):_total_len(max_len),_cur_len(0) {
        _data = new char[_total_len +1]();
    }
    ~MsgNode() {
        delete[] _data;
    }
    void Clear() {
        ::memset(_data, 0, _total_len);
        _cur_len = 0;
    }
private:
    short _cur_len;
    short _total_len;
    char* _data;
};
```

1. 两个参数的构造函数做了完善，之前的构造函数通过消息首地址和长度构造节点数据，现在需要在构造节点的同时把长度信息也写入节点,该构造函数主要用来发送数据时构造发送信息的节点。
2. 一个参数的构造函数为较上次新增的，主要根据消息的长度构造消息节点，该构造函数主要是接收对端数据时构造接收节点调用的。
3. 新增一个Clear函数清除消息节点的数据，主要是避免多次构造节点造成开销。

### CSession类完善

为能够对收到的数据切包处理，需要定义一个消息接收节点，一个bool类型的变量表示头部是否解析完成，以及将处理好的头部先缓存起来的结构。
```c++
    //收到的消息结构
    std::shared_ptr<MsgNode> _recv_msg_node;
    bool _b_head_parse;
    //收到的头部结构
    std::shared_ptr<MsgNode> _recv_head_node;
```

- `_recv_msg_node`用来存储接受的消息体信息
- `_recv_head_node`用来存储接收的头部信息
- `_b_head_parse`表示是否处理完头部信息

同时我们新增一个`HEAD_LENGTH`变量表示数据包头部的大小，修改原消息最大长度为`1024*2`

```c++
#define MAX_LENGTH  1024*2
#define HEAD_LENGTH 2
```

### 完善接收逻辑
我们需要修改HandleRead函数

```c++
void CSession::HandleRead(const boost::system::error_code& error, size_t  bytes_transferred, std::shared_ptr<CSession> shared_self){
    if (!error) {
        //已经移动的字符数
        int copy_len = 0;
        while (bytes_transferred>0) {
            if (!_b_head_parse) {
                //收到的数据不足头部大小
                if (bytes_transferred + _recv_head_node->_cur_len < HEAD_LENGTH) {
                    memcpy(_recv_head_node->_data + _recv_head_node->_cur_len, _data+ copy_len, bytes_transferred);
                    _recv_head_node->_cur_len += bytes_transferred;
                    ::memset(_data, 0, MAX_LENGTH);
                    _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH), 
                        std::bind(&CSession::HandleRead, this, std::placeholders::_1, std::placeholders::_2, shared_self));
                    return;
                }
                //收到的数据比头部多
                //头部剩余未复制的长度
                int head_remain = HEAD_LENGTH - _recv_head_node->_cur_len;
                memcpy(_recv_head_node->_data + _recv_head_node->_cur_len, _data+copy_len, head_remain);
                //更新已处理的data长度和剩余未处理的长度
                copy_len += head_remain;
                bytes_transferred -= head_remain;
                //获取头部数据
                short data_len = 0;
                memcpy(&data_len, _recv_head_node->_data, HEAD_LENGTH);
                cout << "data_len is " << data_len << endl;
                //头部长度非法
                if (data_len > MAX_LENGTH) {
                    std::cout << "invalid data length is " << data_len << endl;
                    _server->ClearSession(_uuid);
                    return;
                }
                _recv_msg_node = make_shared<MsgNode>(data_len);
                //消息的长度小于头部规定的长度，说明数据未收全，则先将部分消息放到接收节点里
                if (bytes_transferred < data_len) {
                    memcpy(_recv_msg_node->_data + _recv_msg_node->_cur_len, _data + copy_len, bytes_transferred);
                    _recv_msg_node->_cur_len += bytes_transferred;
                    ::memset(_data, 0, MAX_LENGTH);
                    _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH), 
                        std::bind(&CSession::HandleRead, this, std::placeholders::_1, std::placeholders::_2, shared_self));
                    //头部处理完成
                    _b_head_parse = true;
                    return;
                }
                memcpy(_recv_msg_node->_data + _recv_msg_node->_cur_len, _data + copy_len, data_len);
                _recv_msg_node->_cur_len += data_len;
                copy_len += data_len;
                bytes_transferred -= data_len;
                _recv_msg_node->_data[_recv_msg_node->_total_len] = '\0';
                cout << "receive data is " << _recv_msg_node->_data << endl;
                //此处可以调用Send发送测试
                Send(_recv_msg_node->_data, _recv_msg_node->_total_len);
                //继续轮询剩余未处理数据
                _b_head_parse = false;
                _recv_head_node->Clear();
                if (bytes_transferred <= 0) {
                    ::memset(_data, 0, MAX_LENGTH);
                    _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH), 
                        std::bind(&CSession::HandleRead, this, std::placeholders::_1, std::placeholders::_2, shared_self));
                    return;
                }
                continue;
            }
            //已经处理完头部，处理上次未接受完的消息数据
            //接收的数据仍不足剩余未处理的
            int remain_msg = _recv_msg_node->_total_len - _recv_msg_node->_cur_len;
            if (bytes_transferred < remain_msg) {
                memcpy(_recv_msg_node->_data + _recv_msg_node->_cur_len, _data + copy_len, bytes_transferred);
                _recv_msg_node->_cur_len += bytes_transferred;
                ::memset(_data, 0, MAX_LENGTH);
                _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH), 
                    std::bind(&CSession::HandleRead, this, std::placeholders::_1, std::placeholders::_2, shared_self));
                return;
            }
            memcpy(_recv_msg_node->_data + _recv_msg_node->_cur_len, _data + copy_len, remain_msg);
            _recv_msg_node->_cur_len += remain_msg;
            bytes_transferred -= remain_msg;
            copy_len += remain_msg;
            _recv_msg_node->_data[_recv_msg_node->_total_len] = '\0';
            cout << "receive data is " << _recv_msg_node->_data << endl;
            //此处可以调用Send发送测试
            Send(_recv_msg_node->_data, _recv_msg_node->_total_len);
            //继续轮询剩余未处理数据
            _b_head_parse = false;
            _recv_head_node->Clear();
            if (bytes_transferred <= 0) {
                ::memset(_data, 0, MAX_LENGTH);
                _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH),
                    std::bind(&CSession::HandleRead, this, std::placeholders::_1, std::placeholders::_2, shared_self));
                return;
            }
            continue;
        }
    }
    else {
        std::cout << "handle read failed, error is " << error.what() << endl;
        Close();
        _server->ClearSession(_uuid);
    }
}
```

1. `copy_len`记录的是已经处理过数据的长度，因为存在一次接收多个包的情况，所以`copy_len`用来做已经处理的数据长度的。  
2. 首先判断`_b_head_parse`是否为false，如果为false则说明头部未处理，先判断接收的数据是否小于头部, 如果小于头部大小则将接收到的数据放入`_recv_head_node`节点保存，然后继续调用读取函数监听对端发送数据。否则进入步骤3.  
3. 如果收到的数据比头部多，可能是多个逻辑包，所以要做切包处理。根据之前保留在`_recv_head_node`的长度，计算出剩余未取出的头部长度，然后取出剩余的头部长度保存在`_recv_head_node`节点，然后通过memcpy方式从节点拷贝出数据写入short类型的data_len里，进而获取消息的长度。接下来继续处理包体，也就是消息体，判断接收到的数据未处理部分的长度和总共要接收的数据长度大小，如果小于总共要接受的长度，说明消息体没接收完，则将未处理部分先写入_recv_msg_node里，并且继续监听读事件。否则说明消息体接收完全，进入步骤4  
4. 将消息体数据接收到`_recv_msg_node`中，接受完全后返回给对端。当然存在多个逻辑包粘连，此时要判断`bytes_transferred`是否小于等于0，如果是说明只有一个逻辑包，我们处理完了，继续监听读事件，就直接返回即可。否则说明有多个数据包粘连，就继续执行上述操作。
5. 因为存在`_b_head_parse`为true，也就是包头接收并处理完的情况，但是包体未接受完，再次触发`HandleRead`，此时要继续处理上次未接受完的消息体，大体逻辑和3，4一样。  
6. 以上就是处理粘包的过程，我们绘制流程图更明了一些

<img src="https://cdn.llfc.club/1683373951566.jpg" alt="image.png" style="zoom:70%;" />

### 客户端修改

客户端的发送也要遵循先发送数据2个字节的数据长度，再发送数据消息的结构。  
接收时也是先接收两个字节数据获取数据长度，再根据长度接收消息。

```c++
int main()
{
    try {
        //创建上下文服务
        boost::asio::io_context   ioc;
        //构造endpoint
        tcp::endpoint  remote_ep(address::from_string("127.0.0.1"), 10086);
        tcp::socket  sock(ioc);
        boost::system::error_code   error = boost::asio::error::host_not_found; ;
        sock.connect(remote_ep, error);
        if (error) {
            cout << "connect failed, code is " << error.value() << " error msg is " << error.message();
            return 0;
        }
        std::cout << "Enter message: ";
        char request[MAX_LENGTH];
        std::cin.getline(request, MAX_LENGTH);
        size_t request_length = strlen(request);
        char send_data[MAX_LENGTH] = { 0 };
        memcpy(send_data, &request_length, 2);
        memcpy(send_data + 2, request, request_length);
        boost::asio::write(sock, boost::asio::buffer(send_data, request_length+2));
        char reply_head[HEAD_LENGTH];
        size_t reply_length = boost::asio::read(sock,boost::asio::buffer(reply_head, HEAD_LENGTH));
        short msglen = 0;
        memcpy(&msglen, reply_head, HEAD_LENGTH);
        char msg[MAX_LENGTH] = { 0 };
        size_t  msg_length = boost::asio::read(sock,boost::asio::buffer(msg, msglen));
        std::cout << "Reply is: ";
        std::cout.write(msg, msglen) << endl;
        std::cout << "Reply len is " << msglen;
        std::cout << "\n";
    }
    catch (std::exception& e) {
        std::cerr << "Exception: " << e.what() << endl;
    }
    return 0;
}
```

服务器启动后，启动客户端，然后客户端发送Hello World，服务器收到后打印如下

<img src="https://cdn.llfc.club/1683458311421.jpg" alt="image.png" style="zoom:60%;" />
### 粘包测试
为了测试粘包，需要制造粘包产生的现象，可以让客户端发送的频率高一些，服务器接收的频率低一些，这样造成前后端收发数据不一致导致多个数据包在服务器tcp缓冲区滞留产生粘包现象。
测试粘包之前，在服务器的CSession类里添加打印二进制数据的函数，便于查看缓冲区的数据

```c++
void CSession::PrintRecvData(char* data, int length) {
    stringstream ss;
    string result = "0x";
    for (int i = 0; i < length; i++) {
        string hexstr;
        ss << hex << std::setw(2) << std::setfill('0') << int(data[i]) << endl;
        ss >> hexstr;
        result += hexstr;
    }
    std::cout << "receive raw data is : " << result << endl;;
}
```

然后将这个函数放到HandleRead里，每次收到数据就调用这个函数打印接收到的最原始的数据，然后睡眠2秒再进行收发操作，用来延迟接收对端数据制造粘包，之后的逻辑不变

```c++
void CSession::HandleRead(const boost::system::error_code& error, size_t  bytes_transferred, std::shared_ptr<CSession> shared_self){
    if (!error) {
        PrintRecvData(_data, bytes_transferred);
        std::chrono::milliseconds dura(2000);
        std::this_thread::sleep_for(dura);
    }
}
```

修改客户端逻辑，实现收发分离。

```c++
int main()
{
    try {
        //创建上下文服务
        boost::asio::io_context   ioc;
        //构造endpoint
        tcp::endpoint  remote_ep(address::from_string("127.0.0.1"), 10086);
        tcp::socket  sock(ioc);
        boost::system::error_code   error = boost::asio::error::host_not_found; ;
        sock.connect(remote_ep, error);
        if (error) {
            cout << "connect failed, code is " << error.value() << " error msg is " << error.message();
            return 0;
        }
        thread send_thread([&sock] {
            for (;;) {
                this_thread::sleep_for(std::chrono::milliseconds(2));
                const char* request = "hello world!";
                size_t request_length = strlen(request);
                char send_data[MAX_LENGTH] = { 0 };
                memcpy(send_data, &request_length, 2);
                memcpy(send_data + 2, request, request_length);
                boost::asio::write(sock, boost::asio::buffer(send_data, request_length + 2));
            }
            });
        thread recv_thread([&sock] {
            for (;;) {
                this_thread::sleep_for(std::chrono::milliseconds(2));
                cout << "begin to receive..." << endl;
                char reply_head[HEAD_LENGTH];
                size_t reply_length = boost::asio::read(sock, boost::asio::buffer(reply_head, HEAD_LENGTH));
                short msglen = 0;
                memcpy(&msglen, reply_head, HEAD_LENGTH);
                char msg[MAX_LENGTH] = { 0 };
                size_t  msg_length = boost::asio::read(sock, boost::asio::buffer(msg, msglen));
                std::cout << "Reply is: ";
                std::cout.write(msg, msglen) << endl;
                std::cout << "Reply len is " << msglen;
                std::cout << "\n";
            }
            });
        send_thread.join();
        recv_thread.join();
    }
    catch (std::exception& e) {
        std::cerr << "Exception: " << e.what() << endl;
    }
    return 0;
}
```

再次启动服务器和客户端，看到粘包现象了，我们的服务器也能稳定切割数据包并返回正确的消息给客户端。  
可以看到服务器收到了大量数据，然后准确切割返回给了客户端。如下图。

<img src="https://cdn.llfc.club/1683460029221.jpg" alt="image.png" style="zoom:60%;" />

#### 总结

该服务虽然实现了粘包处理，但是服务器仍存在不足，比如当客户端和服务器处于不同平台时收发数据会出现异常，根本原因是未处理大小端模式的问题，这个留给下节处理。
## 字节序处理和消息队列的控制

字节序的问题看[[网络编程#字节序|这里]]

### 服务器使用网络字节序
为保证字节序一致性，网络传输使用网络字节序，也就是大端模式。
在 `boost::asio` 库中，可以使用 `boost::asio::detail::socket_ops::host_to_network_long()` 和`boost::asio::detail::socket_ops::host_to_network_short()` 函数将主机字节序转换为网络字节序。具体方法如下：

```c++
#include <boost/asio.hpp>
#include <iostream>
int main()
{
    uint32_t host_long_value = 0x12345678;
    uint16_t host_short_value = 0x5678;
    uint32_t network_long_value = boost::asio::detail::socket_ops::host_to_network_long(host_long_value);
    uint16_t network_short_value = boost::asio::detail::socket_ops::host_to_network_short(host_short_value);
    std::cout << "Host long value: 0x" << std::hex << host_long_value << std::endl;
    std::cout << "Network long value: 0x" << std::hex << network_long_value << std::endl;
    std::cout << "Host short value: 0x" << std::hex << host_short_value << std::endl;
    std::cout << "Network short value: 0x" << std::hex << network_short_value << std::endl;
    return 0;
}
```

上述代码中，使用了 `boost::asio::detail::socket_ops::host_to_network_long()` 和 `boost::asio::detail::socket_ops::host_to_network_short()` 函数将主机字节序转换为网络字节序。

`host_to_network_long()` 函数将一个 32 位无符号整数从主机字节序转换为网络字节序，返回转换后的结果。`host_to_network_short()` 函数将一个 16 位无符号整数从主机字节序转换为网络字节序，返回转换后的结果。

在上述代码中，分别将 32 位和 16 位的主机字节序数值转换为网络字节序，并输出转换结果。需要注意的是，在使用这些函数时，应该确保输入参数和返回结果都是无符号整数类型，否则可能会出现错误。  
同样的道理，我们只需要在服务器发送数据时，将数据长度转化为网络字节序，在接收数据时，将长度转为本机字节序。  
在服务器的HandleRead函数里，添加对data_len的转换，将网络字节转为本地字节序。

```c++
short data_len = 0;
memcpy(&data_len, _recv_head_node->_data, HEAD_LENGTH);
//网络字节序转化为本地字节序
data_len=boost::asio::detail::socket_ops::network_to_host_short(data_len);
cout << "data_len is " << data_len << endl;
```

在服务器的发送数据时会构造消息节点，构造消息节点时，将发送长度由本地字节序转化为网络字节序

```c++
    MsgNode(char * msg, short max_len):_total_len(max_len + HEAD_LENGTH),_cur_len(0){
        _data = new char[_total_len+1]();
        //转为网络字节序
        int max_len_host = boost::asio::detail::socket_ops::host_to_network_short(max_len);
        memcpy(_data, &max_len_host, HEAD_LENGTH);
        memcpy(_data+ HEAD_LENGTH, msg, max_len);
        _data[_total_len] = '\0';
    }
```

客户端也遵循同样的处理。

### 消息队列控制
发送时我们会将发送的消息放入队列里以保证发送的时序性，每个session都有一个发送队列，因为有的时候发送的频率过高会导致队列增大，所以要对队列的大小做限制，当队列大于指定数量的长度时，就丢弃要发送的数据包，以保证消息的快速收发。

```c++
void CSession::Send(char* msg, int max_length) {
    std::lock_guard<std::mutex> lock(_send_lock);
    int send_que_size = _send_que.size();
    if (send_que_size > MAX_SENDQUE) {
        cout << "session: " << _uuid << " send que fulled, size is " << MAX_SENDQUE << endl;
        return;
    }
    _send_que.push(make_shared<MsgNode>(msg, max_length));
    if (send_que_size>0) {
        return;
    }
    auto& msgnode = _send_que.front();
    boost::asio::async_write(_socket, boost::asio::buffer(msgnode->_data, msgnode->_total_len), 
        std::bind(&CSession::HandleWrite, this, std::placeholders::_1, SharedSelf()));
}
```


## protobuf使用

protocol buffers是一种语言无关、平台无关、可扩展的**序列化结构数据**的方法，它可用于（数据）通信协议、数据存储等。
Protocol Buffers是一种灵活，高效，**自动化机制的结构数据序列化方法**－可类比 XML，但是比 XML 更小（3 ~ 10倍）、更快（20 ~ 100倍）、更为简单。
`json\xml`都是基于文本格式，protobuf是**二进制格式**。


protobuf将原始文件中定义的架构，转换为所有流行编程语言的数据访问类
你可以通过 ProtoBuf 定义数据结构，然后通过ProtoBuf工具生成各种语言版本的数据结构类库，用于操作 ProtoBuf 协议数据

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923104005.png" alt="image.png" style="zoom:40%;" />


### 使用ProtoBuf的例子

#### **创建 .proto 文件，定义数据结构**

使用ProtoBuf,首先需要通过ProtoBuf语法定义数据结构(消息)，这些定义好的数据结构保存在.proto为后缀的文件中。

例子:
文件名: response.proto
```proto
// 指定protobuf的版本，proto3是最新的语法版本
syntax = "proto3";

// 定义数据结构，message 你可以想象成java的class，c语言中的struct
message Response {
  string data = 1;   // 定义一个string类型的字段，字段名字为data, 序号为1
  int32 status = 2;   // 定义一个int32类型的字段，字段名字为status, 序号为2
}
```

> 说明：proto文件中，字段后面的序号，不能重复，定义了就不能修改，可以理解成字段的唯一ID。

### protobuf结构

1. message:
   - 类似于 Java 的 class 或 C 的 struct
   - 定义数据结构
   - 例子：
     ```protobuf
     message Person {
       string name = 1;
       int32 age = 2;
     }
     ```

2. service:
   - 类似于 Java 的 interface 或 C++ 的抽象类
   - 定义一组可以远程调用的方法
   - 不包含实现，只是声明
   - 例子：
     ```protobuf
     service UserService {
       rpc GetUser(GetUserRequest) returns (User);
       rpc CreateUser(CreateUserRequest) returns (User);
     }
     ```

3. rpc (Remote Procedure Call):
   - 类似于 Java interface 中的方法声明或 C++ 中的虚函数
   - 定义在 service 内部
   - 指定输入参数和返回类型
   - 代表可以远程调用的单个方法

4. 比较：

| Proto   | Java               | C/C++              |
|---------|--------------------|--------------------|
| message | class              | struct             |
| service | interface          | abstract class     |
| rpc     | interface method   | pure virtual function |

5. 代码生成：
   - 当您编译 .proto 文件时，protoc 编译器会生成对应语言的代码
   - 对于 message，生成相应的类/结构体
   - 对于 service，生成接口/抽象类和存根（stub）代码

6. 使用场景：
   - message: 定义数据结构，用于序列化和反序列化
   - service 和 rpc: 定义 API 接口，通常用于 gRPC 等 RPC 框架

7. 实际应用：
   - 客户端使用生成的存根代码调用远程方法
   - 服务器实现 service 中定义的方法
   - `::Service` 是 `UserService` 内部定义的一个基类,是由 gRPC 代码生成器自动生成的,包含了`UserService`定义的所有 RPC 方法的虚函数
   - 这里继承`UserService::Service`，允许重写（override）基类中的虚函数，提供实际的 RPC 方法实现。
   
   例如，在 C++ 中：
   ```cpp
   class UserServiceImpl final : public UserService::Service {
     Status GetUser(ServerContext* context, const GetUserRequest* request, User* reply) override {
       // 实现获取用户的逻辑
     }
   };
   ```

8. gRPC 集成：
   - gRPC 框架使用 Protocol Buffers 的 service 和 rpc 定义来生成客户端和服务器代码
   - 这使得 RPC 调用看起来像本地函数调用

总结：
- message 是数据结构的定义
- service 是一组 RPC 方法的集合，类似接口
- rpc 是单个远程调用方法的定义

这种设计允许您使用类型安全和高效的方式定义数据结构和 API，同时提供了跨语言和跨平台的兼容性。

### 在网络中的应用

先为服务器定义一个用来通信的proto

```c++
syntax = "proto3";
message MsgData
{
   int32  id = 1;
   string data = 2;
}
```

id代表消息id，data代表消息内容  
我们用`protoc`生成对应的pb.h和pb.cc文件  
将`proto`,`pb.cc`,`pb.h`三个文件复制到我们之前的服务器项目里并且配置。

我们修改服务器接收数据和发送数据的逻辑  
当服务器收到数据后，完成切包处理后，将信息反序列化为具体要使用的结构,打印相关的信息，然后再发送给客户端

```c++
	MsgData msgdata;
    std::string receive_data;
    msgdata.ParseFromString(std::string(_recv_msg_node->_data, _recv_msg_node->_total_len));
    std::cout << "recevie msg id  is " << msgdata.id() << " msg data is " << msgdata.data() << endl;
    std::string return_str = "server has received msg, msg data is " + msgdata.data();
    MsgData msgreturn;
    msgreturn.set_id(msgdata.id());
    msgreturn.set_data(return_str);
    msgreturn.SerializeToString(&return_str);
    Send(return_str);
```

同样的道理，客户端在发送的时候也利用protobuf进行消息的序列化，然后发给服务器

```c++
	MsgData msgdata;
    msgdata.set_id(1001);
    msgdata.set_data("hello world");
    std::string request;
    msgdata.SerializeToString(&request);
```

### pb高效得原因

- 跨语言、跨平台，与语言和平台无关的数据描述语言
- Protobuf序列化速度更快
- 数据体积更小（二进制格式） 
- 压缩效率高
- 用的base128得变体，可变长度编码整数，小数字用更少字节
## jsoncpp的使用


### jsoncpp简介
jsoncpp 是一个 C++ JSON 库，它提供了将 JSON 数据解析为 C++ 对象、将 C++ 对象序列化为 JSON 数据的功能。它支持所有主流操作系统（包括 Windows、Linux、Mac OS X 等），并且可以与常见编译器（包括 Visual Studio、GCC 等）兼容。

1. jsoncpp 库是以源代码的形式发布的，因此使用者需要自己构建和链接库文件。该库文件不依赖于第三方库，只需包含头文件即可使用。
2. jsoncpp 库的特点包括：
3. 轻量级：JSON 解析器和序列化器都非常快速，不会占用太多的 CPU 和内存资源；
4. 易于使用：提供简单的 API，易于理解和使用；
5. 可靠性高：经过广泛测试，已被许多企业和开发者用于生产环境中；
6. 开源免费：遵循 MIT 许可证发布，使用和修改均免费。


写一段代码测试

```c++
#include <iostream>
#include <json/json.h>
#include <json/value.h>
#include <json/reader.h>
int main()
{
    Json::Value root;
    root["id"] = 1001;
    root["data"] = "hello world";
    std::string request = root.toStyledString();
    std::cout << "request is " << request << std::endl;
    Json::Value root2;
    Json::Reader reader;
    reader.parse(request, root2);
    std::cout << "msg id is " << root2["id"] << " msg is " << root2["data"] << std::endl;
}
```

从这段代码中，我们先将root序列化为字符串，再将字符串反序列化为root2.  
输出如下

<img src="https://cdn.llfc.club/1684642765063.jpg" alt="image.png" style="zoom:60%;" />


### 网络编程中的应用

在客户端发送数据时对发送的数据进行序列化

```c++
	Json::Value root;
    root["id"] = 1001;
    root["data"] = "hello world";
    std::string request = root.toStyledString();
    size_t request_length = request.length();
    char send_data[MAX_LENGTH] = { 0 };
    //转为网络字节序
    int request_host_length = boost::asio::detail::socket_ops::host_to_network_short(request_length);
    memcpy(send_data, &request_host_length, 2);
    memcpy(send_data + 2, request.c_str(), request_length);
    boost::asio::write(sock, boost::asio::buffer(send_data, request_length + 2));
```

我们可以在服务器收到数据时进行json反序列化

```c++
	Json::Reader reader;
    Json::Value root;
    reader.parse(std::string(_recv_msg_node->_data, _recv_msg_node->_total_len), root);
    std::cout << "recevie msg id  is " << root["id"].asInt() << " msg data is "
    << root["data"].asString() << endl;
```


## asio处理粘包的简易方式

### 简易方式

之前我们介绍了通过async_read_some函数监听读事件，并且绑定了读事件的回调函数`HandleRead`
```c++
_socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH), std::bind(&CSession::HandleRead, this, 
std::placeholders::_1, std::placeholders::_2, SharedSelf()));
```

`async_read_some` 这个函数的特点是只要对端发数据，服务器接收到数据，即使没有收全对端发送的数据也会触发`HandleRead`函数，所以我们会在`HandleRead`回调函数里判断接收的字节数，接收的数据可能不满足头部长度，可能大于头部长度但小于消息体的长度，可能大于消息体的长度，还可能大于多个消息体的长度，所以要切包等，这些逻辑写起来很复杂，所以我们可以通过读取指定字节数，直到读完这些字节才触发回调函数，那么可以采用`async_read`函数，这个函数指定读取指定字节数，只有完全读完才会触发回调函数。

### 获取头部数据
我们可以读取指定的头部长度，大小为`HEAD_LENGTH`字节数，只有读完HEAD_LENGTH字节才触发`HandleReadHead`函数

```c++
void CSession::Start(){
    _recv_head_node->Clear();
    boost::asio::async_read(_socket, boost::asio::buffer(_recv_head_node->_data, HEAD_LENGTH), std::bind(&CSession::HandleReadHead, this, 
        std::placeholders::_1, std::placeholders::_2, SharedSelf()));
}
```

这样我们可以直接在`HandleReadHead`函数内处理头部信息

```c++
void CSession::HandleReadHead(const boost::system::error_code& error, size_t  bytes_transferred, std::shared_ptr<CSession> shared_self) {
    if (!error) {
        if (bytes_transferred < HEAD_LENGTH) {
            cout << "read head lenth error";
            Close();
            _server->ClearSession(_uuid);
            return;
        }
        //头部接收完，解析头部
        short data_len = 0;
        memcpy(&data_len, _recv_head_node->_data, HEAD_LENGTH);
        cout << "data_len is " << data_len << endl;
        //此处省略字节序转换
        // ...
        //头部长度非法
        if (data_len > MAX_LENGTH) {
            std::cout << "invalid data length is " << data_len << endl;
            _server->ClearSession(_uuid);
            return;
        }
        _recv_msg_node= make_shared<MsgNode>(data_len);
        boost::asio::async_read(_socket, boost::asio::buffer(_recv_msg_node->_data, _recv_msg_node->_total_len), 
            std::bind(&CSession::HandleReadMsg, this,
            std::placeholders::_1, std::placeholders::_2, SharedSelf()));
    }
    else {
        std::cout << "handle read failed, error is " << error.what() << endl;
        Close();
        _server->ClearSession(_uuid);
    }
}
```

接下来根据头部内存储的消息体长度，获取指定长度的消息体数据，所以再次调用`async_read`，指定读取`_recv_msg_node->_total_len`长度，然后触发`HandleReadMsg`函数


### 获取消息体
`HandleReadMsg`函数内解析消息体，解析完成后打印收到的消息，接下来继续监听读事件，监听读取指定头部大小字节，触发`HandleReadHead`函数， 然后再在`HandleReadHead`内继续监听读事件，获取消息体长度数据后触发`HandleReadMsg`函数，从而达到循环监听的目的。

```c++
void CSession::HandleReadMsg(const boost::system::error_code& error, size_t  bytes_transferred,
    std::shared_ptr<CSession> shared_self) {
    if (!error) {
        PrintRecvData(_data, bytes_transferred);
        std::chrono::milliseconds dura(2000);
        std::this_thread::sleep_for(dura);
        _recv_msg_node->_data[_recv_msg_node->_total_len] = '\0';
        cout << "receive data is " << _recv_msg_node->_data << endl;
        Send(_recv_msg_node->_data, _recv_msg_node->_total_len);
        //再次接收头部数据
        _recv_head_node->Clear();
        boost::asio::async_read(_socket, boost::asio::buffer(_recv_head_node->_data, HEAD_LENGTH),
            std::bind(&CSession::HandleReadHead, this, std::placeholders::_1, std::placeholders::_2,
                SharedSelf()));
    }
    else {
        cout << "handle read msg failed,  error is " << error.what() << endl;
        Close();
        _server->ClearSession(_uuid);
    }
}
```


## 服务器逻辑层设计和消息完善

本文概述基于`boost::asio`实现的服务器逻辑层结构，并且完善之前设计的消息结构。因为为了简化粘包处理，我们简化了发送数据的结构,这次我们给出完整的消息设计，以及服务器架构设计。

### 服务器架构设计

之前我们设计了Session(会话层)，并且给大家讲述了Asio底层的通信过程，如下图

<img src="https://cdn.llfc.club/1685620269385.jpg" alt="image.png" style="zoom:70%;" />

关键步骤讲解：
1. 监听读事件的文件描述符是否就绪，如果就绪了，说明对端有发送数据过来,就把回调函数放到就绪事件的队列里(ReadHandler)
2. asio底层有个单线程从队列里取出来要回调的函数，调用之后就触发了session这一层的回调函数。


```c++
//此处可以调用Send发送测试
Json::Reader reader;
Json::Value root;
reader.parse(std::string(_recv_msg_node->_data, _recv_msg_node->_total_len), root);
std::cout << "recevie msg id  is " << root["id"].asInt() << " msg data is "
		<< root["data"].asString() << endl;
root["data"] = "server has received msg, msg data is " + root["data"].asString();
std::string return_str = root.toStyledString();
Send(return_str);
```

- 这段代码是模拟逻辑层的处理，实际在公司中不仅仅做的是这种应答的echo的服务器，在收到对端请求解析后，要对这个请求做进一步处理，根据这个**请求类型、消息id**可能要做对应的逻辑层的函数，执行一些操作，比如读数据库、写数据库...,这就需要一个逻辑的类来处理，一般是单例，这里交给逻辑层处理。
- 但这里这里的逻辑操作有一些io操作可能比较耗时，1~2秒，是一个**性能开销**，这种情况下不能直接调用逻辑层的函数，做一个**解耦合**，把要处理的信息放到一个**队列**中，然后逻辑类再从队列中去取数据

接下来要设计的服务器结构是这样的

<img src="https://cdn.llfc.club/1685621283099.jpg" alt="image.png" style="zoom:80%;" />

关键步骤讲解：
1. 回调函数`ReadHandler1`投递给逻辑队列，逻辑队列不断堆积数据，有一个逻辑系统(Logic)是一个单例，会从队列中不断去取数据，自己处理，处理完成交给`async_send`,`async_send`再重新注册事件。  


> [!question] 为什么要用这个逻辑队列？
> 1. 在上图的Session层中，在Session层的ReadHandler里面直接处理逻辑，有一些逻辑看似不是阻塞，但比较耗时，session层的回调函数一直在处理，session层的就不会从队列里取出来要回调的函数了，后面的消息都会等待，这是看似不是阻塞，但是一个伪阻塞的现象，对并发有影响。
> 2. 在第2个图中用逻辑队列的方式让logic系统的单独的另一线程去从逻辑队列中取数据处理，而Session层触发回调函数，把回调函数要处理的数据、回调函数地址放到逻辑队列中，这样就实现了一个解耦合。
> 3. 这样就可以达成网络层在左边，而逻辑层在右边，两边通过一个队列连接。



> [!question] 逻辑线程大家知道为什么要是单线程？
> 1. redis的逻辑线程还是单线程，如果逻辑线程是多线程，逻辑里有交互，比如两个玩家，在一个工会里，a玩家登录了，给工会做了贡献，b玩家登录了，也给工会做了贡献，那么工会就是一个共享资源，多线程肯定要对工会这个共享资源加锁，几千个玩家的话，要频繁的加锁，还不如一个单线程。


### 消息头完善
这里肯定要根据消息id呢执行不同的回调函数，完善一下之前的消息头，之前的消息头仅包含数据域的长度，但是要进行逻辑处理，就需要传递一个id字段表示要处理的消息id，当然可以不在包头传id字段，将id序列化到消息体也是可以的，但是我们为了便于处理也便于回调逻辑层对应的函数，最好是将id写入包头。  
之前我们设计的消息结构是这样的

<img src="https://cdn.llfc.club/1683368829739.jpg" alt="image.png" style="zoom:60%;" />

现在将其完善为如下的样子

<img src="https://cdn.llfc.club/1683367901552.jpg" alt="image.png" style="zoom:60%;" />

- 为了减少耦合和歧义，我们重新设计消息节点。
	- `MsgNode`表示消息节点的基类，头部的消息用这个结构存储。
	- `RecvNode`表示接收消息的节点。
	- `SendNode`表示发送消息的节点。
- 我们将上述结构定义在`MsgNode.h`中

```c++
class MsgNode
{
public:
    MsgNode(short max_len) :_total_len(max_len), _cur_len(0) {
        _data = new char[_total_len + 1]();
        _data[_total_len] = '\0';
    }
    ~MsgNode() {
        std::cout << "destruct MsgNode" << endl;
        delete[] _data;
    }
    void Clear() {
        ::memset(_data, 0, _total_len);
        _cur_len = 0;
    }
    short _cur_len;
    short _total_len;
    char* _data;
};
class RecvNode :public MsgNode {
public:
    RecvNode(short max_len, short msg_id);
private:
    short _msg_id;
};
class SendNode:public MsgNode {
public:
    SendNode(const char* msg,short max_len, short msg_id);
private:
    short _msg_id;
};
```


实现如下

```c++
#include "MsgNode.h"
RecvNode::RecvNode(short max_len, short msg_id):MsgNode(max_len),
_msg_id(msg_id){
}
SendNode::SendNode(const char* msg, short max_len, short msg_id):MsgNode(max_len + HEAD_TOTAL_LEN)
, _msg_id(msg_id){
    //先发送id, 转为网络字节序
    short msg_id_host = boost::asio::detail::socket_ops::host_to_network_short(msg_id);
    memcpy(_data, &msg_id_host, HEAD_ID_LEN);
    //转为网络字节序
    short max_len_host = boost::asio::detail::socket_ops::host_to_network_short(max_len);
    memcpy(_data + HEAD_ID_LEN, &max_len_host, HEAD_DATA_LEN);
    memcpy(_data + HEAD_ID_LEN + HEAD_DATA_LEN, msg, max_len);
}
```

`SendNode`发送节点构造时，先将id转为网络字节序，然后写入`_data`数据域。
然后将要发送数据的长度转为大端字节序，写入`_data`数据域，注意要偏移`HEAD_ID_LEN`长度。
最后将要发送的数据msg写入`_data`数据域，注意要偏移`HEAD_ID_LEN+HEAD_DATA_LEN`


### Session类改写

因为消息结构改变了，所以我们接收和发送数据的逻辑要做对应的修改，我们先修改`Session`类中收发消息结构如下

```c++
	std::queue<shared_ptr<MsgNode> > _send_que;
    std::mutex _send_lock;
    //收到的消息结构
    std::shared_ptr<MsgNode> _recv_msg_node;
    bool _b_head_parse;
    //收到的头部结构
    std::shared_ptr<MsgNode> _recv_head_node;
```


因为头部数据只为4字节，所以我们在`Session`的构造函数中创建头部节点时选择`HEAD_TOTAL_LEN`(4字节)大小。

```c++
CSession::CSession(boost::asio::io_context& io_context, CServer* server):
    _socket(io_context), _server(server), _b_close(false),_b_head_parse(false){
    boost::uuids::uuid  a_uuid = boost::uuids::random_generator()();
    _uuid = boost::uuids::to_string(a_uuid);
    _recv_head_node = make_shared<MsgNode>(HEAD_TOTAL_LEN);
}
```

发送时我们构造发送节点，放到队列中即可

```c++
void CSession::Send(char* msg, short max_length, short msgid) {
    std::lock_guard<std::mutex> lock(_send_lock);
    int send_que_size = _send_que.size();
    if (send_que_size > MAX_SENDQUE) {
        std::cout << "session: " << _uuid << " send que fulled, size is " << MAX_SENDQUE << endl;
        return;
    }
    _send_que.push(make_shared<SendNode>(msg, max_length, msgid));
    if (send_que_size>0) {
        return;
    }
    auto& msgnode = _send_que.front();
    boost::asio::async_write(_socket, boost::asio::buffer(msgnode->_data, msgnode->_total_len), 
        std::bind(&CSession::HandleWrite, this, std::placeholders::_1, SharedSelf()));
}
```

当然我们也实现了一个重载版本

```c++
void CSession::Send(std::string msg, short msgid) {
    std::lock_guard<std::mutex> lock(_send_lock);
    int send_que_size = _send_que.size();
    if (send_que_size > MAX_SENDQUE) {
        std::cout << "session: " << _uuid << " send que fulled, size is " << MAX_SENDQUE << endl;
        return;
    }
    _send_que.push(make_shared<SendNode>(msg.c_str(), msg.length(), msgid));
    if (send_que_size > 0) {
        return;
    }
    auto& msgnode = _send_que.front();
    boost::asio::async_write(_socket, boost::asio::buffer(msgnode->_data, msgnode->_total_len),
        std::bind(&CSession::HandleWrite, this, std::placeholders::_1, SharedSelf()));
}
```


在接收数据时我们解析头部也要解析id字段

```c++
void CSession::HandleRead(const boost::system::error_code& error, size_t  bytes_transferred, std::shared_ptr<CSession> shared_self){
    try {
        if (!error) {
            //已经移动的字符数
            int copy_len = 0;
            while (bytes_transferred > 0) {
                if (!_b_head_parse) {
                    //收到的数据不足头部大小
                    if (bytes_transferred + _recv_head_node->_cur_len < HEAD_TOTAL_LEN) {
                        memcpy(_recv_head_node->_data + _recv_head_node->_cur_len, _data + copy_len, bytes_transferred);
                        _recv_head_node->_cur_len += bytes_transferred;
                        ::memset(_data, 0, MAX_LENGTH);
                        _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH),
                            std::bind(&CSession::HandleRead, this, std::placeholders::_1, std::placeholders::_2, shared_self));
                        return;
                    }
                    //收到的数据比头部多
                    //头部剩余未复制的长度
                    int head_remain = HEAD_TOTAL_LEN - _recv_head_node->_cur_len;
                    memcpy(_recv_head_node->_data + _recv_head_node->_cur_len, _data + copy_len, head_remain);
                    //更新已处理的data长度和剩余未处理的长度
                    copy_len += head_remain;
                    bytes_transferred -= head_remain;
                    //获取头部MSGID数据
                    short msg_id = 0;
                    memcpy(&msg_id, _recv_head_node->_data, HEAD_ID_LEN);
                    //网络字节序转化为本地字节序
                    msg_id = boost::asio::detail::socket_ops::network_to_host_short(msg_id);
                    std::cout << "msg_id is " << msg_id << endl;
                    //id非法
                    if (msg_id > MAX_LENGTH) {
                        std::cout << "invalid msg_id is " << msg_id << endl;
                        _server->ClearSession(_uuid);
                        return;
                    }
                    short msg_len = 0;
                    memcpy(&msg_len, _recv_head_node->_data+HEAD_ID_LEN, HEAD_DATA_LEN);
                    //网络字节序转化为本地字节序
                    msg_len = boost::asio::detail::socket_ops::network_to_host_short(msg_len);
                    std::cout << "msg_len is " << msg_len << endl;
                    //id非法
                    if (msg_len > MAX_LENGTH) {
                        std::cout << "invalid data length is " << msg_len << endl;
                        _server->ClearSession(_uuid);
                        return;
                    }
                    _recv_msg_node = make_shared<RecvNode>(msg_len, msg_id);
                    //消息的长度小于头部规定的长度，说明数据未收全，则先将部分消息放到接收节点里
                    if (bytes_transferred < msg_len) {
                        memcpy(_recv_msg_node->_data + _recv_msg_node->_cur_len, _data + copy_len, bytes_transferred);
                        _recv_msg_node->_cur_len += bytes_transferred;
                        ::memset(_data, 0, MAX_LENGTH);
                        _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH),
                            std::bind(&CSession::HandleRead, this, std::placeholders::_1, std::placeholders::_2, shared_self));
                        //头部处理完成
                        _b_head_parse = true;
                        return;
                    }
                    memcpy(_recv_msg_node->_data + _recv_msg_node->_cur_len, _data + copy_len, msg_len);
                    _recv_msg_node->_cur_len += msg_len;
                    copy_len += msg_len;
                    bytes_transferred -= msg_len;
                    _recv_msg_node->_data[_recv_msg_node->_total_len] = '\0';
                    //cout << "receive data is " << _recv_msg_node->_data << endl;
                    //此处可以调用Send发送测试
                    Json::Reader reader;
                    Json::Value root;
                    reader.parse(std::string(_recv_msg_node->_data, _recv_msg_node->_total_len), root);
                    std::cout << "recevie msg id  is " << root["id"].asInt() << " msg data is "
                        << root["data"].asString() << endl;
                    root["data"] = "server has received msg, msg data is " + root["data"].asString();
                    std::string return_str = root.toStyledString();
                    Send(return_str, root["id"].asInt());
                    //继续轮询剩余未处理数据
                    _b_head_parse = false;
                    _recv_head_node->Clear();
                    if (bytes_transferred <= 0) {
                        ::memset(_data, 0, MAX_LENGTH);
                        _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH),
                            std::bind(&CSession::HandleRead, this, std::placeholders::_1, std::placeholders::_2, shared_self));
                        return;
                    }
                    continue;
                }
                //已经处理完头部，处理上次未接受完的消息数据
                //接收的数据仍不足剩余未处理的
                int remain_msg = _recv_msg_node->_total_len - _recv_msg_node->_cur_len;
                if (bytes_transferred < remain_msg) {
                    memcpy(_recv_msg_node->_data + _recv_msg_node->_cur_len, _data + copy_len, bytes_transferred);
                    _recv_msg_node->_cur_len += bytes_transferred;
                    ::memset(_data, 0, MAX_LENGTH);
                    _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH),
                        std::bind(&CSession::HandleRead, this, std::placeholders::_1, std::placeholders::_2, shared_self));
                    return;
                }
                memcpy(_recv_msg_node->_data + _recv_msg_node->_cur_len, _data + copy_len, remain_msg);
                _recv_msg_node->_cur_len += remain_msg;
                bytes_transferred -= remain_msg;
                copy_len += remain_msg;
                _recv_msg_node->_data[_recv_msg_node->_total_len] = '\0';
                //cout << "receive data is " << _recv_msg_node->_data << endl;
                    //此处可以调用Send发送测试
                Json::Reader reader;
                Json::Value root;
                reader.parse(std::string(_recv_msg_node->_data, _recv_msg_node->_total_len), root);
                std::cout << "recevie msg id  is " << root["id"].asInt() << " msg data is "
                    << root["data"].asString() << endl;
                root["data"] = "server has received msg, msg data is " + root["data"].asString();
                std::string return_str = root.toStyledString();
                Send(return_str, root["id"].asInt());
                //继续轮询剩余未处理数据
                _b_head_parse = false;
                _recv_head_node->Clear();
                if (bytes_transferred <= 0) {
                    ::memset(_data, 0, MAX_LENGTH);
                    _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH),
                        std::bind(&CSession::HandleRead, this, std::placeholders::_1, std::placeholders::_2, shared_self));
                    return;
                }
                continue;
            }
        }
        else {
            std::cout << "handle read failed, error is " << error.what() << endl;
            Close();
            _server->ClearSession(_uuid);
        }
    }
    catch (std::exception& e) {
        std::cout << "Exception code is " << e.what() << endl;
    }
}
```

先解析头部id，再解析长度，然后根据id和长度构造消息节点，copy剩下的消息体, 把上面代码中处理消息头的逻辑截取如下

```c++
//获取头部MSGID数据
    short msg_id = 0;
    memcpy(&msg_id, _recv_head_node->_data, HEAD_ID_LEN);
    //网络字节序转化为本地字节序
    msg_id = boost::asio::detail::socket_ops::network_to_host_short(msg_id);
    std::cout << "msg_id is " << msg_id << endl;
    //id非法
    if (msg_id > MAX_LENGTH) {
        std::cout << "invalid msg_id is " << msg_id << endl;
        _server->ClearSession(_uuid);
        return;
    }
    short msg_len = 0;
    memcpy(&msg_len, _recv_head_node->_data+HEAD_ID_LEN, HEAD_DATA_LEN);
    //网络字节序转化为本地字节序
    msg_len = boost::asio::detail::socket_ops::network_to_host_short(msg_len);
    std::cout << "msg_len is " << msg_len << endl;
    //id非法
    if (msg_len > MAX_LENGTH) {
        std::cout << "invalid data length is " << msg_len << endl;
        _server->ClearSession(_uuid);
        return;
    }
    _recv_msg_node = make_shared<RecvNode>(msg_len, msg_id);
```



## 单例模式实现逻辑层设计

### 优雅退出
服务器优雅退出一直是服务器设计必须考虑的一个方向，意在能通过**捕获信号使服务器安全退出**。我们可以通过asio提供的信号机制绑定回调函数即可实现优雅退出。在主函数中我们添加

```c++
int main()
{
    try {
        boost::asio::io_context  io_context;
        boost::asio::signal_set signals(io_context, SIGINT, SIGTERM);
        signals.async_wait([&io_context](auto, auto) {
            io_context.stop();
            });
        CServer s(io_context, 10086);
        io_context.run();
    }
    catch (std::exception& e) {
        std::cerr << "Exception: " << e.what() << endl;
    }
}
```

利用`signal_set`定义了一系列信号合集，并且绑定了一个匿名函数，匿名函数捕获了`io_context`的引用，并且函数中设置了停止操作，也就是说当捕获到`SIGINT,SIGTERM`等信号时，会调用`io_context.stop`。

### 单例模板类
接下来我们实现一个单例模板类，因为服务器的逻辑处理需要单例模式，后期可能还会有一些模块的设计也需要单例模式，所以先实现一个单例模板类，然后其他想实现单例类只需要继承这个模板类即可。

```c++
#include <memory>
#include <mutex>
#include <iostream>
using namespace std;
template <typename T>
class Singleton {
protected:
    Singleton() = default;
    Singleton(const Singleton<T>&) = delete;
    Singleton& operator=(const Singleton<T>& st) = delete;
    static std::shared_ptr<T> _instance;
public:
    static std::shared_ptr<T> GetInstance() {
        static std::once_flag s_flag;
        std::call_once(s_flag, [&]() {
            _instance = shared_ptr<T>(new T);
            });
        return _instance;
    }
    void PrintAddress() {
        std::cout << _instance.get() << endl;
    }
    ~Singleton() {
        std::cout << "this is singleton destruct" << std::endl;
    }
};
template <typename T>
std::shared_ptr<T> Singleton<T>::_instance = nullptr;
```

单例模式模板类将无参构造，拷贝构造，拷贝赋值都设定为`protected`属性，其他的类无法访问，其实也可以设置为私有属性。析构函数设置为公有的，为了让智能指针析构。
Singleton有一个static类型的属性`_instance`, 它是我们实际要开辟类型的智能指针类型。
`s_flag`是函数`GetInstance`内的局部静态变量，该变量在函数`GetInstance`第一次调用时被初始化。以后无论调用多少次`GetInstance` `s_flag`都不会被重复初始化，而且`s_flag`存在静态区，会随着进程结束而自动释放。
`call_once`只会调用一次，而且是线程安全的， 其内部的原理就是调用该函数时加锁，然后设置`s_flag`内部的标记，设置为已经初始化，执行lambda表达式逻辑初始化智能指针，然后解锁。第二次调用`GetInstance`内部还会调用`call_once`, 只是`call_once`判断`s_flag`已经被初始化了就不执行初始化智能指针的操作了。

### LogicSystem单例类
我们实现逻辑系统的单例类，继承自`Singleton<LogicSystem>`，这样LogicSystem的构造函数和拷贝构造函数就都变为私有的了，因为基类的构造函数和拷贝构造函数都是私有的。另外LogicSystem也用了基类的成员`_instance`和GetInstance函数。从而达到单例效果。

```c++
typedef  function<void(shared_ptr<CSession>, short msg_id, string msg_data)> FunCallBack;
class LogicSystem:public Singleton<LogicSystem>
{
    friend class Singleton<LogicSystem>;
public:
    ~LogicSystem();
    void PostMsgToQue(shared_ptr < LogicNode> msg);
private:
    LogicSystem();
    void DealMsg();
    void RegisterCallBacks();
    void HelloWordCallBack(shared_ptr<CSession>, short msg_id, string msg_data);
    std::thread _worker_thread;
    std::queue<shared_ptr<LogicNode>> _msg_que;
    std::mutex _mutex;
    std::condition_variable _consume;
    bool _b_stop;
    std::map<short, FunCallBack> _fun_callbacks;
};
```

1. `FunCallBack`为要注册的回调函数类型，其参数为会话类智能指针，消息id，以及消息内容。
2. `_msg_que`为逻辑队列
3. `_mutex` 为保证逻辑队列安全的互斥量
4. `_consume`表示消费者条件变量，用来控制当逻辑队列为空时保证线程暂时挂起等待，不要干扰其他线程。
5. `_fun_callbacks`表示回调函数的map，根据id查找对应的逻辑处理函数。
6. `_worker_thread`表示工作线程，用来从逻辑队列中取数据并执行回调函数。
7. `_b_stop`表示收到外部的停止信号，逻辑类要中止工作线程并优雅退出

`LogicNode`定义在CSession.h中

```c++
class LogicNode {
    friend class LogicSystem;
public:
    LogicNode(shared_ptr<CSession>, shared_ptr<RecvNode>);
private:
    shared_ptr<CSession> _session;
    shared_ptr<RecvNode> _recvnode;
};
```

其包含算了会话类的智能指针，主要是为了实现伪闭包，防止session被释放。
其次包含了接收消息的节点类的智能指针。
实现如下

```c++
LogicNode::LogicNode(shared_ptr<CSession>  session, 
    shared_ptr<RecvNode> recvnode):_session(session),_recvnode(recvnode) {
}
```

LogicSystem的构造函数如下

```c++
LogicSystem::LogicSystem():_b_stop(false){
    RegisterCallBacks();
    _worker_thread = std::thread (&LogicSystem::DealMsg, this);
}
```

构造函数中将停止信息初始化为false，注册消息处理函数并且启动了一个工作线程，工作线程执行`DealMsg`逻辑。  
注册消息处理函数的逻辑如下
```c++
void LogicSystem::RegisterCallBacks() {
    _fun_callbacks[MSG_HELLO_WORD] = std::bind(&LogicSystem::HelloWordCallBack, this,
        placeholders::_1, placeholders::_2, placeholders::_3);
}
```

`MSG_HELLO_WORD`定义在const.h中
```c++
enum MSG_IDS {
    MSG_HELLO_WORD = 1001
};
```

`MSG_HELLO_WORD`表示消息id，`HelloWordCallBack`为对应的回调处理函数
```c++
void LogicSystem::HelloWordCallBack(shared_ptr<CSession> session, short msg_id, string msg_data) {
    Json::Reader reader;
    Json::Value root;
    reader.parse(msg_data, root);
    std::cout << "recevie msg id  is " << root["id"].asInt() << " msg data is "
        << root["data"].asString() << endl;
    root["data"] = "server has received msg, msg data is " + root["data"].asString();
    std::string return_str = root.toStyledString();
    session->Send(return_str, root["id"].asInt());
}
```

在`HelloWordCallBack`里我们根据消息id和收到的消息，做了相应的处理并且回应给客户端。
工作线程的处理函数DealMsg逻辑
```c++
void LogicSystem::DealMsg() {
    for (;;) {
        std::unique_lock<std::mutex> unique_lk(_mutex);
        //判断队列为空则用条件变量阻塞等待，并释放锁
        while (_msg_que.empty() && !_b_stop) {
            _consume.wait(unique_lk);
        }
        //判断是否为关闭状态，把所有逻辑执行完后则退出循环
        if (_b_stop ) {
            while (!_msg_que.empty()) {
                auto msg_node = _msg_que.front();
                cout << "recv_msg id  is " << msg_node->_recvnode->_msg_id << endl;
                auto call_back_iter = _fun_callbacks.find(msg_node->_recvnode->_msg_id);
                if (call_back_iter == _fun_callbacks.end()) {
                    _msg_que.pop();
                    continue;
                }
                call_back_iter->second(msg_node->_session, msg_node->_recvnode->_msg_id,
                    std::string(msg_node->_recvnode->_data, msg_node->_recvnode->_cur_len));
                _msg_que.pop();
            }
            break;
        }
        //如果没有停服，且说明队列中有数据
        auto msg_node = _msg_que.front();
        cout << "recv_msg id  is " << msg_node->_recvnode->_msg_id << endl;
        auto call_back_iter = _fun_callbacks.find(msg_node->_recvnode->_msg_id);
        if (call_back_iter == _fun_callbacks.end()) {
            _msg_que.pop();
            continue;
        }
        call_back_iter->second(msg_node->_session, msg_node->_recvnode->_msg_id, 
            std::string(msg_node->_recvnode->_data, msg_node->_recvnode->_cur_len));
        _msg_que.pop();
    }
}
```

1. DealMsg逻辑中初始化了一个`unique_lock`，主要是用来控制队列安全，并且配合条件变量可以随时解锁。`lock_guard`不具备解锁功能，所以此处用`unique_lock`。
2. 我们判断队列为空，并且不是停止状态，就挂起线程。否则继续执行之后的逻辑，如果`_b_stop`为true，说明处于停服状态，则将队列中未处理的消息全部处理完然后退出循环。如果`_b_stop`未false，则说明没有停服，是`consumer`发送的激活信号激活了线程，则继续取队列中的数据处理。

LogicSystem的析构函数需要等待工作线程处理完再退出，但是工作线程可能处于挂起状态，所以要发送一个激活信号唤醒工作线程。并且将_b_stop标记设置为true。
```c++
LogicSystem::~LogicSystem(){
    _b_stop = true;
    _consume.notify_one();
    _worker_thread.join();
}
```

因为网络层收到消息后我们需要将消息投递给逻辑队列进行处理，那么LogicSystem就要封装一个投递函数

```c++
void LogicSystem::PostMsgToQue(shared_ptr < LogicNode> msg) {
    std::unique_lock<std::mutex> unique_lk(_mutex);
    _msg_que.push(msg);
    //由0变为1则发送通知信号
    if (_msg_que.size() == 1) {
        _consume.notify_one();
    }
}
```

在Session收到数据时这样调用
```c++
LogicSystem::GetInstance()->PostMsgToQue(make_shared<LogicNode>(shared_from_this(), _recv_msg_node));
```

再次启动服务器，编译启动，和之前一样可以看到数据收发正常。如下图:
<img src="https://cdn.llfc.club/1685849713914.jpg" alt="image.png" style="zoom:70%;" />

## asio多线程模型IOServicePool

前面的设计，我们对asio的使用都是**单线程模式**，为了提升网络io并发处理的效率，这一次我们设计多线程模式下asio的使用方式。总体来说asio有两个多线程模型，第一个是启动多个线程，每个线程管理一个iocontext。第二种是只启动一个iocontext，被多个线程共享，后面的文章会对比两个模式的区别，这里先介绍第一种模式，多个线程，每个线程管理独立的iocontext服务。

### 单线程和多线程对比

之前的单线程模式图如下

<img src="https://cdn.llfc.club/1685620269385.jpg" alt="image.png" style="zoom:70%;" />

我们设计的`IOServicePool`类型的多线程模型如下：

<img src="https://cdn.llfc.club/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230604151126.png" alt="image.png" style="zoom:60%;" />

`IOServicePool`多线程模式特点

1. 每一个`io_context`跑在不同的线程里，所以同一个`socket`会被注册在同一个`io_context`里，它的回调函数也会被单独的一个线程回调，那么对于同一个`socket`，他的回调函数每次触发都是在同一个线程里，就不会有线程安全问题，网络io层面上的并发是线程安全的。 
 
2. 但是对于不同的`socket`,回调函数的触发可能是同一个线程(两个`socket`被分配到同一个`io_context`)，也可能不是同一个线程(两个socket被分配到不同的io_context里)。所以如果两个`socket`对应的上层逻辑处理，如果有交互或者访问共享区，会存在线程安全问题。比如`socket1`代表玩家1，socket2代表玩家2，玩家1和玩家2在逻辑层存在交互，比如两个玩家都在做工会任务，他们属于同一个工会，工会积分的增加就是共享区的数据，需要保证线程安全。可以通过加锁或者逻辑队列的方式解决安全问题，我们目前采取了后者。

3. 多线程相比单线程，极大的提高了并发能力，因为单线程仅有一个`io_context`服务用来监听读写事件，就绪后回调函数在一个线程里串行调用, 如果一个回调函数的调用时间较长肯定会影响后续的函数调用，毕竟是穿行调用。而采用多线程方式，可以在一定程度上减少前一个逻辑调用影响下一个调用的情况，比如两个`socket`被部署到不同的`iocontext`上，但是当两个`socket`部署到同一个`iocontext`上时仍然存在调用时间影响的问题。不过我们已经通过逻辑队列的方式将网络线程和逻辑线程解耦合了，不会出现前一个调用时间影响下一个回调触发的问题。

### IOServicePool实现

`IOServicePool`本质上是一个线程池，基本功能就是根据构造函数传入的数量创建n个线程和`iocontext`，然后每个线程跑一个iocontext，这样就可以并发处理不同iocontext读写事件了。

IOServicePool的声明
```c++
class AsioIOServicePool:public Singleton<AsioIOServicePool>
{
    friend Singleton<AsioIOServicePool>;
public:
    using IOService = boost::asio::io_context;
    using Work = boost::asio::io_context::work;
    using WorkPtr = std::unique_ptr<Work>;
    ~AsioIOServicePool();
    AsioIOServicePool(const AsioIOServicePool&) = delete;
    AsioIOServicePool& operator=(const AsioIOServicePool&) = delete;
    // 使用 round-robin 的方式返回一个 io_service
    boost::asio::io_context& GetIOService();
    void Stop();
private:
    AsioIOServicePool(std::size_t size = std::thread::hardware_concurrency());
    std::vector<IOService> _ioServices;
    std::vector<WorkPtr> _works;
    std::vector<std::thread> _threads;
    std::size_t   _nextIOService;
};
```

1. `_ioServices`是一个IOService的vector变量，用来存储初始化的多个IOService。
2. WorkPtr是`boost::asio::io_context::work`类型的unique指针。
	在实际使用中，我们通常会将一些异步操作提交给io_context进行处理，然后该操作会被异步执行，而不会立即返回结果。如果没有其他任务需要执行，那么io_context就会停止工作，导致所有正在进行的异步操作都被取消。这时，我们需要使用`boost::asio::io_context::work`对象来防止`io_context`停止工作。

	`boost::asio::io_context::work`的作用是持有一个指向`io_context`的引用，并通过创建一个“工作”项来保证io_context不会停止工作，直到work对象被销毁或者调用reset()方法为止。当所有异步操作完成后，程序可以使用`work.reset()`方法来释放`io_context`，从而让其正常退出。
3. `_threads`是一个线程vector,管理我们开辟的所有线程。
4. `_nextIOService`是一个轮询索引，我们用最简单的轮询算法为每个新创建的连接分配io_context.
5. 因为`IOServicePool`不允许被copy构造，所以我们将其拷贝构造和拷贝复制函数置为delete

接下来我们实现构造函数

```c++
AsioIOServicePool::AsioIOServicePool(std::size_t size):_ioServices(size),
_works(size), _nextIOService(0){
    for (std::size_t i = 0; i < size; ++i) {
        _works[i] = std::unique_ptr<Work>(new Work(_ioServices[i]));
    }
    //遍历多个ioservice，创建多个线程，每个线程内部启动ioservice
    for (std::size_t i = 0; i < _ioServices.size(); ++i) {
        _threads.emplace_back([this, i]() {
            _ioServices[i].run();
            });
    }
}
```

`_works`是`unique_ptr`的vector类型，所以初始化时要么放在构造函数初始化列表里初始化，要么通过一个临时的`std::unique_ptr`右值初始化，我们采取的是第二种。

实现获取io_context&的函数
```c++
boost::asio::io_context& AsioIOServicePool::GetIOService() {
    auto& service = _ioServices[_nextIOService++];
    if (_nextIOService == _ioServices.size()) {
        _nextIOService = 0;
    }
    return service;
}
```

我们根据`_nextIOService`作为索引，轮询获取io_context&。

同样我们要实现Stop函数，控制AsioIOServicePool停止的行为。因为我们要保证每个线程安全退出后再让AsioIOServicePool停止。
```c++
void AsioIOServicePool::Stop(){
    for (auto& work : _works) {
        work.reset();
    }
    for (auto& t : _threads) {
        t.join();
    }
}
```

其中`work.reset()`是让unique指针置空并释放，那么work的析构函数就会被调用，work被析构，其管理的io_service在没有事件监听时就会被释放。


## asio多线程模式IOThreadPool

这一次介绍的另一种多线程模式IOThreadPool，我们只初始化一个iocontext用来监听服务器的读写事件，包括新连接到来的监听也用这个`iocontext`。只是我们让`iocontext.run`在多个线程中调用，这样回调函数就会被不同的线程触发，从这个角度看回调函数被并发调用了。

### 结构图
线程池模式的多线程模型调度结构图,如下

<img src="https://cdn.llfc.club/_20230607190415.png" alt="image.png" style="zoom:60%;" />


先实现IOThreadPool
```c++
#include <boost/asio.hpp>
#include "Singleton.h"
class AsioThreadPool:public Singleton<AsioThreadPool>
{
public:
    friend class Singleton<AsioThreadPool>;
    ~AsioThreadPool(){}
    AsioThreadPool& operator=(const AsioThreadPool&) = delete;
    AsioThreadPool(const AsioThreadPool&) = delete;
    boost::asio::io_context& GetIOService();
    void Stop();
private:
    AsioThreadPool(int threadNum = std::thread::hardware_concurrency());
    boost::asio::io_context _service;
    std::unique_ptr<boost::asio::io_context::work> _work;
    std::vector<std::thread> _threads;
};
```

`AsioThreadPool`继承了`Singleton<AsioThreadPool>`，实现了一个函数GetIOService获取iocontext

接下来我们看看具体实现
```c++
#include "AsioThreadPool.h"
AsioThreadPool::AsioThreadPool(int threadNum ):_work(new boost::asio::io_context::work(_service)){
    for (int i = 0; i < threadNum; ++i) {
        _threads.emplace_back([this]() {
            _service.run();
            });
    }
}
boost::asio::io_context& AsioThreadPool::GetIOService() {
    return _service;
}
void AsioThreadPool::Stop() {
    _work.reset();
    for (auto& t : _threads) {
        t.join();
    }
}
```


构造函数中实现了一个线程池，线程池里每个线程都会运行`_service.run`函数，`_service.run`函数内部就是从`iocp`或者`epoll`获取就绪描述符和绑定的回调函数，进而调用回调函数，因为回调函数是在不同的线程里调用的，所以会存在不同的线程调用同一个socket的回调函数的情况。
`_service.run` 内部在Linux环境下调用的是`epoll_wait`返回所有就绪的描述符列表，在`windows`上会循环调用`GetQueuedCompletionStatus`函数返回就绪的描述符，二者原理类似，进而通过描述符找到对应的注册的回调函数，然后调用回调函数。
比如iocp的流程是这样的
```txt
IOCP的使用主要分为以下几步：
1 创建完成端口(iocp)对象
2 创建一个或多个工作线程，在完成端口上执行并处理投递到完成端口上的I/O请求
3 Socket关联iocp对象，在Socket上投递网络事件
4 工作线程调用GetQueuedCompletionStatus函数获取完成通知封包，取得事件信息并进行处理
```

epoll的流程是这样的
```
1 调用epoll_creat在内核中创建一张epoll表
2 开辟一片包含n个epoll_event大小的连续空间
3 将要监听的socket注册到epoll表里
4 调用epoll_wait，传入之前我们开辟的连续空间，epoll_wait返回就绪的epoll_event列表，epoll会将就绪的socket信息写入我们之前开辟的连续空间
```

### 隐患
`IOThreadPool`模式有一个隐患，同一个`socket`的就绪后，触发的回调函数可能在不同的线程里，比如第一次是在线程1，第二次是在线程3，如果这两次触发间隔时间不大，那么很可能出现不同线程并发访问数据的情况，比如在处理读事件时，第一次回调触发后我们从`socket`的接收缓冲区读数据出来，第二次回调触发,还是从`socket`的接收缓冲区读数据，就会造成两个线程同时从`socket`中读数据的情况，会造成数据混乱。

### 利用strand改进
对于多线程触发回调函数的情况，我们可以利用asio提供的串行类`strand`封装一下，这样就可以被**串行调用**了，其基本原理就是在线程各自调用函数时取消了直接调用的方式，而是利用一个strand类型的对象将要调用的函数投递到strand管理的队列中，再由一个统一的线程调用回调函数，调用是串行的，解决了线程并发带来的安全问题。

<img src="https://cdn.llfc.club/_20230607192843.png" alt="image.png" style="zoom:60%;" />

图中当socket就绪后并不是由多个线程调用每个socket注册的回调函数，而是将回调函数投递给strand管理的队列，再由strand统一调度派发。
为了让回调函数被派发到strand的队列，我们只需要在注册回调函数时加一层strand的包装即可。
在CSession类中添加一个成员变量
```c++
strand<io_context::executor_type> _strand;
```

CSession的构造函数
```c++
CSession::CSession(boost::asio::io_context& io_context, CServer* server):
    _socket(io_context), _server(server), _b_close(false),
    _b_head_parse(false), _strand(io_context.get_executor()){
    boost::uuids::uuid  a_uuid = boost::uuids::random_generator()();
    _uuid = boost::uuids::to_string(a_uuid);
    _recv_head_node = make_shared<MsgNode>(HEAD_TOTAL_LEN);
}
```

可以看到`_strand`的初始化是放在初始化列表里，利用`io_context.get_executor()`返回的执行器构造strand。
因为在asio中无论`iocontext`还是strand，底层都是通过`executor`调度的，我们将他理解为调度器就可以了，如果多个`iocontext`和`strand`的调度器是一个，那他们的消息派发统一由这个调度器执行。
我们利用`iocontext`的调度器构造`strand`，这样他们统一由一个调度器管理。在绑定回调函数的调度器时，我们选择`strand`绑定即可。
比如我们在Start函数里添加绑定 ，将回调函数的调用者绑定为`_strand`

```c++
void CSession::Start(){
    ::memset(_data, 0, MAX_LENGTH);
    _socket.async_read_some(boost::asio::buffer(_data, MAX_LENGTH),
        boost::asio::bind_executor(_strand, std::bind(&CSession::HandleRead, this,
            std::placeholders::_1, std::placeholders::_2, SharedSelf())));
}
```

同样的道理，在所有收发的地方，都将调度器绑定为`_strand`， 比如发送部分我们需要修改为如下

```c++
auto& msgnode = _send_que.front();
    boost::asio::async_write(_socket, boost::asio::buffer(msgnode->_data, msgnode->_total_len), 
    boost::asio::bind_executor(_strand, std::bind(&CSession::HandleWrite, this, std::placeholders::_1, SharedSelf()))
        );
```

回调函数的处理部分也做对应的修改即可。

实际的生产和开发中，我们尽可能利用C++特性，使用多核的优势，将iocontext分布在不同的线程中效率更可取一点，但也要防止线程过多导致cpu切换带来的时间片开销，所以尽量让开辟的线程数小于或等于cpu的核数，从而利用多核优势。

## asio实现http服务器

前文介绍了asio如何实现并发的长连接tcp服务器，今天介绍如何实现http服务器，在介绍实现http服务器之前，需要讲述下http报文头的格式，其实http报文头的格式就是为了避免我们之前提到的粘包现象，告诉服务器一个数据包的开始和结尾，并在包头里标识请求的类型如get或post等信息。

### HTTP包头信息
一个标准的HTTP报文头通常由请求头和响应头两部分组成。

### HTTP 请求头
HTTP请求头包括以下字段：


下面的英文名称加`` 中文保留不变
- `Request-line`：包含用于描述请求类型、要访问的资源以及所使用的HTTP版本的信息。
- `Host`：指定被请求资源的主机名或IP地址和端口号。
- `Accept`：指定客户端能够接收的媒体类型列表，用逗号分隔，例如 text/plain, text/html。
- `User-Agent`：客户端使用的浏览器类型和版本号，供服务器统计用户代理信息。
- `Cookie`：如果请求中包含cookie信息，则通过这个字段将cookie信息发送给Web服务器。
- `Connection`：表示是否需要持久连接（keep-alive）。


```shell
GET /index.html HTTP/1.1
Host: www.example.com
Accept: text/html, application/xhtml+xml, */*
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0
Cookie: sessionid=abcdefg1234567
Connection: keep-alive
```

上述请求头包括了以下字段：

- `Request-line`：指定使用GET方法请求`/index.html`资源，并使用`HTTP/1.1`协议版本。
- `Host`：指定被请求资源所在主机名或IP地址和端口号。
- `Accept`：客户端期望接收的媒体类型列表，本例中指定了`text/html、application/xhtml+xml`和任意类型的文件`（/）`。
- `User-Agent`：客户端浏览器类型和版本号。
- `Cookie`：客户端发送给服务器的cookie信息。
- `Connection`：客户端请求后是否需要保持长连接。

### HTTP 响应头
HTTP响应头包括以下字段：

- `Status-line`：包含协议版本、状态码和状态消息。
- `Content-Type`：响应体的MIME类型。
- `Content-Length`：响应体的字节数。
- `Set-Cookie`：服务器向客户端发送cookie信息时使用该字段。
- `Server`：服务器类型和版本号。
- `Connection`：表示是否需要保持长连接（keep-alive）。
在实际的HTTP报文头中，还可以包含其他可选字段。
如下是一个http响应头的示例

```shell
HTTP/1.1 200 OK
Content-Type: text/html; charset=UTF-8
Content-Length: 1024
Set-Cookie: sessionid=abcdefg1234567; HttpOnly; Path=/
Server: Apache/2.2.32 (Unix) mod_ssl/2.2.32 OpenSSL/1.0.1e-fips mod_bwlimited/1.4
Connection: keep-alive
```

上述响应头包括了以下字段：
- `Status-line`：指定HTTP协议版本、状态码和状态消息。
- `Content-Type`：指定响应体的MIME类型及字符编码格式。
- `Content-Length`：指定响应体的字节数。
- `Set-Cookie`：服务器向客户端发送cookie信息时使用该字段。
- `Server`：服务器类型和版本号。
- `Connection`：服务器是否需要保持长连接。

### 客户端的编写
客户端每次发送数据都要携带头部信息，所以为了减少每次重新构造头部的开销，我们在客户端的构造函数里将头部信息构造好，作为一个成员放入客户端的类成员里。

```c++
client(boost::asio::io_context& io_context,
        const std::string& server, const std::string& path)
        : resolver_(io_context),
        socket_(io_context)
    {
        // Form the request. We specify the "Connection: close" header so that the
        // server will close the socket after transmitting the response. This will
        // allow us to treat all data up until the EOF as the content.
        std::ostream request_stream(&request_);
        request_stream << "GET " << path << " HTTP/1.0\r\n";
        request_stream << "Host: " << server << "\r\n";
        request_stream << "Accept: */*\r\n";
        request_stream << "Connection: close\r\n\r\n";
        size_t pos = server.find(":");
        std::string ip = server.substr(0, pos);
        std::string port = server.substr(pos + 1);
        // Start an asynchronous resolve to translate the server and service names
        // into a list of endpoints.
        resolver_.async_resolve(ip, port,
            boost::bind(&client::handle_resolve, this,
                boost::asio::placeholders::error,
                boost::asio::placeholders::results));
    }
```

我们的客户端构造了一个`request_`成员变量，依次写入请求的路径，主机地址，期望接受的媒体类型，以及每次收到请求后断开连接，也就是短链接的方式。
接着又异步解析ip和端口，解析成功后调用`handle_resolve`函数。
`handle_resolve`函数里异步处理连接

```c++
void handle_resolve(const boost::system::error_code& err,
        const tcp::resolver::results_type& endpoints)
    {
        if (!err)
        {
            // Attempt a connection to each endpoint in the list until we
            // successfully establish a connection.
            boost::asio::async_connect(socket_, endpoints,
                boost::bind(&client::handle_connect, this,
                    boost::asio::placeholders::error));
        }
        else
        {
            std::cout << "Error: " << err.message() << "\n";
        }
    }
```

处理连接
```c++
void handle_connect(const boost::system::error_code& err)
    {
        if (!err)
        {
            // The connection was successful. Send the request.
            boost::asio::async_write(socket_, request_,
                boost::bind(&client::handle_write_request, this,
                    boost::asio::placeholders::error));
        }
        else
        {
            std::cout << "Error: " << err.message() << "\n";
        }
    }
```

在连接成功后，我们首先将头部信息发送给服务器,发送完成后监听对端发送的数据

```c++
void handle_write_request(const boost::system::error_code& err)
    {
        if (!err)
        {
            // Read the response status line. The response_ streambuf will
            // automatically grow to accommodate the entire line. The growth may be
            // limited by passing a maximum size to the streambuf constructor.
            boost::asio::async_read_until(socket_, response_, "\r\n",
                boost::bind(&client::handle_read_status_line, this,
                    boost::asio::placeholders::error));
        }
        else
        {
            std::cout << "Error: " << err.message() << "\n";
        }
    }
```

当收到对方数据时，先解析响应的头部信息
```c++
void handle_read_status_line(const boost::system::error_code& err)
    {
        if (!err)
        {
            // Check that response is OK.
            std::istream response_stream(&response_);
            std::string http_version;
            response_stream >> http_version;
            unsigned int status_code;
            response_stream >> status_code;
            std::string status_message;
            std::getline(response_stream, status_message);
            if (!response_stream || http_version.substr(0, 5) != "HTTP/")
            {
                std::cout << "Invalid response\n";
                return;
            }
            if (status_code != 200)
            {
                std::cout << "Response returned with status code ";
                std::cout << status_code << "\n";
                return;
            }
            // Read the response headers, which are terminated by a blank line.
            boost::asio::async_read_until(socket_, response_, "\r\n\r\n",
                boost::bind(&client::handle_read_headers, this,
                    boost::asio::placeholders::error));
        }
        else
        {
            std::cout << "Error: " << err << "\n";
        }
    }
```

上面的代码先读出HTTP版本，以及返回的状态码，如果状态码不是200，则返回，是200说明响应成功。接下来把所有的头部信息都读出来。

```c++
void handle_read_headers(const boost::system::error_code& err)
    {
        if (!err)
        {
            // Process the response headers.
            std::istream response_stream(&response_);
            std::string header;
            while (std::getline(response_stream, header) && header != "\r")
                std::cout << header << "\n";
            std::cout << "\n";
            // Write whatever content we already have to output.
            if (response_.size() > 0)
                std::cout << &response_;
            // Start reading remaining data until EOF.
            boost::asio::async_read(socket_, response_,
                boost::asio::transfer_at_least(1),
                boost::bind(&client::handle_read_content, this,
                    boost::asio::placeholders::error));
        }
        else
        {
            std::cout << "Error: " << err << "\n";
        }
    }
```

上面的代码逐行读出头部信息，然后读出响应的内容，继续监听读事件读取相应的内容，直到接收到EOF信息，也就是对方关闭，继续监听读事件是因为有可能是长连接的方式，当然如果是短链接，则服务器关闭连接后，客户端也是通过异步函数读取EOF进而结束请求。
```c++
void handle_read_content(const boost::system::error_code& err)
    {
        if (!err)
        {
            // Write all of the data that has been read so far.
            std::cout << &response_;
            // Continue reading remaining data until EOF.
            boost::asio::async_read(socket_, response_,
                boost::asio::transfer_at_least(1),
                boost::bind(&client::handle_read_content, this,
                    boost::asio::placeholders::error));
        }
        else if (err != boost::asio::error::eof)
        {
            std::cout << "Error: " << err << "\n";
        }
    }
```

在主函数中调用客户端请求服务器信息, 请求的路由地址为`/`

```c++
int main(int argc, char* argv[])
{
    try
    {
        boost::asio::io_context io_context;
        client c(io_context, "127.0.0.1:8080", "/");
        io_context.run();
        getchar();
    }
    catch (std::exception& e)
    {
        std::cout << "Exception: " << e.what() << "\n";
    }
    return 0;
}
```

### 服务器设计
为了方便理解，我们从服务器的调用流程讲起
```c++
int main(int argc, char* argv[])
{
    try
    {
        std::filesystem::path path = std::filesystem::current_path() / "res";
        // 使用 std::cout 输出拼接后的路径
        std::cout << "Path: " << path.string() << '\n';
        std::cout << "Usage: http_server <127.0.0.1> <8080> "<< path.string() <<"\n";
        // Initialise the server.
        http::server::server s("127.0.0.1", "8080", path.string());
        // Run the server until stopped.
        s.run();
    }
    catch (std::exception& e)
    {
        std::cerr << "exception: " << e.what() << "\n";
    }
    return 0;
}
```

主函数里构造了一个server对象，然后调用了run函数使其跑起来。
run函数其实就是调用了server类成员的ioservice
```c++
void server::run()
{
    io_service_.run();
}
```

server类的构造函数里初始化一些成员变量，比如acceptor连接器，绑定了终止信号，并且监听对端连接
```c++
server::server(const std::string& address, const std::string& port,
    const std::string& doc_root)
            : io_service_(),
            signals_(io_service_),
            acceptor_(io_service_),
            connection_manager_(),
            socket_(io_service_),
            request_handler_(doc_root)
    {
            signals_.add(SIGINT);
            signals_.add(SIGTERM);
#if defined(SIGQUIT)
            signals_.add(SIGQUIT);
#endif 
            do_await_stop();
            boost::asio::ip::tcp::resolver resolver(io_service_);
            boost::asio::ip::tcp::endpoint endpoint = *resolver.resolve({ address, port });
            acceptor_.open(endpoint.protocol());
            acceptor_.set_option(boost::asio::ip::tcp::acceptor::reuse_address(true));
            acceptor_.bind(endpoint);
            acceptor_.listen();
            do_accept();
    }
```

接收连接请求 

```c++
void server::do_accept()
{
    acceptor_.async_accept(socket_,
        [this](boost::system::error_code ec)
        {
            if (!acceptor_.is_open())
            {
                return;
            }
            if (!ec)
            {
                connection_manager_.start(std::make_shared<connection>(
                            std::move(socket_), connection_manager_, request_handler_));
                    }
                do_accept();
            });
        }
```

接收函数里通过`connection_manager_`启动了一个新的连接，用来处理读写函数。  
处理方式和我们之前的写法类似，只是我们之前管理连接用的server，这次用的`conneciton_manager` 
```c++
void connection_manager::start(connection_ptr c)
{
    connections_.insert(c);
    c->start();
}
```

start函数里处理读写
```c++
void connection::start()
{
    do_read();
}
```

处理读数据比较复杂，我们分部分解释
```c++
void connection::do_read()
{
    auto self(shared_from_this());
    socket_.async_read_some(boost::asio::buffer(buffer_),
        [this, self](boost::system::error_code ec, std::size_t bytes_transferred)
    {
        if (!ec)
        {
            request_parser::result_type result;
            std::tie(result, std::ignore) = request_parser_.parse(
                request_, buffer_.data(), buffer_.data() + bytes_transferred);
            if (result == request_parser::good)
            {
                request_handler_.handle_request(request_, reply_);
                do_write();
            }
            else if (result == request_parser::bad)
            {
                reply_ = reply::stock_reply(reply::bad_request);
                do_write();
            }
            else
            {
                do_read();
            }
        }
        else if (ec != boost::asio::error::operation_aborted)
        {
            connection_manager_.stop(shared_from_this());
        }
    });
}

```

通过`request_parser_`解析请求，然后根据请求结果选择处理请求还是返回错误。
```c++
std::tuple<result_type, InputIterator> parse(request& req,
                InputIterator begin, InputIterator end)
{
    while (begin != end)
    {
        result_type result = consume(req, *begin++);
        if (result == good || result == bad)
            return std::make_tuple(result, begin);
    }
    return std::make_tuple(indeterminate, begin);
}
```

parse是解析请求的函数，内部调用了consume不断处理请求头中的数据，其实就是一个逐行解析的过程, consume函数很长，这里就不解释了，其实就是每解析一行就更改一下状态，这样可以继续解析。具体可以看看源码。

在consume()函数中，根据每个字符输入的不同情况，判断当前所处状态state_，进而执行相应的操作，包括：

- 将HTTP请求方法、URI和HTTP版本号解析到request结构体中。
- 解析每个请求头部字段的名称和值，并将其添加到request结构体中的headers vector中。
- 如果输入字符为`\r\n`，则修改状态以开始下一行的解析。

最后，返回一个枚举类型request_parser::result_type作为解析结果，包括indeterminate、good和bad三种状态。其中，indeterminate表示还需要继续等待更多字符输入；good表示成功解析出了一个完整的HTTP请求头部；bad表示遇到无效字符或格式错误，解析失败。

解析完成头部后会调用处理请求的函数,这里只是简单的写了一个作为资源服务器解析资源请求的逻辑，具体可以看源码。

```c++
void request_handler::handle_request(const request& req, reply& rep)
{
    // Decode url to path.
    std::string request_path;
    if (!url_decode(req.uri, request_path))
    {
        rep = reply::stock_reply(reply::bad_request);
        return;
    }
    // Request path must be absolute and not contain "..".
    if (request_path.empty() || request_path[0] != '/'
        || request_path.find("..") != std::string::npos)
    {
        rep = reply::stock_reply(reply::bad_request);
        return;
    }
    // If path ends in slash (i.e. is a directory) then add "index.html".
    if (request_path[request_path.size() - 1] == '/')
    {
        request_path += "index.html";
    }
    // Determine the file extension.
    std::size_t last_slash_pos = request_path.find_last_of("/");
    std::size_t last_dot_pos = request_path.find_last_of(".");
    std::string extension;
    if (last_dot_pos != std::string::npos && last_dot_pos > last_slash_pos)
    {
        extension = request_path.substr(last_dot_pos + 1);
    }
    // Open the file to send back.
    std::string full_path = doc_root_ + request_path;
    std::ifstream is(full_path.c_str(), std::ios::in | std::ios::binary);
    if (!is)
    {
        rep = reply::stock_reply(reply::not_found);
        return;
    }
    // Fill out the reply to be sent to the client.
    rep.status = reply::ok;
    char buf[512];
    while (is.read(buf, sizeof(buf)).gcount() > 0)
        rep.content.append(buf, is.gcount());
        rep.headers.resize(2);
        rep.headers[0].name = "Content-Length";
        rep.headers[0].value = std::to_string(rep.content.size());
        rep.headers[1].name = "Content-Type";
        rep.headers[1].value = mime_types::extension_to_type(extension);
    }
```

上述代码根据url中的`.`来做切割，获取请求的文件类型，然后根据`/`切割url，获取资源目录，最后返回资源文件。  
如果你想实现普通的路由请求返回json或者text格式，可以重写处理请求的逻辑。

## beast网络库搭建http服务器


前面的几篇文章已经介绍了如何使用asio搭建高并发的tcp服务器，以及http服务器。但是纯手写http服务器太麻烦了，有网络库beast已经帮我们实现了。这一期讲讲如何使用beast实现一个http服务器。

### 连接类
我们先实现http_server函数
```c++
void http_server(tcp::acceptor& acceptor, tcp::socket& socket)
{
    acceptor.async_accept(socket,
        [&](beast::error_code ec)
        {
            if (!ec)
                std::make_shared<http_connection>(std::move(socket))->start();
            http_server(acceptor, socket);
        });
}
```

http_server中添加了异步接收连接的逻辑，当有新的连接到来时创建`http_connection`,然后启动服务，新连接监听对端数据。接下来http_server继续监听对端的新连接。
连接类`http_connection`里实现了start函数监听对端数据

```c++
void start()
{
    read_request();
    check_deadline();
}
```

处理读请求,将读到的数据存储再成员变量request_中，然后调用process_request处理请求

```c++
void read_request()
{
    auto self = shared_from_this();
    http::async_read(
        socket_,
        buffer_,
        request_,
        [self](beast::error_code ec,
                std::size_t bytes_transferred)
        {
            boost::ignore_unused(bytes_transferred);
            if (!ec)
                    self->process_request();
        });
}
```


`check_deadline`主要时用来检测超时，当超过一定时间后自动关闭连接，因为http请求时短链接
```c++
void
        check_deadline()
    {
        auto self = shared_from_this();
        deadline_.async_wait(
            [self](beast::error_code ec)
            {
                if (!ec)
                {
                    // Close socket to cancel any outstanding operation.
                    self->socket_.close(ec);
                }
            });
    }
```

`process_request`函数中区分请求的类型，进行不同类型的处理如post还是get请求

```c++
void process_request()
{
    response_.version(request_.version());
    response_.keep_alive(false);
    switch (request_.method())
    {
        case http::verb::get:
            response_.result(http::status::ok);
            response_.set(http::field::server, "Beast");
            create_response();
            break;
        case http::verb::post:
            response_.result(http::status::ok);
            response_.set(http::field::server, "Beast");
            create_post_response();
            break;
        default:
            // We return responses indicating an error if
            // we do not recognize the request method.
            response_.result(http::status::bad_request);
            response_.set(http::field::content_type, "text/plain");
            beast::ostream(response_.body())
                << "Invalid request-method '"
                << std::string(request_.method_string())
                << "'";
            break;
        }
    write_response();
}
```

`create_response`函数中解析了不同的路由处理get请求

```c++
void
        create_response()
    {
        if (request_.target() == "/count")
        {
            response_.set(http::field::content_type, "text/html");
            beast::ostream(response_.body())
                << "<html>\n"
                << "<head><title>Request count</title></head>\n"
                << "<body>\n"
                << "<h1>Request count</h1>\n"
                << "<p>There have been "
                << my_program_state::request_count()
                << " requests so far.</p>\n"
                << "</body>\n"
                << "</html>\n";
        }
        else if (request_.target() == "/time")
        {
            response_.set(http::field::content_type, "text/html");
            beast::ostream(response_.body())
                << "<html>\n"
                << "<head><title>Current time</title></head>\n"
                << "<body>\n"
                << "<h1>Current time</h1>\n"
                << "<p>The current time is "
                << my_program_state::now()
                << " seconds since the epoch.</p>\n"
                << "</body>\n"
                << "</html>\n";
        }
        else
        {
            response_.result(http::status::not_found);
            response_.set(http::field::content_type, "text/plain");
            beast::ostream(response_.body()) << "File not found\r\n";
        }
    }
```

`create_post_response`处理了post请求中的一部分路由

```c++
void create_post_response() {
        if (request_.target() == "/email")
        {
            auto& body = this->request_.body();
            auto body_str = boost::beast::buffers_to_string(body.data());
            std::cout << "receive body is " << body_str << std::endl;
            this->response_.set(http::field::content_type, "text/json");
            Json::Value root;
            Json::Reader reader;
            Json::Value src_root;
            bool parse_success = reader.parse(body_str, src_root);
            if (!parse_success) {
                std::cout << "Failed to parse JSON data!" << std::endl;
                root["error"] = 1001;
                std::string jsonstr = root.toStyledString();
                beast::ostream(this->response_.body()) << jsonstr;
                return ;
            }
            auto email = src_root["email"].asString();
            std::cout << "email is " << email << std::endl;
            root["error"] = 0;
            root["email"] = src_root["email"];
            root["msg"] = "recevie email post success";
            std::string jsonstr = root.toStyledString();
            beast::ostream(this->response_.body()) << jsonstr;
        }
        else
        {
            response_.result(http::status::not_found);
            response_.set(http::field::content_type, "text/plain");
            beast::ostream(response_.body()) << "File not found\r\n";
        }
    }
```

`write_response`发送请求

```c++
void write_response()
{
    auto self = shared_from_this();
    response_.content_length(response_.body().size());
    http::async_write(
        socket_,
        response_,
        [self](beast::error_code ec, std::size_t)
        {
            self->socket_.shutdown(tcp::socket::shutdown_send, ec);
            self->deadline_.cancel();
    });
}
```

## beast网络库实现websocket服务器

使用beast网络库实现websocket服务器，一般来说websocket是一个长连接的协议，但是自动包含了解包处理，当我们在浏览器输入一个http请求时如果是以ws开头的如`ws://127.0.0.1:9501`就是请求本地9501端口的websocket服务器处理。而beast为我们提供了websocket的处理方案，我们可以在http服务器的基础上升级协议为websocket，处理部分websocket请求。如果服务器收到的是普通的http请求则按照http请求处理。我们可以从官方文档中按照示例逐步搭建websocket服务器。

### 构造websocket

在开始前我们先准备几个变量

```c++
#include <boost/beast.hpp>
#include <boost/beast/ssl.hpp>
#include <boost/asio.hpp>
#include <boost/asio/ssl.hpp>
namespace net = boost::asio;
namespace beast = boost::beast;
using namespace boost::beast;
using namespace boost::beast::websocket;
net::io_context ioc;
tcp_stream sock(ioc);
net::ssl::context ctx(net::ssl::context::tlsv12);
```

WebSocket连接需要一个有状态对象，由Beast中的一个类模板`websocket::stream`表示。该接口使用分层流模型。一个`websocket stream`对象包含另一个流对象，称为“下一层”，它用于执行I/O操作。以下是每个模板参数的描述：

```c++
namespace boost {
namespace beast {
namespace websocket {
template<
    class NextLayer,
    bool deflateSupported = true>
class stream;
} // websocket
} // beast
} // boost
```

这段代码定义了Beast库中WebSocket实现的命名空间。其中，`websocket`命名空间下包含一个模板类`stream`，用于表示WebSocket连接。

`stream`类有两个模板参数：`NextLayer`和`deflateSupported`。其中，`NextLayer`表示WebSocket连接使用的下一层流类型，例如TCP套接字或TLS握手后的数据流；而`deflateSupported`则是一个bool值，表示是否支持WebSocket协议内置的压缩功能。

这些代码所在的三个命名空间分别是`boost`、`beast`和`websocket`，是为了防止与其他库或用户代码发生名称冲突而创建的。将Beast库放在`beast`命名空间下是为了与Boost库本身分离，方便管理和维护。  
当创建一个WebSocket流对象时，构造函数提供的任何参数都会被传递给下一层对象的构造函数。以下示例代码声明了一个基于TCP/IP套接字和I/O上下文的`WebSocket`流对象：

```c++
stream<tcp_stream> ws(ioc);
```

上述代码创建了一个基于TCP流的WebSocket流对象，使用了指定的I/O上下文，该代码中的stream是Beast库中WebSocket流类模板的别名，其下一层流类型为tcp_stream。

需要注意的是，WebSocket流使用自己特定的超时功能来管理连接。如果使用tcp_stream或basic_stream类模板与WebSocket流一起使用，那么在连接建立后应该禁用TCP或basic流上的超时设置，否则流的行为将是不确定的。

这是因为WebSocket协议本身包含了超时机制，当流上发生超时时，WebSocket库会优先处理超时并关闭连接，而不会将超时事件传递给下层TCP或basic流。如果同时在`WebSocket`和TCP或basic流上启用超时设置，就可能出现冲突和未定义的行为。

因此，当使用WebSocket流时，应该避免在底层的TCP或basic流上设置超时，而是可以通过WebSocket流对象的`set_option`函数来设置WebSocket连接的超时时间。这样可以确保`WebSocket`连接中的超时机制正常工作，并且不会干扰底层流的超时设置。

与大多数I/O对象一样，WebSocket流对象也不是线程安全的。如果两个不同的线程同时访问该对象，则会产生未定义行为。

对于多线程程序，可以通过在构造`tcp_stream`对象时使用`executor`（如strand）来保证线程安全。下面的代码声明了一个使用strand来调用所有完成处理程序的WebSocket流：

```c++
stream<tcp_stream> ws(net::make_strand(ioc));
```

如果下一层流支持移动构造，那么WebSocket流对象可以从一个已移动的对象中构造。
这意味着，在创建WebSocket流对象时，可以将下一层流对象的所有权转移到WebSocket流对象中，而不需要进行复制或重新分配。这种方式可以避免额外的内存开销和数据拷贝操作，提高程序运行效率。
例如，可以使用std::move函数将一个已存在的TCP套接字对象移动到WebSocket流中：

```c++
stream<tcp_stream> ws(std::move(sock));
```


可以通过调用WebSocket流对象的next_layer函数来访问下一层流对象。
```c++
ws.next_layer().close();
```

### 使用SSL

使用`net::ssl::stream`类模板作为流的模板类型，并且将`net::io_context`和`net::ssl::context`参数传递给包装流的构造函数。

```c++
stream<ssl_stream<tcp_stream>> wss(net::make_strand(ioc), ctx);
```

当然如果websocket stream 使用SSL类型需要包含`<boost/beast/websocket/ssl.hpp>`  
`next_layer()` 函数用于访问底层的 SSL 流。`ssl::stream` 类中的 `next_layer()` 函数返回对底层 `ssl_stream` 的引用，它代表了建立在 `SSL/TLS` 层之上的网络流。

```c++
wss.next_layer().handshake(net::ssl::stream_base::client);
```

在上述声明的多层流（如 SSL 流）中，使用 next_layer 进行链式调用访问每个层可能会很麻烦。为了简化这个过程，Boost.Beast 提供了 get_lowest_layer() 函数，用于获取多层流中的最底层流。

通过调用 `get_lowest_layer()` 函数，您可以直接获取多层流中的最底层流，而无需逐层调用 `next_layer()`。这对于取消所有未完成的 I/O 操作非常有用，例如在关闭连接之前取消所有挂起的异步操作。

```c++
get_lowest_layer(wss).cancel();
```

### 连接
在进行 WebSocket 通信之前，需要首先连接 WebSocket 流，然后执行 WebSocket 握手。WebSocket 流将建立连接的任务委托给下一层流。例如，如果下一层是可连接的流或套接字对象，则可以访问它以调用必要的连接函数。以下是作为客户端执行的示例代码
```c++
stream<tcp_stream> ws(ioc);
net::ip::tcp::resolver resolver(ioc);
get_lowest_layer(ws).connect(resolver.resolve("example.com", "ws"));
```

对于服务器接收连接，在WebSocket服务器中使用一个acceptor来接受传入的连接。当建立了一个传入连接时，可以从acceptor返回的socket构造WebSocket流。
```c++
net::ip::tcp::acceptor acceptor(ioc);
acceptor.bind(net::ip::tcp::endpoint(net::ip::tcp::v4(), 0));
acceptor.listen();
stream<tcp_stream> ws(acceptor.accept());
```

也可以通过使用acceptor成员函数的另一个重载，将传入连接直接接受到WebSocket流拥有的socket中
```c++
stream<tcp_stream> ws(net::make_strand(ioc));
acceptor.accept(get_lowest_layer(ws).socket());
```

### 握手
websocket通过握手将http升级为websocket协议，一个websocket协议如下
```c++
GET / HTTP/1.1
Host: www.example.com
Upgrade: websocket
Connection: upgrade
Sec-WebSocket-Key: 2pGeTR0DsE4dfZs2pH+8MA==
Sec-WebSocket-Version: 13
User-Agent: Boost.Beast/216
```

先说客户端如何升级
这段代码使用`websocket::stream`的成员函数`handshake`和`async_handshake`，用于使用所需的主机名和目标字符串发送请求。该代码连接到从主机名查找返回的IP地址，然后在客户端角色中执行WebSocket握手

```c++
stream<tcp_stream> ws(ioc);
net::ip::tcp::resolver resolver(ioc);
get_lowest_layer(ws).connect(resolver.resolve("www.example.com", "ws"));
ws.handshake(
    "www.example.com",  // The Host field
    "/"                 // The request-target
);
```

在客户端收到来自服务器的HTTP Upgrade响应并指示成功升级时，调用者可能希望对接收到的HTTP响应消息进行额外的验证。例如，检查对基本身份验证挑战的响应是否有效。为了实现这一目的，handshake成员函数提供了重载形式，允许调用者将接收到的HTTP消息存储在类型为response_type的输出引用参数中。

```c++
response_type res;
ws.handshake(
    res,                // Receives the HTTP response
    "www.example.com",  // The Host field
    "/"                 // The request-target
);
```

所以上述握手函数根据自己的需求调用一个即可。

再说服务器如何升级

对于接受传入连接的服务器，`websocket::stream`可以读取传入的升级请求并自动回复。如果握手符合要求，流将发送带有101切换协议状态码的升级响应。如果握手不符合要求，或者超出了调用者之前设置的流选项允许的参数范围，流将发送一个带有表示错误的状态码的HTTP响应。根据保持活动设置，连接可能保持打开状态，以进行后续握手尝试。在接收到升级请求握手后，由实现创建和发送的典型HTTP升级响应如下所示：

```c++
HTTP/1.1 101 Switching Protocols
Upgrade: websocket
Connection: upgrade
Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=
Server: Boost.Beast
```

stream的accept和async_accept成员函数用于从已连接到传入对等方的流中读取WebSocket HTTP升级请求握手，然后发送WebSocket HTTP升级响应。示例如下

```c++
ws.accept();
```

在某些情况下，服务器可能需要从流中读取数据，并在稍后决定将缓冲的字节解释为WebSocket升级请求。为了满足这种需求，accept和async_accept提供了接受额外缓冲区参数的重载版本。

以下是一个示例，展示了服务器如何将初始的HTTP请求头部读取到一个动态缓冲区中，然后稍后使用缓冲的数据尝试进行WebSocket升级：
```c++
std::string s;
net::read_until(sock, net::dynamic_buffer(s), "\r\n\r\n");
ws.accept(net::buffer(s));
```

在实现同时支持WebSocket的HTTP服务器时，服务器通常需要从客户端读取HTTP请求。为了检测传入的HTTP请求是否是WebSocket升级请求，可以使用函数is_upgrade。

一旦调用者确定HTTP请求是WebSocket升级请求，就会提供额外的accept和async_accept重载版本，这些版本接收整个HTTP请求头作为一个对象，以进行握手处理。通过手动读取请求，程序可以处理普通的HTTP请求以及升级请求。程序还可以根据HTTP字段强制执行策略，例如基本身份验证。在这个示例中，首先使用HTTP算法读取请求，然后将其传递给新构建的流：

```c++
flat_buffer buffer;
http::request<http::string_body> req;
http::read(sock, buffer, req);
if(websocket::is_upgrade(req))
{
    stream<tcp_stream> ws(std::move(sock));
    BOOST_ASSERT(buffer.size() == 0);
    ws.accept(req);
}
else
{
}
```

所以综上所述，在构建websocket升级时，可以用两种方式，一种是websocket来accept，另一种是在处理http的请求时将请求升级为websocket。
而这两种我们在实战的代码中都有实现，可以下载源码查看。

### 收发数据
当我们建立好websocket的握手后，就可以通过读写操作收发数据了。
```c++
flat_buffer buffer;
ws.read(buffer);
ws.text(ws.got_text());
ws.write(buffer.data());
buffer.consume(buffer.size());
```

上面的代码片段采用同步读和同步写的方式，根据接收消息的类型设置 WebSocket 连接的模式。
回显接收到的消息给对等端。
清空缓冲区，以便下一次读取可以开始构建新的消息。

但有些场景我们不能通过buffer一次性的读出数据

这是一些使用场景，这些场景中无法预先缓冲整个消息：

1. 向终端流式传输多媒体：在流式传输多媒体到终端时，通常不适合或不可能预先缓冲整个消息。例如，在实时视频流或音频流的传输过程中，数据可能以非常大的速率产生，并且需要立即传输给接收端进行实时播放。由于数据量巨大且需即时传输，预先缓冲整个消息可能会导致延迟或资源耗尽。
2. 发送超出内存容量的消息：有时候需要发送的消息太大，无法一次性完全存储在内存中。这可能发生在需要传输大型文件或大量数据的情况下。如果尝试将整个消息加载到内存中，可能会导致内存溢出或系统性能下降。在这种情况下，需要通过分块或逐步读取的方式来发送消息，以便逐步加载和传输数据。
3. 提供增量结果：在某些情况下，需要在处理过程中提供增量的结果，而不是等待整个处理完成后再返回结果。这可以在长时间运行的计算、搜索或处理任务中发生。通过逐步提供部分结果，可以让用户或应用程序更早地获得一些数据，并可以在处理过程中进行进一步的操作或显示。这种方式可以改善用户体验，并减少等待时间。

在这些特定的使用场景中，需要采用逐步处理、流式传输或增量输出的方式，而不是依赖于预先缓冲整个消息。这样可以提高性能、降低内存消耗，并满足特定需求。

如下是asio提供的官方案例，通过流式读取的方式获取对端信息

```c++
multi_buffer buffer;
do
{
    ws.read_some(buffer, 512);
}
while(! ws.is_message_done());
ws.binary(ws.got_binary());
buffers_suffix<multi_buffer::const_buffers_type> cb{buffer.data()};
for(;;)
{
    if(buffer_bytes(cb) > 512)
    {
        ws.write_some(false, buffers_prefix(512, cb));
        cb.consume(512);
    }
    else
    {
        ws.write_some(true, cb);
        break;
    }
}
buffer.consume(buffer.size());
```

这段代码涉及 WebSocket 数据的读取和写入操作。以下是对每个部分的解释：

1. `multi_buffer buffer`;
这行代码定义了一个名为 buffer 的 multi_buffer 对象，用于存储读取的 WebSocket 数据。
2. `do { ... } while(! ws.is_message_done())`;
这部分代码使用一个循环，连续调用 ws.read_some 方法从 WebSocket 连接中读取数据，并将其存储到 buffer 中，直到 WebSocket 消息全部接收完成。
3. `ws.binary(ws.got_binary())`;
此行代码将 WebSocket 连接设置为二进制模式，以便在后续的写入操作中正确处理二进制数据。
4. `buffers_suffix<multi_buffer::const_buffers_type> cb{buffer.data()}`;
这行代码创建了一个 cb 对象，它是 buffer 的后缀子序列。它提供了对 buffer 中已接收数据的访问。
5. `if(buffer_bytes(cb) > 512) { ... } else { ... }`
这段代码检查 cb 中的数据量是否大于 512 字节。如果是，将执行 if 语句块；否则，将执行 else 语句块。
6. `ws.write_some(false, buffers_prefix(512, cb))`;
如果 cb 中的数据量大于 512 字节，此行代码将发送 cb 的前缀（前 512 字节）到 WebSocket 连接中，并保留剩余的数据。
7. `cb.consume(512)`;
这行代码告知 cb 对象消耗了前 512 字节的数据，以便在后续迭代中更新迭代范围。
8. `ws.write_some(true, cb)`;
如果 cb 中的数据量少于等于 512 字节，此行代码将发送 cb 中的所有数据到 WebSocket 连接中。
`buffer.consume(buffer.size())`;
这行代码清空 buffer 中存储的数据，使其为空。

总体来说，这段代码的作用是读取 `WebSocket` 数据并将其写回 WebSocket 连接，确保数据按照预期进行处理。最后，通过 `buffer.consume(buffer.size())` 清空缓冲区，准备下一次数据读取操作。

### 关闭连接
当我们想关闭连接时，可以通过close 或 async_close 的关闭函数。
具体而言，WebSocket 协议规定了两种关闭方式：

`close`
这是一个同步操作的函数，用于关闭 WebSocket 会话。当调用 close 时，客户端或服务器会向对方发送一个关闭帧，并且在收到对方的关闭帧后，会话将被正常关闭。
`async_close`
这是一个异步操作的函数，用于关闭 WebSocket 会话。与 close 不同，`async_close` 是一个非阻塞操作，它不会等待对方的关闭帧，而是立即返回，并触发一个异步关闭操作。这样可以在关闭过程中继续处理其他任务，而不必等待关闭完成。
这些关闭函数允许主机在 WebSocket 会话中发起关闭请求，以便安全地终止连接。应用程序可以根据需要选择适合的关闭函数，具体取决于其对同步或异步关闭的要求。
因为一个`websocket`会包含多个层，所以我们可以通过获取最底层，再执行关闭。这样保证所有层都安全关闭。
```c++
get_lowest_layer(wss).close();
```


## 配置和编译grpc

### grpc简介

gRPC是Google开发的一种高性能、开源的**远程过程调用**（RPC）框架。它可以让客户端应用程序像调用本地服务一样轻松地调用**远程服务**(调用另一台机器得某些代码)，从用户角度来看函数本地调用一样，GRPC并提供了多种语言的支持，如C++、Java、Python、Go等。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923103741.png" alt="image.png" style="zoom:60%;" />



gRPC使用`Protocol Buffers`作为数据交换格式，可以在**不同的平台**上进行应用程序之间的通信，支持多种编程语言和多种操作系统。它采用基于`HTTP/2`的协议，提供了高效、快速且可扩展的远程调用功能，并带有负载均衡、认证、监控等功能，方便用户管理和维护分布式系统。
gRPC可用于构建各种类型的**分布式应用程序**，如微服务、云原生应用程序、大规模Web应用程序、移动应用程序等场景。由于其高性能和可扩展性，越来越多的企业和组织开始采用gRPC来构建他们的应用程序和服务。


GRPC服务也是通过指定RPC方法、参数、返回类型在原型文件中定义

```proto
service Publisher{
	rpc SignBook (SignRequest) returns (SignReply) {}
}

message SignRequest {
	string name=1;
}

message SignReply {
	string signature =1;
}
```

使用相同得工具从PROTO文件生成G2PC客户端和服务器代码，开发人员在客户端中使用这些生成的类，来进行RPC调用，并在服务器中完成RPC请求


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923105002.png" alt="image.png" style="zoom:60%;" />


通过支持多种编程语言，客户端和服务器可以独立选择，最适合自己特定用例的编程语言和生态系统，大多数其他RPC框架传统上并非如此

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923105243.png?" alt="image.png" style="zoom:60%;" />
第二个受欢迎得点是grpc具有开箱即用的高性能,有两个因素影响其高性能

首先`Protocol Buffers`是一种是一种非常高效的二进制编码格式，比json快的多

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923105546.png" alt="image.png" style="zoom:60%;" />

其次其次GRPC构建在HTTP2之上，以提供大规模的高性能基础，HTTP2的使用带来了很多好处，GRPC使用HTTP2流，它允许通过单个长期TCP连接发送多个消息流，这使得GRPC框架，能够通过客户端和服务器之间的**少量TCP连接**，处理许多并发RPC调用


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923105712.png?" alt="image.png" style="zoom:60%;" />

gRPC同时支持同步调用和异步调用,同步RPC调用时会一直阻塞直到服务端处 理完成返回结果,异步RPC是客户端调用服务端时**不等待服务段处理完成返回**, 而是服务端处理完成后**主动回调**客户端告诉客户端处理完成我

### GRPC工作原理

从grpc客户端到grpc服务器的典型流程

1. 在下图订单服务是G2PC客户端，支付服务是grpc服务器，当订单服务向付款服务发出GRPC调用时，它在构建时调用G2PC工具生成的客户端代码，生成的客户端代码称为客户端层根(stub)，grpc将传递到客户端存根的数据，编码到`proto buffer`,并将其发送到第一级传输层。

	<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923111311.png" alt="image.png" style="zoom:70%;" />


2. GRPC通过网络以HTTP2数据帧流的形式发送数据，由于二进制编码和网络优化，据说grpc比json快五倍 

	<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923111812.png" alt="image.png" style="zoom:40%;" />

3. 支付服务从网络接收数据包,对其进行解码,然后调用服务器应用程序,从服务器应用程序返回的结果被编码到`proto buffer`中,并发送到传输层,订单服务接收数据包,对其进行解码,并将结果发送到客户端应用程序,正如我们从上面的示例中看到的,grpc非常容易实现。

	<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923112347.png" alt="image.png" style="zoom:60%;" />

为什么grpc不在web客户端和微服务器之间得到广泛使用
原因之一是grpc依赖于对HTTP2原语的较低级别访问(底层功能)，gRPC 需要的底层访问：gRPC 需要直接控制 HTTP/2 的帧（frames）和流（streams）。它需要能够设置特定的 HTTP/2 头部和控制消息的发送方式。目前没有浏览器提供支持，出于安全和抽象的考虑，浏览器不允许 Web 应用直接操作这些底层 HTTP/2 细节。
这意味着标准的 gRPC 客户端无法在浏览器中直接运行，grpc客户端所需的对web请求的控制级别,



## websocket


为什么需要websocket


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923203519.png" alt="image.png" style="zoom:60%;" />



以前的方式由这种由客户端主动请求，服务器响应的方式，也满足大部分网页的功能场景，这种情况下，服务器从来不会主动给客户端发一次消息，


<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923204046.png" alt="image.png" style="zoom:60%;" />


我们在浏览网页时会看到这样的场景，点击一个小的广告或者什么游戏页面，游戏里面的ai会动，这里是**服务器主动发消息给客户端**，这是怎么做到的呢？

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20240923204753.png" alt="image.png" style="zoom:40%;" />


其实问题的痛点在于怎么样才能在用户**不做任何操作**的情况下，网页能收到消息并发生变更，最常见的解决方案是前端代码不断定时发HTTP请求到服务器，服务器收到后响应消息，这是一种伪服务器推送的方式，比如二维码扫描，出现二维码后，前端会不断向后端服务器去询问查看是否扫过，时间间隔大概是1~2秒，这叫做**HTTP定时轮询**，但是这样做的缺点就是消耗服务器的负担和数据带宽，这个时间间隔1~2秒会让用户感到停顿。

更好解决方案是**长轮询机制**，长轮询是将http请求的超时时间设置的很长在较长时间内等待服务器响应，这样就减少了HTTP请求的个数。


考虑到网页游戏这种客户端和服务器之间都要互相主动发大量数据的场景，就需要用到WebSocket,WebSocket是一种网络传输协议，可在单个 TCP 连接上进行**全双工**通信，位于 OSI 模型的应用层。

WebSocket 使得客户端和服务器之间的数据交换变得更加简单，允许服务端主动向客户端推送数据。在 WebSocket API 中，浏览器和服务器只需要完成一次握手，两者之间就可以创建持久性的连接，并进行双向数据传输。