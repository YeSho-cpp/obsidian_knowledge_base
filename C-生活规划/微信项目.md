# 项目简介

- 项目描述：使用C++开发的跨平台全栈仿微信聊天系统，实现了包括用户注册、登录、即时通讯、离线消息、文件传输等核心功能。项目采用分布式架构，涵盖了从前端界面到后端服务的完整技术栈。
- 技术栈：客户端：C++/Qt, Asio (TCP通信)， Beast (HTTP请求)，服务端：C++， Asio， Beast， GRPC， Node.js (验证服务)，数据存储：Redis (缓存)， MySQL (持久化存储)，开发环境：Linux，CLion
- 主要工作：  
    1. 使用Qt框架设计并实现跨平台客户端界面，包括气泡聊天对话框、好友添加、好友通信、聊天记录展示等功能，并进行qss优化。  
    2. 基于Asio库开发高性能的异步TCP服务器，实现客户端与服务器间的实时通信；利用Beast库构建HTTP网关服务（GateServer），处理客户端连接请求和用户注册流程。  
    3. 设计并实现基于GRPC的微服务架构，包括验证服务（VarifyServer）和状态服务（StatusServer），并分布式部署多个ChatServer承载用户聊天连接。  
    4. 集成Redis用于缓存处理和会话管理，提高系统响应速度和用户体验；实现数据持久化存储，使用MySQL数据库管理用户信息和聊天记录，封装了基于mysqlconnector的连接池。  
    5. 开发文件传输功能，支持断点续传，提高大文件传输的可靠性和效率。  
    6. 基于Asio实现异步通信，通过连接池管理连接，包括Redis连接池和gRPC连接池。
- 项目难点：  
    1. 设计高并发、低延迟的分布式系统架构，确保消息的实时性和系统的可扩展性。  
    2. 实现复杂的网络通信模型，包括TCP长连接、HTTP短连接和GRPC微服务通信。  
    3. 优化大规模并发连接下的服务器性能，合理利用Redis缓存减轻数据库压力，单服务器支持8000连接，多服务器分布部署可支持1W~2W活跃用户。


- 这是一个全栈的即时通讯项目，前端基于QT实现气泡聊天对话框，通过`QListWidget`实现好友列表，利用`GridLayout`和`QPainter`封装气泡聊天框组件，基于`QT network`模块封装`http`和`tcp`服务。支持添加好友，好友通信，聊天记录展示等功能，仿微信布局并使用`qss`优化界面

- 后端采用分布式设计，分为`GateServer`网关服务，多个`ChatServer`聊天服务，`StatusServer`状态服务以及`VerifyServer`验证服务。

- 各服务通过`grpc`通信，支持断线重连。`GateServer`网关对外采用`http`服务，负责处理用户登录和注册功能。登录时`GateServer`从`StatusServer`查询聊天服务达到负载均衡，`ChatServer`聊天服务采用`asio`实现tcp可靠长链接异步通信和转发, 采用多线程模式封装`iocontext`池提升并发性能。数据存储采用mysql服务，并基于`mysqlconnector`库封装连接池，同时封装`redis`连接池处理缓存数据，以及`grpc`连接池保证多服务并发访问。

经测试单服务器支持8000连接，多服务器分布部署可支持1W~2W活跃用户。

**技术点**

**asio 网络库**，**grpc**，**Node.js**，**多线程，Redis, MySql，Qt 信号槽，网络编程，设计模式**

**项目意义**
关于项目意义可结合自身讨论，比如项目解决了高并发场景下单个服务连接数吃紧的情况，提升了自己对并发和异步的认知和处理能力等。


# 项目结构


<img src="https://cdn.llfc.club/1709009717000.jpg" alt="image.png" style="zoom:60%;" />




1. GateServer为网关服务，主要应对客户端的连接和注册请求，因为服务器是是分布式，所以GateServer收到用户连接请求后会查询状态服务选择一个负载较小的Server地址给客户端，客户端拿着这个地址直接和Server通信建立长连接。
2. 当用户注册时会发送给GateServer, GateServer调用VarifyServer验证注册的合理性并发送验证码给客户端，客户端拿着这个验证码去GateServer注册即可。
3. StatusServer， ServerA， ServerB都可以直接访问Redis和Mysql服务。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241009222820.png" alt="image.png" style="zoom:60%;" />

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241009222856.png" alt="image.png" style="zoom:60%;" />

# 考察点

#### 1 如何利用asio实现的tcp服务

利用asio 的多线程模式，根据cpu核数封装iocontext连接池，每个连接池跑在独立线程，采用异步`async_read`和`async_write`方式读写，通过消息回调完成数据收发。整个项目采用的网络模式是Proactor模式，每个连接通过Session类管理，通过智能指针管理Session,b保证回调之前Session可用，底层绑定用户id和session关联，回调函数可根据session反向查找用户进行消息推送。客户端和服务器通信采用json, 通过tlv方式(消息头(`消息id+消息长度`)+消息内容)封装消息包防止粘包。通过心跳机制检测连接可用性。

#### 2 如何保证服务高可用

1. **故障检测与自动恢复**：
    - 实施监控系统，实时检测服务的健康状况。
    - 配置自动重启或故障转移机制，确保在故障发生时能够迅速恢复服务。
2. **分布式架构**：
    - 采用微服务架构，将应用拆分为多个独立的服务，降低单个服务故障对整体系统的影响。
3. **数据备份与恢复**：
    - 定期备份数据，并进行恢复演练，确保在数据丢失或损坏时能够快速恢复。
4. **多活部署**：
    - 在不同地理位置部署多个活跃的数据中心，确保在某个数据中心发生故障时，其他数据中心可以继续提供服务。

#### 3 为何封装Mysql连接池

​ 首先多个线程使用同一个mysql连接是不安全的，所以要为每个线程分配独立连接，而连接数不能随着线程数无线增加，所以考虑连接池，每个线程想要操作mysql的时候从连接池取出连接进行数据访问。

Mysql连接池封装包括Mgr管理层和Dao数据访问层，Mgr管理层是单例模式，Dao层包含了一个连接池，采用生产者消费者模式管理可用连接，并且通过心跳定时访问mysql保活连接。


#### 5 用到哪些设计模式和思想

- Acto模式，逻辑解耦
- 生产者消费者模式（涉及线程池）
- 单例模式(网络管理和数据库管理类)
- RAII思想(defer 回收连接)
- 代理模式(数据库，redis等通过代理对接应用层调用，底层线程池隐藏技术细节)
- MVC控制思想，客户端通过MVC三层结构设计
- 线程分离，网络线程，数据处理线程，以及UI渲染线程分离
- 心跳服务
- 数据序列化压缩发送(Protobuf，Json)
- 队列解耦合，服务器采用发送队列保证异步顺序，通过接受队列缓存收到数据，通过逻辑队列处理数据。
- 分布式设计，多服务通过grpc通信，支持断线重连
- C++11 现代化技术，智能指针，伪闭包，模板类型推导，线程池，future, promise等

#### 6 描述线程池封装

​ 描述线程池封装，线程池采用C++ 11 风格编写，整体来说线程池通过单例封装，内部初始化N个线程，采用生产者消费者方式管理线程，包含任务队列，任务队列采用package_task打包存储，提供对外接口commit提交任务，采用bind语法实现任务提交在commit内部自行绑定，通过智能指针伪闭包方式保证任务生命周期。同时使用C++ 11 future特性，允许外部等待任务执行完成。

#### 7 为什么要设计心跳？

在网络情况下，会出现各种各样的中断，有些是网络不稳定或者客户端主动断开连接，这种服务器是可以检测到的。
PC拔掉网线，还有一种情况客户端突然崩溃，有时候服务器会检测不到断开连接，那么你这个用户就相当于僵尸连接。
当服务器有太多僵尸连接就会造成服务器性能的损耗。
另外心跳还有一个作用，保证连接持续可用，比如mysql，redis这种连接池，如果不设计心跳，
时间过长没有访问的时候连接会自动断开。


# 项目用到的一些闪亮点

1. 使用 `reqId`来解决异步请求-响应匹配
	在qt发送http请求时，一般都采用异步，而这个时候请求发送和响应接收是分离的。当收到响应时，`reqId` 可以帮助我们确定这个响应对应哪个原始请求以及状态跟踪。 
2. 使用lambda的捕获列表来达到闭包以及加shared_from_this解决异步回调函数参数可能销毁导致导致访问已释放的内存。
3. 使用定义的ConfigMgr.h来读取配置信息,减少
4. 使用c++封装了一个类似go语言中Defer类，可以写一些清理代码，无论条件(异常)怎么样，清理代码都能被执行，增强了代码的健壮性，延迟执行任意的代码块，不仅限于资源释放。
5. 这里用到了状态负载均衡
6. 这里使用队列去保证了**异步发送的有序性**，每个send是判断队列是否有，有了就放入，没有就执行队列的第一个任务，还有解耦的作用。
7. 这里当网络想要收到数据放到**逻辑线程**去处理的时候，它是通过队列来通信的，逻辑线程里面有个工作线程会不断从队列中取数据
8. StateServer返回给GateServer最早采用的方法是轮询的方法，后来改用了最小负载均衡的算法
9. 当连接量一些连接的量上来的时候内存压力比较大，可以放到redis中
10. 这里的滚动聊天是做了一点优化，每次滚动条范围变化都可能触发滚动到底部的操作，当新消息被添加时，`isAppended` 被设置为 true，在接下来的 500 毫秒内，所有的滚动条范围变化都会被视为"新消息添加"的一部分，500 毫秒后，`isAppended` 被重置为 false，这样，多个快速连续的变化被视为一个整体，只触发一次滚动到底部的操作


# 项目可以改进点

### 没有分表、分库和合服

假设我们正在运营一个大型多人在线角色扮演游戏（MMORPG）。

1. 初始状态：

假设我们的游戏刚开始时只有一个数据库，包含以下表：
- players (玩家信息)
- items (物品信息)
- quests (任务信息)

```sql
CREATE TABLE players (
    id INT PRIMARY KEY,
    name VARCHAR(50),
    level INT,
    experience INT
);

CREATE TABLE items (
    id INT PRIMARY KEY,
    player_id INT,
    name VARCHAR(50),
    quantity INT
);

CREATE TABLE quests (
    id INT PRIMARY KEY,
    player_id INT,
    name VARCHAR(100),
    status VARCHAR(20)
);
```

2. 分表：

随着玩家数量增加，`items` 表变得非常大，查询速度下降。我们决定按玩家ID范围分表：

```sql
CREATE TABLE items_0_9999 (
    id INT PRIMARY KEY,
    player_id INT,
    name VARCHAR(50),
    quantity INT
);

CREATE TABLE items_10000_19999 (
    -- 相同结构
);

-- 更多分表...
```

现在，当查询某个玩家的物品时，我们可以根据玩家ID直接查询对应的分表，提高查询效率。

3. 分库：

随着进一步增长，我们决定将不同的功能分到不同的数据库：

- player_db: 存储玩家基本信息
- inventory_db: 存储物品信息（已分表）
- quest_db: 存储任务信息

每个数据库可以部署在不同的服务器上，分散负载。

4. 分服：

为了处理不同地区的玩家，我们创建多个游戏服务器，每个服务器有自己的一套数据库：

- 美国服务器：us_player_db, us_inventory_db, us_quest_db
- 欧洲服务器：eu_player_db, eu_inventory_db, eu_quest_db
- 亚洲服务器：asia_player_db, asia_inventory_db, asia_quest_db

5. 合服：

一段时间后，我们发现美国和欧洲服务器的玩家活跃度下降，决定合并这两个服务器。合服过程可能包括：

a. 数据迁移：
   - 创建新的合并数据库：merged_player_db, merged_inventory_db, merged_quest_db
   - 将美国和欧洲服务器的数据导入到新数据库

b. 数据冲突处理：
   - 处理重复的玩家名称：
     ```sql
     UPDATE merged_player_db.players
     SET name = CONCAT(name, '_US')
     WHERE id IN (SELECT id FROM us_player_db.players);
     ```

c. 更新关联：
   - 更新物品表中的 player_id 引用
   - 更新任务表中的 player_id 引用

d. 服务器配置：
   - 更新游戏服务器配置，指向新的合并数据库

实际应用：

1. 分表：
   ```cpp
   int getPlayerItemTable(int playerId) {
       return playerId / 10000;  // 决定使用哪个分表
   }

   void addItem(int playerId, int itemId, int quantity) {
       int tableNum = getPlayerItemTable(playerId);
       std::string query = "INSERT INTO items_" + std::to_string(tableNum) + 
                           " (player_id, item_id, quantity) VALUES (?, ?, ?)";
       // 执行查询...
   }
   ```

2. 分库：
   ```cpp
   sql::Connection* getInventoryDBConnection(int playerId) {
       int serverNum = playerId % NUM_INVENTORY_SERVERS;
       return connectToDatabase("inventory_db_" + std::to_string(serverNum));
   }
   ```

3. 合服：
   ```cpp
   void mergePlayerData() {
       // 从美国服务器导入数据
       executeSql("INSERT INTO merged_player_db.players SELECT * FROM us_player_db.players");
       
       // 从欧洲服务器导入数据，处理可能的ID冲突
       executeSql("INSERT INTO merged_player_db.players "
                  "SELECT id + 1000000, name, level, experience "
                  "FROM eu_player_db.players");
       
       // 更新物品表的引用
       executeSql("UPDATE merged_inventory_db.items "
                  "SET player_id = player_id + 1000000 "
                  "WHERE player_id IN (SELECT id FROM eu_player_db.players)");
   }
   ```

这个例子展示了如何通过分表、分库和合服来处理大规模数据和用户基础。这些技术可以帮助提高系统的性能、可扩展性和管理效率。在实际应用中，这些操作通常会更复杂，可能需要考虑数据一致性、事务处理、数据备份等多个方面。


### 没有根据应用场景选取不同逻辑队列和工作线程

- 单一逻辑队列和单一工作线程可能成为性能瓶颈，无法根据不同类型的消息或任务进行优先级处理，可能导致某些高频或耗时任务阻塞其他任务的处理
- a. 任务分类：
	- 根据消息类型或处理逻辑将任务分类。
	- 例如：登录/注册、好友操作、聊天消息、系统通知等。
- b. 多队列设计：
	- 为每种类型的任务创建专门的队列。


登录功能和状态服务


1. 客户端发送登录请求给这个GateServer,这里是做一个简单的验证(邮箱和密码，这里是查询数据库)
2. 然后调用grpc请求给这个StateServer，StateServer会去查chatServer1,这里两个server是TCP的一个**长连接**的server 
3. StateServer会维护一张表，里面是这两个服务连上了多少客户端，会选择一个连接比较少的server,回传到GateServer的是一个ip和token(是在GateServer这里生成的)
4. 这里StateServer起到了一个中转以及查询的作用


可能的问题：

1. 你的项目为什么要用到StateServer？
	我们这里的GateServer要去StateServer获取服务器ip和token，这里的StateServer主要起到一个负载均衡的作用(查看那个TCP服务器比较空闲)
	chatserver是一个TCP长连接的服务，StateServer会维护一张表，维护这两个服务连上了多少个客户端，会给GateServer返回连接较小的chatserver,这个服务器ip和token是在状态服务里面生成的，这里起一个中转以及查询的作用


# asio底层的io模型和数据处理的调度方案

## One loop per thread

"One loop per thread"是一种并发编程模型，最初由陈硕在muduo网络库中提出并实现。这个模型在网络编程和高性能服务器设计中非常有效。

- 模型结构：
	- 主线程：运行主事件循环，负责接受新连接
	- 工作线程：每个都运行自己的事件循环，处理已建立连接的I/O操作
- 核心思想：
	- 每个线程运行一个事件循环（Event Loop）,与单 Reactor多线程模型相比，每个线程都有自己的 Reactor（事件循环），与 Reactor 多线程/进程模型相比，主线程也运行一个完整的事件循环，除了接受新连接，还可以处理I/O。
	- 每个循环处理该线程上的所有I/O事件和其他任务
	- 线程之间通过任务队列进行通信，而不直接共享数据
- 工作原理：
	- 当新连接到达时，主线程接受连接并将其分配给某个工作线程
	- 工作线程在其事件循环中处理该连接的所有后续I/O操作
	- 每个连接的所有操作都在同一线程中进行，避免了多线程竞争
- 优点：
	- 每个连接在单一的线程中处理，避免了多线程竞争，对比EPOLLONESHOT是一种更高级的设计，通过结构上避免了并发访问的需求，而EPOLLONESHOT是一种防御机制，还需要需要手动重新设置才能再次触发。
	- 有可能会提高缓存效率，连接相关的数据总是在同一线程中访问，提高了缓存命中率
	- 相比单 Reactor多线程模型，不需要在主线程和工作线程间频繁切换
- 并发问题上在muduo库和其他高性能网络库中，通常采用的是一种混合策略：
	- 使用轮询或最小负载算法选择工作线程。
	- 每个工作线程维护自己的任务队列。
	- 主线程直接将新连接的信息（如文件描述符）放入选定的工作线程的任务队列。
	- 工作线程在自己的事件循环中检查任务队列，处理新的连接。

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241010165207.png" alt="image.png" style="zoom:60%;" />

  

asio底层io模型和数据处理的调度方案

<img src="http://yesho-web.oss-cn-hangzhou.aliyuncs.com/img/20241010165247.png" alt="image.png" style="zoom:80%;" />

asio是以io_context为核心的，管理所有的异步操作和事件处理，包含一个scheduler（调度器）对象，负责任务的调度和执行。而scheduler维护一个op_queue_（任务队列），存储待执行的任务。以及存储待执行的任务。

异步调用流程(以 `http::async_read` 为例子)

1. 注册异步操作：
	- 当调用 `http::async_read` 时，Asio创建一个异步操作对象。
	- 这个对象包含了 I/O 请求的详情（socket、buffer等和回调函数）。
2. 将操作添加到 reactor：
	- Asio 将这个异步操作注册到reactor（图中的 `epoll_fd`）。
	- 实际上，它会将 socket 的文件描述符添加到 epoll 的监视列表中(放入到任务队列了)。
3. 返回控制权：
	- `async_read` 调用立即返回，不会阻塞。
	- 程序可以继续执行其他操作。
4. 等待 I/O 事件：
	- Asio 的内部线程（图中的"线程1"或"线程2"）在 `epoll_wait()` 中等待事件。
5. 事件触发：
	- 当 socket 有数据可读时，`epoll_wait()` 返回。
6. 创建任务：
	- Asio从事件中提取相关信息，创建一个任务。  
	- 这个任务包括执行我们提供的回调函数。
7. 将任务加入队列：
	- 任务被添加到 `op_queue_`（图中的任务队列）。
8. 执行回调：
	- 某个线程的 loop 从 `op_queue_` 中取出这个任务。 
	- 执行回调函数，处理读取的数据。
9. 继续循环：
	- 回调函数执行完毕后，线程继续循环，处理下一个任务或等待新的事件。
    
`op_queue_`的设计

- 使用无锁队列：Asio 实现了一个特殊的无锁队列结构，允许多个线程并发地访问队列，而不需要传统的互斥锁。
- 工作窃取算法：每个线程有自己的本地任务队列，当本地队列为空时，才会尝试从全局队列或其他线程的队列中"窃取"任务。
- 原子操作：使用原子操作来管理共享状态，减少锁的使用。


scheduler的设计主要负责：

- 任务分发：
	例子：假设有一个 HTTP 服务器，同时处理多个客户端请求。
	- 当新的连接到来时，scheduler 可能会选择一个负载较轻的线程来处理这个新连接。
	- 对于已建立的连接，scheduler 可能会尝试将同一连接的后续操作分配给同一个线程，以提高缓存效率。
- 负载均衡：
	例子：在高并发情况下，某些线程可能比其他线程更忙。
	- scheduler 可能会监控每个线程的任务队列长度。
	- 如果发现某个线程的队列特别长，它可能会将新任务分配给其他较空闲的线程。
- 管理 I/O 事件：
	例子：使用 epoll 监听多个 socket。
	- scheduler 可能负责调用 epoll_wait，等待 I/O 事件。
	- 当 epoll_wait 返回时，scheduler 解析哪些 socket 有事件发生，并创建相应的任务。
- 定时器管理：
	例子：实现一个定时重连机制。
	- 当连接断开时，应用程序设置一个 5 秒后重连的定时器。
	- scheduler 管理这个定时器，确保在 5 秒后触发重连操作。
- 优化策略实施：
	例子：使用工作窃取算法。
	- 如果一个线程完成了自己队列中的所有任务，scheduler 可能允许它"窃取"其他繁忙线程队列中的任务。
	- 这有助于在线程间更均匀地分配工作负载。


## 项目中redis的用处

1. 在验证服务器中存储验证码
2. 保存每个聊天服务器的当前连接数和aiso线程的socket数量
3. 保存一个登录请求的token(用户连接这个chatserver需要验证这个token)
4. 也会保存一份用户个人信息(如果合理再从内存中寻找用户信息，如果没找到则从数据库加载一份。)
5. 保存用户在那个聊天服务器上。
    

项目中Mysql的用处
1. 存储用户个人信息，注册时也会把这部分信息填上去
2. mysql更新密码
3. 登录时查询用户表判断用户是否存在

`接收：网络 -> _recv_msg_node -> 逻辑系统`
`发送：逻辑系统 -> _send_que -> 网络`



## 4 如何测试性能

​ 测试性能分为三个方面：

- 压力测试，测试服务器连接上限
- 测试一定连接数下，收发效率稳定性
- 采用pingpong协议，收发效率稳定在10ms下，连接数上限
压力测试，看服务器性能，客户端初始多个线程定时间隔连接，单服务节点连接上限2w以上稳定连接，并未出现掉线情况
测试稳定性，单服务节点连接数1W情况下，收发稳定未出现丢包和断线，并且延迟稳定在10ms
保证10ms延迟情况下，增加连接数，测下连接数上限，这个看机器性能，8000~2W连接不等。

编写测试脚本，开几个多个线程模拟postman发送，然后查询mysql连接数和top服务器查看信息