# 机器学习中的batch_size是什么？
今天研究了一下<font color=#ff0000>机器学习</font>，发现里面出现了一个参数叫`batch_size`。那么什么是`batch_size`呢？

`batch_size`：表示单次传递给程序用以训练的数据（样本）个数。比如我们的<font color=#ff0000>训练集</font>有`1000`个数据。这是如果我们设置`batch_size=100`，那么程序首先会用<font color=#ff0000>数据集</font>中的前`100`个参数，即第`1-100`个数据来训练模型。当训练完成后更新权重，再使用第`101-200`的个数据训练，直至第十次使用完训练集中的1000个数据后停止。

那么为什么要设置`batch_size`呢？

优势：

1. 可以减少内存的使用，因为我们每次只取`100`个数据，因此训练时所使用的内存量会比较小。这对于我们的电脑内存不能满足一次性训练所有数据时十分有效。可以理解为训练数据集的分块训练。
2. 提高训练的速度，因为每次完成训练后我们都会更新我们的权重值使其更趋向于精确值。所以完成训练的速度较快。
劣势：
	使用少量数据训练时可能因为数据量较少而造成训练中的<font color=#ff0000>梯度值较大</font>的波动

---

# Epoch

一个`epoch`, 表示： 所有的数据送入网络中， 完成了一次<font color=#ff0000>前向计算 + 反向传播</font>的过程。

由于一个`epoch`常常太大， 分成几个小的`baches` .

将所有数据迭代训练一次是不够的，需要反复多次才能拟合、收敛。

在实际训练时、将所有数据分成多个batch，每次送入一部分数据。

使用单个`epoch`更新权重不够。

随着`epoch`数量的增加，权重更新迭代的次数增多，曲线从最开始的不拟合状态， 进入优化拟合状态，最终进入过拟合。

`epoch`如何设置：大小与数据集的多样化程度有关， 多样化程度越强， `epoch`越大。

---

## iterations

完成一次epoch 需要的<font color=#ff0000>batch个数</font>

batch numbers就是 iterations .

分为了多少个batch? : 
	数据总数/ batch_size

---

