# 区别
通过卷积和池化等技术可以将图像进行降维，因此，一些研究人员也想办法恢复原分辨率大小的图像，特别是在语义分割领域应用很成熟。通过对一些资料的学习，简单的整理下三种恢复方法，并进行对比。

## 1、Upsampling（上采样）
在FCN、U-net等网络结构中，涉及到了上采样。上采样概念：<font color=#ff0000>上采样指的是任何可以让图像变成更高分辨率的技术</font>。最简单的方式是**重采样和插值**：将输入图片进行rescale到一个想要的尺寸，而且计算每个点的像素点，使用如双线性插值等插值方法对其余点进行插值来完成上采样过程。

![](https://img-blog.csdn.net/20181008201910304?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI3ODcxOTcz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

---

## 2、上池化

Unpooling是在CNN中常用的来表示max pooling的逆操作。这是论文《Visualizing and Understanding Convolutional Networks》中产生的思想，下图示意：
![png](https://img-blog.csdn.net/20181008201555319?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI3ODcxOTcz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

对比上面两个示意图，可以发现区别：

- 两者的区别在于==UnSampling阶段没有使用MaxPooling时的位置信息==，而是直接将内容复制来扩充Feature Map。第一幅图中右边4`*`4矩阵，用了四种颜色的正方形框分割为四个区域，每一个区域内的内容是直接复制上采样前的对应信息。
- UnPooling的过程，特点是在Maxpooling的时候保留最大值的<font color=#ff0000>位置信息</font>，之后在unPooling阶段使用该信息扩充Feature Map，除最大值位置以外，其余补0。从图中即可看到两者结果的不同。
---



## 反卷积(Transposed conv deconv)实现原理

### 什么是卷积

卷积就是把[卷积核](https://so.csdn.net/so/search?q=%E5%8D%B7%E7%A7%AF%E6%A0%B8&spm=1001.2101.3001.7020)放在输入上进行滑窗，将当前卷积核覆盖范围内的输入与卷积核相乘，值进行累加，得到当前位置的输出，其本质在于融合多个像素值的信息输出一个像素值，本质上是下采样的，所以输出的大小必然小于输入的大小，如下图所示：
![链接](https://img-blog.csdnimg.cn/817576d6121446c6b82266641c14c757.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASnVtaeeIseeskeeskQ==,size_20,color_FFFFFF,t_70,g_se,x_16)
![链接](https://img-blog.csdnimg.cn/1d0693b218564b2781c0bf8b4a634429.png)
### 什么是反卷积

反卷积和转置卷积都是一个意思，所谓的反卷积，就是卷积的逆操作，我们将上图的卷积看成是输入通过卷积核的透视，那么反卷积就可以看成输出通过卷积核的透视，具体如下图所示：
![链接](https://img-blog.csdnimg.cn/6d1fc0152ab54d78b87ec0deb8d6b4eb.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASnVtaeeIseeskeeskQ==,size_20,color_FFFFFF,t_70,g_se,x_16)
比如左上角的图，将输出的55按照绿色的线的逆方向投影回去，可以得到`[[55,110,55],[110,55,110],[55,55,110]]`的结果；  
我们将得到的四张特征图进行叠加（重合的地方其值相加），可以得到下图：
![链接](https://img-blog.csdnimg.cn/74c4abdc74a044ebbd813c4b6c860e7f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASnVtaeeIseeskeeskQ==,size_20,color_FFFFFF,t_70,g_se,x_16)
最终我们得到的特征图与卷积输入的特征图值的大小并不相同，说明卷积和反卷积<font color=#ff0000>并不是完全对等的可逆操作</font>（因为采用相同的卷积核，卷积和反卷积得到的输入输出不同），也就是==反卷积只能恢复尺寸，不能恢复数值==

**反卷积(deconv)为何能被称作转置卷积(Transposed conv)
因为反卷积就是将卷积操作中的卷积核矩阵进行了转置！！！！**
下面详细来说：
我们知道，卷积在框架中的底层实现是通过<font color=#ff0000>矩阵相乘</font>的方式，仍然以第一个卷积的例子来说：
我们将3X3的卷积核变换成4X16的矩阵，如下图所示：
![链接](https://img-blog.csdnimg.cn/ccbc8dbcd1f044ad8f15bcf45b68bf9e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASnVtaeeIseeskeeskQ==,size_20,color_FFFFFF,t_70,g_se,x_16)
为什么要将3X3的卷积核变换成4X16的矩阵呢？
原因：
1. 在我们最开始讲到的卷积的例子当中，对于4X4的输入而言，卷积核一共要放置4次（移动四次），一次在原位置，一次横向移动一下，再纵向移动一下，再横向移动一下，把输入看成一维的向量的话，就有了以上的卷积核矩阵；
2. 为什么要补0变成16维呢？因为是为了矩阵相乘，我们的输入是4X4，打成一维的就是1X16，当单个卷积核卷积的时候，另外7个值是没有参与计算的，所以<font color=#ff0000>要补7个0</font>；
接下来，我们将4X4的输入变成16X1的矩阵，如下图所示：
![链接](https://img-blog.csdnimg.cn/5d0d318ff4e54584bfe3d6daba86e76f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASnVtaeeIseeskeeskQ==,size_9,color_FFFFFF,t_70,g_se,x_16)
然后矩阵相乘再进行reshape即可得到卷积的结果：
![链接](https://img-blog.csdnimg.cn/44d0dcfcfcfe4a72a5c73ebeb23e6a5c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASnVtaeeIseeskeeskQ==,size_20,color_FFFFFF,t_70,g_se,x_16)
然后我们反卷积是如何实现的，如下图所示：
![链接](https://img-blog.csdnimg.cn/a47307af1279425393565d43f350d329.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASnVtaeeIseeskeeskQ==,size_20,color_FFFFFF,t_70,g_se,x_16)
在反卷积中，将4X16的卷积核进行了转置，与1X4的输入相乘，四个中间过程可以代表矩阵相乘过程中的中间结果；  
将中间结果reshape成4X4,再相加，可以得到与我们之前介绍反卷积推导相同的中间流程；
![链接](https://img-blog.csdnimg.cn/0df7cb33442e43c1a79618410384ed1b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASnVtaeeIseeskeeskQ==,size_20,color_FFFFFF,t_70,g_se,x_16)
![链接](https://img-blog.csdnimg.cn/24ed068e87df42bf9fa674274602033c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASnVtaeeIseeskeeskQ==,size_20,color_FFFFFF,t_70,g_se,x_16)
